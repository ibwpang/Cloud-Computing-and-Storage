Log file created at: 2016/11/27 14:16:22
Running on machine: pang-VirtualBox
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1127 14:16:22.168294  2966 caffe.cpp:210] Use CPU.
I1127 14:16:22.168640  2966 solver.cpp:48] Initializing solver from parameters: 
train_net: "/home/caffe/caffe/examples/myfile/myfile_nin/train.prototxt"
test_net: "/home/caffe/caffe/examples/myfile/myfile_nin/val.prototxt"
test_iter: 86
test_interval: 242
base_lr: 1e-05
display: 100
max_iter: 40000
lr_policy: "poly"
gamma: 0.1
power: 1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot"
snapshot_diff: false
solver_mode: CPU
train_state {
  level: 0
  stage: ""
}
snapshot_after_train: true
type: "SGD"
I1127 14:16:22.169739  2966 solver.cpp:81] Creating training net from train_net file: /home/caffe/caffe/examples/myfile/myfile_nin/train.prototxt
I1127 14:16:22.171111  2966 net.cpp:58] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    mirror: true
    crop_size: 114
    mean_file: "/home/caffe/caffe/examples/myfile/myfile_nin/mean.binaryproto"
  }
  data_param {
    source: "/home/caffe/caffe/examples/myfile/myfile_nin/img_train_lmdb"
    batch_size: 6
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.09
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Convolution3"
  top: "Convolution3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Convolution3"
  top: "Convolution4"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution5"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.2
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Convolution5"
  top: "Convolution5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Convolution5"
  top: "Convolution6"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Pooling2"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Pooling3"
  top: "Convolution7"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Convolution7"
  top: "Convolution7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution8"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Convolution4"
  bottom: "Convolution6"
  bottom: "Convolution7"
  bottom: "Convolution8"
  top: "Concat1"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.09
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.2
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Pooling4"
  type: "Pooling"
  bottom: "Concat1"
  top: "Pooling4"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Pooling4"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Convolution9"
  bottom: "Convolution11"
  bottom: "Convolution13"
  bottom: "Convolution14"
  top: "Concat2"
}
layer {
  name: "Pooling5"
  type: "Pooling"
  bottom: "Concat2"
  top: "Pooling5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Pooling5"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Pooling5"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.09
    }
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 208
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Pooling5"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.2
    }
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Pooling6"
  type: "Pooling"
  bottom: "Pooling5"
  top: "Pooling6"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Pooling6"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Convolution15"
  bottom: "Convolution17"
  bottom: "Convolution19"
  bottom: "Convolution20"
  top: "Concat3"
}
layer {
  name: "Pooling7"
  type: "Pooling"
  bottom: "Concat3"
  top: "Pooling7"
  pooling_param {
    pool: AVE
    kernel_size: 7
    stride: 1
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling7"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct2"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I1127 14:16:22.176208  2966 layer_factory.hpp:77] Creating layer Data1
I1127 14:16:22.177980  2966 net.cpp:100] Creating Layer Data1
I1127 14:16:22.178010  2966 net.cpp:408] Data1 -> Data1
I1127 14:16:22.178033  2966 net.cpp:408] Data1 -> Data2
I1127 14:16:22.178052  2966 data_transformer.cpp:25] Loading mean file from: /home/caffe/caffe/examples/myfile/myfile_nin/mean.binaryproto
I1127 14:16:22.181252  2968 db_lmdb.cpp:35] Opened lmdb /home/caffe/caffe/examples/myfile/myfile_nin/img_train_lmdb
I1127 14:16:22.186825  2966 data_layer.cpp:41] output data size: 6,1,114,114
I1127 14:16:22.189600  2966 net.cpp:150] Setting up Data1
I1127 14:16:22.189648  2966 net.cpp:157] Top shape: 6 1 114 114 (77976)
I1127 14:16:22.189658  2966 net.cpp:157] Top shape: 6 (6)
I1127 14:16:22.189664  2966 net.cpp:165] Memory required for data: 311928
I1127 14:16:22.189674  2966 layer_factory.hpp:77] Creating layer Convolution1
I1127 14:16:22.189695  2966 net.cpp:100] Creating Layer Convolution1
I1127 14:16:22.189703  2966 net.cpp:434] Convolution1 <- Data1
I1127 14:16:22.189716  2966 net.cpp:408] Convolution1 -> Convolution1
I1127 14:16:22.191139  2966 net.cpp:150] Setting up Convolution1
I1127 14:16:22.198863  2966 net.cpp:157] Top shape: 6 64 57 57 (1247616)
I1127 14:16:22.198912  2966 net.cpp:165] Memory required for data: 5302392
I1127 14:16:22.198966  2966 layer_factory.hpp:77] Creating layer ReLU1
I1127 14:16:22.199012  2966 net.cpp:100] Creating Layer ReLU1
I1127 14:16:22.199053  2966 net.cpp:434] ReLU1 <- Convolution1
I1127 14:16:22.199092  2966 net.cpp:395] ReLU1 -> Convolution1 (in-place)
I1127 14:16:22.200044  2966 net.cpp:150] Setting up ReLU1
I1127 14:16:22.200057  2966 net.cpp:157] Top shape: 6 64 57 57 (1247616)
I1127 14:16:22.200060  2966 net.cpp:165] Memory required for data: 10292856
I1127 14:16:22.200064  2966 layer_factory.hpp:77] Creating layer Pooling1
I1127 14:16:22.200072  2966 net.cpp:100] Creating Layer Pooling1
I1127 14:16:22.200074  2966 net.cpp:434] Pooling1 <- Convolution1
I1127 14:16:22.200079  2966 net.cpp:408] Pooling1 -> Pooling1
I1127 14:16:22.200114  2966 net.cpp:150] Setting up Pooling1
I1127 14:16:22.200119  2966 net.cpp:157] Top shape: 6 64 28 28 (301056)
I1127 14:16:22.200121  2966 net.cpp:165] Memory required for data: 11497080
I1127 14:16:22.200124  2966 layer_factory.hpp:77] Creating layer Convolution2
I1127 14:16:22.200135  2966 net.cpp:100] Creating Layer Convolution2
I1127 14:16:22.200139  2966 net.cpp:434] Convolution2 <- Pooling1
I1127 14:16:22.200142  2966 net.cpp:408] Convolution2 -> Convolution2
I1127 14:16:22.201145  2966 net.cpp:150] Setting up Convolution2
I1127 14:16:22.201155  2966 net.cpp:157] Top shape: 6 192 28 28 (903168)
I1127 14:16:22.201158  2966 net.cpp:165] Memory required for data: 15109752
I1127 14:16:22.201165  2966 layer_factory.hpp:77] Creating layer ReLU2
I1127 14:16:22.201170  2966 net.cpp:100] Creating Layer ReLU2
I1127 14:16:22.201174  2966 net.cpp:434] ReLU2 <- Convolution2
I1127 14:16:22.201177  2966 net.cpp:395] ReLU2 -> Convolution2 (in-place)
I1127 14:16:22.201184  2966 net.cpp:150] Setting up ReLU2
I1127 14:16:22.201186  2966 net.cpp:157] Top shape: 6 192 28 28 (903168)
I1127 14:16:22.201189  2966 net.cpp:165] Memory required for data: 18722424
I1127 14:16:22.201191  2966 layer_factory.hpp:77] Creating layer Pooling2
I1127 14:16:22.201195  2966 net.cpp:100] Creating Layer Pooling2
I1127 14:16:22.201198  2966 net.cpp:434] Pooling2 <- Convolution2
I1127 14:16:22.201201  2966 net.cpp:408] Pooling2 -> Pooling2
I1127 14:16:22.201207  2966 net.cpp:150] Setting up Pooling2
I1127 14:16:22.201211  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.201213  2966 net.cpp:165] Memory required for data: 19625592
I1127 14:16:22.201216  2966 layer_factory.hpp:77] Creating layer Pooling2_Pooling2_0_split
I1127 14:16:22.201622  2966 net.cpp:100] Creating Layer Pooling2_Pooling2_0_split
I1127 14:16:22.201697  2966 net.cpp:434] Pooling2_Pooling2_0_split <- Pooling2
I1127 14:16:22.201740  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_0
I1127 14:16:22.201782  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_1
I1127 14:16:22.201824  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_2
I1127 14:16:22.201867  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_3
I1127 14:16:22.201910  2966 net.cpp:150] Setting up Pooling2_Pooling2_0_split
I1127 14:16:22.202059  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.202100  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.202185  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.202313  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.202354  2966 net.cpp:165] Memory required for data: 23238264
I1127 14:16:22.202409  2966 layer_factory.hpp:77] Creating layer Convolution3
I1127 14:16:22.202457  2966 net.cpp:100] Creating Layer Convolution3
I1127 14:16:22.202497  2966 net.cpp:434] Convolution3 <- Pooling2_Pooling2_0_split_0
I1127 14:16:22.202538  2966 net.cpp:408] Convolution3 -> Convolution3
I1127 14:16:22.202713  2966 net.cpp:150] Setting up Convolution3
I1127 14:16:22.202755  2966 net.cpp:157] Top shape: 6 96 14 14 (112896)
I1127 14:16:22.202781  2966 net.cpp:165] Memory required for data: 23689848
I1127 14:16:22.202792  2966 layer_factory.hpp:77] Creating layer ReLU3
I1127 14:16:22.202802  2966 net.cpp:100] Creating Layer ReLU3
I1127 14:16:22.202808  2966 net.cpp:434] ReLU3 <- Convolution3
I1127 14:16:22.202814  2966 net.cpp:395] ReLU3 -> Convolution3 (in-place)
I1127 14:16:22.202823  2966 net.cpp:150] Setting up ReLU3
I1127 14:16:22.202829  2966 net.cpp:157] Top shape: 6 96 14 14 (112896)
I1127 14:16:22.202834  2966 net.cpp:165] Memory required for data: 24141432
I1127 14:16:22.202839  2966 layer_factory.hpp:77] Creating layer Convolution4
I1127 14:16:22.202848  2966 net.cpp:100] Creating Layer Convolution4
I1127 14:16:22.202854  2966 net.cpp:434] Convolution4 <- Convolution3
I1127 14:16:22.202862  2966 net.cpp:408] Convolution4 -> Convolution4
I1127 14:16:22.203610  2966 net.cpp:150] Setting up Convolution4
I1127 14:16:22.203621  2966 net.cpp:157] Top shape: 6 128 14 14 (150528)
I1127 14:16:22.203626  2966 net.cpp:165] Memory required for data: 24743544
I1127 14:16:22.203634  2966 layer_factory.hpp:77] Creating layer ReLU4
I1127 14:16:22.203641  2966 net.cpp:100] Creating Layer ReLU4
I1127 14:16:22.203647  2966 net.cpp:434] ReLU4 <- Convolution4
I1127 14:16:22.203654  2966 net.cpp:395] ReLU4 -> Convolution4 (in-place)
I1127 14:16:22.203660  2966 net.cpp:150] Setting up ReLU4
I1127 14:16:22.203667  2966 net.cpp:157] Top shape: 6 128 14 14 (150528)
I1127 14:16:22.203672  2966 net.cpp:165] Memory required for data: 25345656
I1127 14:16:22.203677  2966 layer_factory.hpp:77] Creating layer Convolution5
I1127 14:16:22.203686  2966 net.cpp:100] Creating Layer Convolution5
I1127 14:16:22.205510  2966 net.cpp:434] Convolution5 <- Pooling2_Pooling2_0_split_1
I1127 14:16:22.205535  2966 net.cpp:408] Convolution5 -> Convolution5
I1127 14:16:22.205585  2966 net.cpp:150] Setting up Convolution5
I1127 14:16:22.205595  2966 net.cpp:157] Top shape: 6 16 14 14 (18816)
I1127 14:16:22.205600  2966 net.cpp:165] Memory required for data: 25420920
I1127 14:16:22.205610  2966 layer_factory.hpp:77] Creating layer ReLU5
I1127 14:16:22.205618  2966 net.cpp:100] Creating Layer ReLU5
I1127 14:16:22.205624  2966 net.cpp:434] ReLU5 <- Convolution5
I1127 14:16:22.205631  2966 net.cpp:395] ReLU5 -> Convolution5 (in-place)
I1127 14:16:22.205638  2966 net.cpp:150] Setting up ReLU5
I1127 14:16:22.205646  2966 net.cpp:157] Top shape: 6 16 14 14 (18816)
I1127 14:16:22.205651  2966 net.cpp:165] Memory required for data: 25496184
I1127 14:16:22.205656  2966 layer_factory.hpp:77] Creating layer Convolution6
I1127 14:16:22.205663  2966 net.cpp:100] Creating Layer Convolution6
I1127 14:16:22.205687  2966 net.cpp:434] Convolution6 <- Convolution5
I1127 14:16:22.205696  2966 net.cpp:408] Convolution6 -> Convolution6
I1127 14:16:22.205798  2966 net.cpp:150] Setting up Convolution6
I1127 14:16:22.205809  2966 net.cpp:157] Top shape: 6 32 14 14 (37632)
I1127 14:16:22.205816  2966 net.cpp:165] Memory required for data: 25646712
I1127 14:16:22.205823  2966 layer_factory.hpp:77] Creating layer ReLU6
I1127 14:16:22.205832  2966 net.cpp:100] Creating Layer ReLU6
I1127 14:16:22.205837  2966 net.cpp:434] ReLU6 <- Convolution6
I1127 14:16:22.205847  2966 net.cpp:395] ReLU6 -> Convolution6 (in-place)
I1127 14:16:22.205855  2966 net.cpp:150] Setting up ReLU6
I1127 14:16:22.205863  2966 net.cpp:157] Top shape: 6 32 14 14 (37632)
I1127 14:16:22.205868  2966 net.cpp:165] Memory required for data: 25797240
I1127 14:16:22.205874  2966 layer_factory.hpp:77] Creating layer Pooling3
I1127 14:16:22.205889  2966 net.cpp:100] Creating Layer Pooling3
I1127 14:16:22.205898  2966 net.cpp:434] Pooling3 <- Pooling2_Pooling2_0_split_2
I1127 14:16:22.205905  2966 net.cpp:408] Pooling3 -> Pooling3
I1127 14:16:22.205919  2966 net.cpp:150] Setting up Pooling3
I1127 14:16:22.205925  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.205930  2966 net.cpp:165] Memory required for data: 26700408
I1127 14:16:22.205936  2966 layer_factory.hpp:77] Creating layer Convolution7
I1127 14:16:22.205950  2966 net.cpp:100] Creating Layer Convolution7
I1127 14:16:22.205956  2966 net.cpp:434] Convolution7 <- Pooling3
I1127 14:16:22.205965  2966 net.cpp:408] Convolution7 -> Convolution7
I1127 14:16:22.206044  2966 net.cpp:150] Setting up Convolution7
I1127 14:16:22.206068  2966 net.cpp:157] Top shape: 6 32 14 14 (37632)
I1127 14:16:22.206073  2966 net.cpp:165] Memory required for data: 26850936
I1127 14:16:22.206082  2966 layer_factory.hpp:77] Creating layer ReLU7
I1127 14:16:22.206089  2966 net.cpp:100] Creating Layer ReLU7
I1127 14:16:22.206095  2966 net.cpp:434] ReLU7 <- Convolution7
I1127 14:16:22.206102  2966 net.cpp:395] ReLU7 -> Convolution7 (in-place)
I1127 14:16:22.206111  2966 net.cpp:150] Setting up ReLU7
I1127 14:16:22.206117  2966 net.cpp:157] Top shape: 6 32 14 14 (37632)
I1127 14:16:22.206123  2966 net.cpp:165] Memory required for data: 27001464
I1127 14:16:22.206128  2966 layer_factory.hpp:77] Creating layer Convolution8
I1127 14:16:22.206141  2966 net.cpp:100] Creating Layer Convolution8
I1127 14:16:22.206147  2966 net.cpp:434] Convolution8 <- Pooling2_Pooling2_0_split_3
I1127 14:16:22.206157  2966 net.cpp:408] Convolution8 -> Convolution8
I1127 14:16:22.206265  2966 net.cpp:150] Setting up Convolution8
I1127 14:16:22.206276  2966 net.cpp:157] Top shape: 6 64 14 14 (75264)
I1127 14:16:22.206281  2966 net.cpp:165] Memory required for data: 27302520
I1127 14:16:22.206288  2966 layer_factory.hpp:77] Creating layer ReLU8
I1127 14:16:22.206295  2966 net.cpp:100] Creating Layer ReLU8
I1127 14:16:22.206301  2966 net.cpp:434] ReLU8 <- Convolution8
I1127 14:16:22.206331  2966 net.cpp:395] ReLU8 -> Convolution8 (in-place)
I1127 14:16:22.206403  2966 net.cpp:150] Setting up ReLU8
I1127 14:16:22.206415  2966 net.cpp:157] Top shape: 6 64 14 14 (75264)
I1127 14:16:22.206421  2966 net.cpp:165] Memory required for data: 27603576
I1127 14:16:22.206444  2966 layer_factory.hpp:77] Creating layer Concat1
I1127 14:16:22.206466  2966 net.cpp:100] Creating Layer Concat1
I1127 14:16:22.206473  2966 net.cpp:434] Concat1 <- Convolution4
I1127 14:16:22.206480  2966 net.cpp:434] Concat1 <- Convolution6
I1127 14:16:22.206486  2966 net.cpp:434] Concat1 <- Convolution7
I1127 14:16:22.206492  2966 net.cpp:434] Concat1 <- Convolution8
I1127 14:16:22.206501  2966 net.cpp:408] Concat1 -> Concat1
I1127 14:16:22.206514  2966 net.cpp:150] Setting up Concat1
I1127 14:16:22.206521  2966 net.cpp:157] Top shape: 6 256 14 14 (301056)
I1127 14:16:22.206527  2966 net.cpp:165] Memory required for data: 28807800
I1127 14:16:22.206532  2966 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I1127 14:16:22.206540  2966 net.cpp:100] Creating Layer Concat1_Concat1_0_split
I1127 14:16:22.206562  2966 net.cpp:434] Concat1_Concat1_0_split <- Concat1
I1127 14:16:22.206570  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I1127 14:16:22.206578  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I1127 14:16:22.206588  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_2
I1127 14:16:22.206595  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_3
I1127 14:16:22.206606  2966 net.cpp:150] Setting up Concat1_Concat1_0_split
I1127 14:16:22.206614  2966 net.cpp:157] Top shape: 6 256 14 14 (301056)
I1127 14:16:22.206619  2966 net.cpp:157] Top shape: 6 256 14 14 (301056)
I1127 14:16:22.206625  2966 net.cpp:157] Top shape: 6 256 14 14 (301056)
I1127 14:16:22.206632  2966 net.cpp:157] Top shape: 6 256 14 14 (301056)
I1127 14:16:22.206636  2966 net.cpp:165] Memory required for data: 33624696
I1127 14:16:22.206642  2966 layer_factory.hpp:77] Creating layer Convolution9
I1127 14:16:22.206655  2966 net.cpp:100] Creating Layer Convolution9
I1127 14:16:22.206660  2966 net.cpp:434] Convolution9 <- Concat1_Concat1_0_split_0
I1127 14:16:22.206667  2966 net.cpp:408] Convolution9 -> Convolution9
I1127 14:16:22.206951  2966 net.cpp:150] Setting up Convolution9
I1127 14:16:22.206960  2966 net.cpp:157] Top shape: 6 128 14 14 (150528)
I1127 14:16:22.206965  2966 net.cpp:165] Memory required for data: 34226808
I1127 14:16:22.206976  2966 layer_factory.hpp:77] Creating layer ReLU9
I1127 14:16:22.207000  2966 net.cpp:100] Creating Layer ReLU9
I1127 14:16:22.207005  2966 net.cpp:434] ReLU9 <- Convolution9
I1127 14:16:22.207010  2966 net.cpp:395] ReLU9 -> Convolution9 (in-place)
I1127 14:16:22.207018  2966 net.cpp:150] Setting up ReLU9
I1127 14:16:22.207025  2966 net.cpp:157] Top shape: 6 128 14 14 (150528)
I1127 14:16:22.207029  2966 net.cpp:165] Memory required for data: 34828920
I1127 14:16:22.207034  2966 layer_factory.hpp:77] Creating layer Convolution10
I1127 14:16:22.207043  2966 net.cpp:100] Creating Layer Convolution10
I1127 14:16:22.207049  2966 net.cpp:434] Convolution10 <- Concat1_Concat1_0_split_1
I1127 14:16:22.207073  2966 net.cpp:408] Convolution10 -> Convolution10
I1127 14:16:22.207350  2966 net.cpp:150] Setting up Convolution10
I1127 14:16:22.207360  2966 net.cpp:157] Top shape: 6 128 14 14 (150528)
I1127 14:16:22.207365  2966 net.cpp:165] Memory required for data: 35431032
I1127 14:16:22.207373  2966 layer_factory.hpp:77] Creating layer ReLU10
I1127 14:16:22.207379  2966 net.cpp:100] Creating Layer ReLU10
I1127 14:16:22.207386  2966 net.cpp:434] ReLU10 <- Convolution10
I1127 14:16:22.207391  2966 net.cpp:395] ReLU10 -> Convolution10 (in-place)
I1127 14:16:22.207398  2966 net.cpp:150] Setting up ReLU10
I1127 14:16:22.207404  2966 net.cpp:157] Top shape: 6 128 14 14 (150528)
I1127 14:16:22.207409  2966 net.cpp:165] Memory required for data: 36033144
I1127 14:16:22.207414  2966 layer_factory.hpp:77] Creating layer Convolution11
I1127 14:16:22.207422  2966 net.cpp:100] Creating Layer Convolution11
I1127 14:16:22.209844  2966 net.cpp:434] Convolution11 <- Convolution10
I1127 14:16:22.209856  2966 net.cpp:408] Convolution11 -> Convolution11
I1127 14:16:22.211386  2966 net.cpp:150] Setting up Convolution11
I1127 14:16:22.211397  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.211402  2966 net.cpp:165] Memory required for data: 36936312
I1127 14:16:22.211410  2966 layer_factory.hpp:77] Creating layer ReLU11
I1127 14:16:22.211416  2966 net.cpp:100] Creating Layer ReLU11
I1127 14:16:22.211422  2966 net.cpp:434] ReLU11 <- Convolution11
I1127 14:16:22.211428  2966 net.cpp:395] ReLU11 -> Convolution11 (in-place)
I1127 14:16:22.211436  2966 net.cpp:150] Setting up ReLU11
I1127 14:16:22.211441  2966 net.cpp:157] Top shape: 6 192 14 14 (225792)
I1127 14:16:22.211447  2966 net.cpp:165] Memory required for data: 37839480
I1127 14:16:22.211452  2966 layer_factory.hpp:77] Creating layer Convolution12
I1127 14:16:22.211458  2966 net.cpp:100] Creating Layer Convolution12
I1127 14:16:22.211463  2966 net.cpp:434] Convolution12 <- Concat1_Concat1_0_split_2
I1127 14:16:22.211480  2966 net.cpp:408] Convolution12 -> Convolution12
I1127 14:16:22.211547  2966 net.cpp:150] Setting up Convolution12
I1127 14:16:22.211558  2966 net.cpp:157] Top shape: 6 32 14 14 (37632)
I1127 14:16:22.211563  2966 net.cpp:165] Memory required for data: 37990008
I1127 14:16:22.211570  2966 layer_factory.hpp:77] Creating layer ReLU12
I1127 14:16:22.211576  2966 net.cpp:100] Creating Layer ReLU12
I1127 14:16:22.211581  2966 net.cpp:434] ReLU12 <- Convolution12
I1127 14:16:22.211587  2966 net.cpp:395] ReLU12 -> Convolution12 (in-place)
I1127 14:16:22.211594  2966 net.cpp:150] Setting up ReLU12
I1127 14:16:22.211601  2966 net.cpp:157] Top shape: 6 32 14 14 (37632)
I1127 14:16:22.211604  2966 net.cpp:165] Memory required for data: 38140536
I1127 14:16:22.211609  2966 layer_factory.hpp:77] Creating layer Convolution13
I1127 14:16:22.211617  2966 net.cpp:100] Creating Layer Convolution13
I1127 14:16:22.211623  2966 net.cpp:434] Convolution13 <- Convolution12
I1127 14:16:22.211630  2966 net.cpp:408] Convolution13 -> Convolution13
I1127 14:16:22.212141  2966 net.cpp:150] Setting up Convolution13
I1127 14:16:22.212149  2966 net.cpp:157] Top shape: 6 96 14 14 (112896)
I1127 14:16:22.212154  2966 net.cpp:165] Memory required for data: 38592120
I1127 14:16:22.212162  2966 layer_factory.hpp:77] Creating layer ReLU13
I1127 14:16:22.212170  2966 net.cpp:100] Creating Layer ReLU13
I1127 14:16:22.212175  2966 net.cpp:434] ReLU13 <- Convolution13
I1127 14:16:22.212182  2966 net.cpp:395] ReLU13 -> Convolution13 (in-place)
I1127 14:16:22.212188  2966 net.cpp:150] Setting up ReLU13
I1127 14:16:22.212194  2966 net.cpp:157] Top shape: 6 96 14 14 (112896)
I1127 14:16:22.212199  2966 net.cpp:165] Memory required for data: 39043704
I1127 14:16:22.212222  2966 layer_factory.hpp:77] Creating layer Pooling4
I1127 14:16:22.212260  2966 net.cpp:100] Creating Layer Pooling4
I1127 14:16:22.212266  2966 net.cpp:434] Pooling4 <- Concat1_Concat1_0_split_3
I1127 14:16:22.212275  2966 net.cpp:408] Pooling4 -> Pooling4
I1127 14:16:22.212283  2966 net.cpp:150] Setting up Pooling4
I1127 14:16:22.212290  2966 net.cpp:157] Top shape: 6 256 14 14 (301056)
I1127 14:16:22.212294  2966 net.cpp:165] Memory required for data: 40247928
I1127 14:16:22.212299  2966 layer_factory.hpp:77] Creating layer Convolution14
I1127 14:16:22.212308  2966 net.cpp:100] Creating Layer Convolution14
I1127 14:16:22.212313  2966 net.cpp:434] Convolution14 <- Pooling4
I1127 14:16:22.212321  2966 net.cpp:408] Convolution14 -> Convolution14
I1127 14:16:22.212440  2966 net.cpp:150] Setting up Convolution14
I1127 14:16:22.214354  2966 net.cpp:157] Top shape: 6 64 14 14 (75264)
I1127 14:16:22.214362  2966 net.cpp:165] Memory required for data: 40548984
I1127 14:16:22.214370  2966 layer_factory.hpp:77] Creating layer ReLU14
I1127 14:16:22.214378  2966 net.cpp:100] Creating Layer ReLU14
I1127 14:16:22.214385  2966 net.cpp:434] ReLU14 <- Convolution14
I1127 14:16:22.214390  2966 net.cpp:395] ReLU14 -> Convolution14 (in-place)
I1127 14:16:22.214398  2966 net.cpp:150] Setting up ReLU14
I1127 14:16:22.214404  2966 net.cpp:157] Top shape: 6 64 14 14 (75264)
I1127 14:16:22.214409  2966 net.cpp:165] Memory required for data: 40850040
I1127 14:16:22.214414  2966 layer_factory.hpp:77] Creating layer Concat2
I1127 14:16:22.214422  2966 net.cpp:100] Creating Layer Concat2
I1127 14:16:22.214428  2966 net.cpp:434] Concat2 <- Convolution9
I1127 14:16:22.214433  2966 net.cpp:434] Concat2 <- Convolution11
I1127 14:16:22.214439  2966 net.cpp:434] Concat2 <- Convolution13
I1127 14:16:22.214445  2966 net.cpp:434] Concat2 <- Convolution14
I1127 14:16:22.214452  2966 net.cpp:408] Concat2 -> Concat2
I1127 14:16:22.214459  2966 net.cpp:150] Setting up Concat2
I1127 14:16:22.214465  2966 net.cpp:157] Top shape: 6 480 14 14 (564480)
I1127 14:16:22.214470  2966 net.cpp:165] Memory required for data: 43107960
I1127 14:16:22.214475  2966 layer_factory.hpp:77] Creating layer Pooling5
I1127 14:16:22.214483  2966 net.cpp:100] Creating Layer Pooling5
I1127 14:16:22.214488  2966 net.cpp:434] Pooling5 <- Concat2
I1127 14:16:22.214504  2966 net.cpp:408] Pooling5 -> Pooling5
I1127 14:16:22.214514  2966 net.cpp:150] Setting up Pooling5
I1127 14:16:22.214519  2966 net.cpp:157] Top shape: 6 480 7 7 (141120)
I1127 14:16:22.214524  2966 net.cpp:165] Memory required for data: 43672440
I1127 14:16:22.214529  2966 layer_factory.hpp:77] Creating layer Pooling5_Pooling5_0_split
I1127 14:16:22.214535  2966 net.cpp:100] Creating Layer Pooling5_Pooling5_0_split
I1127 14:16:22.214541  2966 net.cpp:434] Pooling5_Pooling5_0_split <- Pooling5
I1127 14:16:22.214547  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_0
I1127 14:16:22.214555  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_1
I1127 14:16:22.214565  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_2
I1127 14:16:22.214572  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_3
I1127 14:16:22.214581  2966 net.cpp:150] Setting up Pooling5_Pooling5_0_split
I1127 14:16:22.214586  2966 net.cpp:157] Top shape: 6 480 7 7 (141120)
I1127 14:16:22.214593  2966 net.cpp:157] Top shape: 6 480 7 7 (141120)
I1127 14:16:22.214599  2966 net.cpp:157] Top shape: 6 480 7 7 (141120)
I1127 14:16:22.214604  2966 net.cpp:157] Top shape: 6 480 7 7 (141120)
I1127 14:16:22.214609  2966 net.cpp:165] Memory required for data: 45930360
I1127 14:16:22.214614  2966 layer_factory.hpp:77] Creating layer Convolution15
I1127 14:16:22.214622  2966 net.cpp:100] Creating Layer Convolution15
I1127 14:16:22.214627  2966 net.cpp:434] Convolution15 <- Pooling5_Pooling5_0_split_0
I1127 14:16:22.214635  2966 net.cpp:408] Convolution15 -> Convolution15
I1127 14:16:22.215315  2966 net.cpp:150] Setting up Convolution15
I1127 14:16:22.215328  2966 net.cpp:157] Top shape: 6 192 7 7 (56448)
I1127 14:16:22.215333  2966 net.cpp:165] Memory required for data: 46156152
I1127 14:16:22.215340  2966 layer_factory.hpp:77] Creating layer ReLU15
I1127 14:16:22.215348  2966 net.cpp:100] Creating Layer ReLU15
I1127 14:16:22.215353  2966 net.cpp:434] ReLU15 <- Convolution15
I1127 14:16:22.215360  2966 net.cpp:395] ReLU15 -> Convolution15 (in-place)
I1127 14:16:22.215368  2966 net.cpp:150] Setting up ReLU15
I1127 14:16:22.215374  2966 net.cpp:157] Top shape: 6 192 7 7 (56448)
I1127 14:16:22.215379  2966 net.cpp:165] Memory required for data: 46381944
I1127 14:16:22.215384  2966 layer_factory.hpp:77] Creating layer Convolution16
I1127 14:16:22.215392  2966 net.cpp:100] Creating Layer Convolution16
I1127 14:16:22.215397  2966 net.cpp:434] Convolution16 <- Pooling5_Pooling5_0_split_1
I1127 14:16:22.215405  2966 net.cpp:408] Convolution16 -> Convolution16
I1127 14:16:22.215720  2966 net.cpp:150] Setting up Convolution16
I1127 14:16:22.215728  2966 net.cpp:157] Top shape: 6 96 7 7 (28224)
I1127 14:16:22.215734  2966 net.cpp:165] Memory required for data: 46494840
I1127 14:16:22.215740  2966 layer_factory.hpp:77] Creating layer ReLU16
I1127 14:16:22.215746  2966 net.cpp:100] Creating Layer ReLU16
I1127 14:16:22.215751  2966 net.cpp:434] ReLU16 <- Convolution16
I1127 14:16:22.215757  2966 net.cpp:395] ReLU16 -> Convolution16 (in-place)
I1127 14:16:22.215764  2966 net.cpp:150] Setting up ReLU16
I1127 14:16:22.215770  2966 net.cpp:157] Top shape: 6 96 7 7 (28224)
I1127 14:16:22.215775  2966 net.cpp:165] Memory required for data: 46607736
I1127 14:16:22.215780  2966 layer_factory.hpp:77] Creating layer Convolution17
I1127 14:16:22.215786  2966 net.cpp:100] Creating Layer Convolution17
I1127 14:16:22.215791  2966 net.cpp:434] Convolution17 <- Convolution16
I1127 14:16:22.229843  2966 net.cpp:408] Convolution17 -> Convolution17
I1127 14:16:22.231119  2966 net.cpp:150] Setting up Convolution17
I1127 14:16:22.231130  2966 net.cpp:157] Top shape: 6 208 7 7 (61152)
I1127 14:16:22.231133  2966 net.cpp:165] Memory required for data: 46852344
I1127 14:16:22.231144  2966 layer_factory.hpp:77] Creating layer ReLU17
I1127 14:16:22.231150  2966 net.cpp:100] Creating Layer ReLU17
I1127 14:16:22.231154  2966 net.cpp:434] ReLU17 <- Convolution17
I1127 14:16:22.231170  2966 net.cpp:395] ReLU17 -> Convolution17 (in-place)
I1127 14:16:22.231178  2966 net.cpp:150] Setting up ReLU17
I1127 14:16:22.231181  2966 net.cpp:157] Top shape: 6 208 7 7 (61152)
I1127 14:16:22.231184  2966 net.cpp:165] Memory required for data: 47096952
I1127 14:16:22.231186  2966 layer_factory.hpp:77] Creating layer Convolution18
I1127 14:16:22.231192  2966 net.cpp:100] Creating Layer Convolution18
I1127 14:16:22.231196  2966 net.cpp:434] Convolution18 <- Pooling5_Pooling5_0_split_2
I1127 14:16:22.231200  2966 net.cpp:408] Convolution18 -> Convolution18
I1127 14:16:22.231261  2966 net.cpp:150] Setting up Convolution18
I1127 14:16:22.231266  2966 net.cpp:157] Top shape: 6 16 7 7 (4704)
I1127 14:16:22.231267  2966 net.cpp:165] Memory required for data: 47115768
I1127 14:16:22.231271  2966 layer_factory.hpp:77] Creating layer ReLU18
I1127 14:16:22.231276  2966 net.cpp:100] Creating Layer ReLU18
I1127 14:16:22.231278  2966 net.cpp:434] ReLU18 <- Convolution18
I1127 14:16:22.231282  2966 net.cpp:395] ReLU18 -> Convolution18 (in-place)
I1127 14:16:22.231287  2966 net.cpp:150] Setting up ReLU18
I1127 14:16:22.231290  2966 net.cpp:157] Top shape: 6 16 7 7 (4704)
I1127 14:16:22.231292  2966 net.cpp:165] Memory required for data: 47134584
I1127 14:16:22.231295  2966 layer_factory.hpp:77] Creating layer Convolution19
I1127 14:16:22.231300  2966 net.cpp:100] Creating Layer Convolution19
I1127 14:16:22.231303  2966 net.cpp:434] Convolution19 <- Convolution18
I1127 14:16:22.231308  2966 net.cpp:408] Convolution19 -> Convolution19
I1127 14:16:22.231494  2966 net.cpp:150] Setting up Convolution19
I1127 14:16:22.231513  2966 net.cpp:157] Top shape: 6 48 7 7 (14112)
I1127 14:16:22.231516  2966 net.cpp:165] Memory required for data: 47191032
I1127 14:16:22.231521  2966 layer_factory.hpp:77] Creating layer ReLU19
I1127 14:16:22.231524  2966 net.cpp:100] Creating Layer ReLU19
I1127 14:16:22.231528  2966 net.cpp:434] ReLU19 <- Convolution19
I1127 14:16:22.231533  2966 net.cpp:395] ReLU19 -> Convolution19 (in-place)
I1127 14:16:22.231536  2966 net.cpp:150] Setting up ReLU19
I1127 14:16:22.231539  2966 net.cpp:157] Top shape: 6 48 7 7 (14112)
I1127 14:16:22.231542  2966 net.cpp:165] Memory required for data: 47247480
I1127 14:16:22.231544  2966 layer_factory.hpp:77] Creating layer Pooling6
I1127 14:16:22.231549  2966 net.cpp:100] Creating Layer Pooling6
I1127 14:16:22.231552  2966 net.cpp:434] Pooling6 <- Pooling5_Pooling5_0_split_3
I1127 14:16:22.231561  2966 net.cpp:408] Pooling6 -> Pooling6
I1127 14:16:22.231570  2966 net.cpp:150] Setting up Pooling6
I1127 14:16:22.231572  2966 net.cpp:157] Top shape: 6 480 7 7 (141120)
I1127 14:16:22.231575  2966 net.cpp:165] Memory required for data: 47811960
I1127 14:16:22.231577  2966 layer_factory.hpp:77] Creating layer Convolution20
I1127 14:16:22.231585  2966 net.cpp:100] Creating Layer Convolution20
I1127 14:16:22.231586  2966 net.cpp:434] Convolution20 <- Pooling6
I1127 14:16:22.231592  2966 net.cpp:408] Convolution20 -> Convolution20
I1127 14:16:22.231801  2966 net.cpp:150] Setting up Convolution20
I1127 14:16:22.231806  2966 net.cpp:157] Top shape: 6 64 7 7 (18816)
I1127 14:16:22.231808  2966 net.cpp:165] Memory required for data: 47887224
I1127 14:16:22.231813  2966 layer_factory.hpp:77] Creating layer ReLU20
I1127 14:16:22.231817  2966 net.cpp:100] Creating Layer ReLU20
I1127 14:16:22.231820  2966 net.cpp:434] ReLU20 <- Convolution20
I1127 14:16:22.231824  2966 net.cpp:395] ReLU20 -> Convolution20 (in-place)
I1127 14:16:22.231827  2966 net.cpp:150] Setting up ReLU20
I1127 14:16:22.231832  2966 net.cpp:157] Top shape: 6 64 7 7 (18816)
I1127 14:16:22.231833  2966 net.cpp:165] Memory required for data: 47962488
I1127 14:16:22.231837  2966 layer_factory.hpp:77] Creating layer Concat3
I1127 14:16:22.231840  2966 net.cpp:100] Creating Layer Concat3
I1127 14:16:22.231842  2966 net.cpp:434] Concat3 <- Convolution15
I1127 14:16:22.234550  2966 net.cpp:434] Concat3 <- Convolution17
I1127 14:16:22.234566  2966 net.cpp:434] Concat3 <- Convolution19
I1127 14:16:22.234573  2966 net.cpp:434] Concat3 <- Convolution20
I1127 14:16:22.234606  2966 net.cpp:408] Concat3 -> Concat3
I1127 14:16:22.234622  2966 net.cpp:150] Setting up Concat3
I1127 14:16:22.234632  2966 net.cpp:157] Top shape: 6 512 7 7 (150528)
I1127 14:16:22.234637  2966 net.cpp:165] Memory required for data: 48564600
I1127 14:16:22.234642  2966 layer_factory.hpp:77] Creating layer Pooling7
I1127 14:16:22.234650  2966 net.cpp:100] Creating Layer Pooling7
I1127 14:16:22.234657  2966 net.cpp:434] Pooling7 <- Concat3
I1127 14:16:22.234664  2966 net.cpp:408] Pooling7 -> Pooling7
I1127 14:16:22.234680  2966 net.cpp:150] Setting up Pooling7
I1127 14:16:22.234688  2966 net.cpp:157] Top shape: 6 512 1 1 (3072)
I1127 14:16:22.234694  2966 net.cpp:165] Memory required for data: 48576888
I1127 14:16:22.234699  2966 layer_factory.hpp:77] Creating layer InnerProduct1
I1127 14:16:22.234715  2966 net.cpp:100] Creating Layer InnerProduct1
I1127 14:16:22.234720  2966 net.cpp:434] InnerProduct1 <- Pooling7
I1127 14:16:22.234729  2966 net.cpp:408] InnerProduct1 -> InnerProduct1
I1127 14:16:22.257145  2966 net.cpp:150] Setting up InnerProduct1
I1127 14:16:22.265610  2966 net.cpp:157] Top shape: 6 4096 (24576)
I1127 14:16:22.265625  2966 net.cpp:165] Memory required for data: 48675192
I1127 14:16:22.265637  2966 layer_factory.hpp:77] Creating layer ReLU21
I1127 14:16:22.265650  2966 net.cpp:100] Creating Layer ReLU21
I1127 14:16:22.265656  2966 net.cpp:434] ReLU21 <- InnerProduct1
I1127 14:16:22.265663  2966 net.cpp:395] ReLU21 -> InnerProduct1 (in-place)
I1127 14:16:22.265676  2966 net.cpp:150] Setting up ReLU21
I1127 14:16:22.265681  2966 net.cpp:157] Top shape: 6 4096 (24576)
I1127 14:16:22.265686  2966 net.cpp:165] Memory required for data: 48773496
I1127 14:16:22.265692  2966 layer_factory.hpp:77] Creating layer Dropout1
I1127 14:16:22.265699  2966 net.cpp:100] Creating Layer Dropout1
I1127 14:16:22.265705  2966 net.cpp:434] Dropout1 <- InnerProduct1
I1127 14:16:22.265714  2966 net.cpp:395] Dropout1 -> InnerProduct1 (in-place)
I1127 14:16:22.265733  2966 net.cpp:150] Setting up Dropout1
I1127 14:16:22.265739  2966 net.cpp:157] Top shape: 6 4096 (24576)
I1127 14:16:22.265745  2966 net.cpp:165] Memory required for data: 48871800
I1127 14:16:22.265750  2966 layer_factory.hpp:77] Creating layer InnerProduct2
I1127 14:16:22.265758  2966 net.cpp:100] Creating Layer InnerProduct2
I1127 14:16:22.265763  2966 net.cpp:434] InnerProduct2 <- InnerProduct1
I1127 14:16:22.265770  2966 net.cpp:408] InnerProduct2 -> InnerProduct2
I1127 14:16:22.341938  2966 net.cpp:150] Setting up InnerProduct2
I1127 14:16:22.345587  2966 net.cpp:157] Top shape: 6 1024 (6144)
I1127 14:16:22.345636  2966 net.cpp:165] Memory required for data: 48896376
I1127 14:16:22.345682  2966 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1127 14:16:22.349133  2966 net.cpp:100] Creating Layer SoftmaxWithLoss1
I1127 14:16:22.349148  2966 net.cpp:434] SoftmaxWithLoss1 <- InnerProduct2
I1127 14:16:22.349153  2966 net.cpp:434] SoftmaxWithLoss1 <- Data2
I1127 14:16:22.349160  2966 net.cpp:408] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1127 14:16:22.349750  2966 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1127 14:16:22.349802  2966 net.cpp:150] Setting up SoftmaxWithLoss1
I1127 14:16:22.349807  2966 net.cpp:157] Top shape: (1)
I1127 14:16:22.349810  2966 net.cpp:160]     with loss weight 1
I1127 14:16:22.349824  2966 net.cpp:165] Memory required for data: 48896380
I1127 14:16:22.349828  2966 net.cpp:226] SoftmaxWithLoss1 needs backward computation.
I1127 14:16:22.349833  2966 net.cpp:226] InnerProduct2 needs backward computation.
I1127 14:16:22.349836  2966 net.cpp:226] Dropout1 needs backward computation.
I1127 14:16:22.349838  2966 net.cpp:226] ReLU21 needs backward computation.
I1127 14:16:22.349841  2966 net.cpp:226] InnerProduct1 needs backward computation.
I1127 14:16:22.349844  2966 net.cpp:226] Pooling7 needs backward computation.
I1127 14:16:22.349846  2966 net.cpp:226] Concat3 needs backward computation.
I1127 14:16:22.349851  2966 net.cpp:226] ReLU20 needs backward computation.
I1127 14:16:22.349869  2966 net.cpp:226] Convolution20 needs backward computation.
I1127 14:16:22.349872  2966 net.cpp:226] Pooling6 needs backward computation.
I1127 14:16:22.349875  2966 net.cpp:226] ReLU19 needs backward computation.
I1127 14:16:22.349879  2966 net.cpp:226] Convolution19 needs backward computation.
I1127 14:16:22.349881  2966 net.cpp:226] ReLU18 needs backward computation.
I1127 14:16:22.349884  2966 net.cpp:226] Convolution18 needs backward computation.
I1127 14:16:22.349887  2966 net.cpp:226] ReLU17 needs backward computation.
I1127 14:16:22.349889  2966 net.cpp:226] Convolution17 needs backward computation.
I1127 14:16:22.349892  2966 net.cpp:226] ReLU16 needs backward computation.
I1127 14:16:22.349895  2966 net.cpp:226] Convolution16 needs backward computation.
I1127 14:16:22.349898  2966 net.cpp:226] ReLU15 needs backward computation.
I1127 14:16:22.349900  2966 net.cpp:226] Convolution15 needs backward computation.
I1127 14:16:22.349905  2966 net.cpp:226] Pooling5_Pooling5_0_split needs backward computation.
I1127 14:16:22.349906  2966 net.cpp:226] Pooling5 needs backward computation.
I1127 14:16:22.349910  2966 net.cpp:226] Concat2 needs backward computation.
I1127 14:16:22.349926  2966 net.cpp:226] ReLU14 needs backward computation.
I1127 14:16:22.349930  2966 net.cpp:226] Convolution14 needs backward computation.
I1127 14:16:22.349932  2966 net.cpp:226] Pooling4 needs backward computation.
I1127 14:16:22.349936  2966 net.cpp:226] ReLU13 needs backward computation.
I1127 14:16:22.349938  2966 net.cpp:226] Convolution13 needs backward computation.
I1127 14:16:22.349941  2966 net.cpp:226] ReLU12 needs backward computation.
I1127 14:16:22.349944  2966 net.cpp:226] Convolution12 needs backward computation.
I1127 14:16:22.349947  2966 net.cpp:226] ReLU11 needs backward computation.
I1127 14:16:22.349949  2966 net.cpp:226] Convolution11 needs backward computation.
I1127 14:16:22.349952  2966 net.cpp:226] ReLU10 needs backward computation.
I1127 14:16:22.349956  2966 net.cpp:226] Convolution10 needs backward computation.
I1127 14:16:22.349958  2966 net.cpp:226] ReLU9 needs backward computation.
I1127 14:16:22.349961  2966 net.cpp:226] Convolution9 needs backward computation.
I1127 14:16:22.349963  2966 net.cpp:226] Concat1_Concat1_0_split needs backward computation.
I1127 14:16:22.349967  2966 net.cpp:226] Concat1 needs backward computation.
I1127 14:16:22.349970  2966 net.cpp:226] ReLU8 needs backward computation.
I1127 14:16:22.349973  2966 net.cpp:226] Convolution8 needs backward computation.
I1127 14:16:22.349977  2966 net.cpp:226] ReLU7 needs backward computation.
I1127 14:16:22.349978  2966 net.cpp:226] Convolution7 needs backward computation.
I1127 14:16:22.349982  2966 net.cpp:226] Pooling3 needs backward computation.
I1127 14:16:22.349987  2966 net.cpp:226] ReLU6 needs backward computation.
I1127 14:16:22.349989  2966 net.cpp:226] Convolution6 needs backward computation.
I1127 14:16:22.349992  2966 net.cpp:226] ReLU5 needs backward computation.
I1127 14:16:22.349994  2966 net.cpp:226] Convolution5 needs backward computation.
I1127 14:16:22.349997  2966 net.cpp:226] ReLU4 needs backward computation.
I1127 14:16:22.349999  2966 net.cpp:226] Convolution4 needs backward computation.
I1127 14:16:22.350003  2966 net.cpp:226] ReLU3 needs backward computation.
I1127 14:16:22.350005  2966 net.cpp:226] Convolution3 needs backward computation.
I1127 14:16:22.350008  2966 net.cpp:226] Pooling2_Pooling2_0_split needs backward computation.
I1127 14:16:22.350010  2966 net.cpp:226] Pooling2 needs backward computation.
I1127 14:16:22.350014  2966 net.cpp:226] ReLU2 needs backward computation.
I1127 14:16:22.350016  2966 net.cpp:226] Convolution2 needs backward computation.
I1127 14:16:22.350019  2966 net.cpp:226] Pooling1 needs backward computation.
I1127 14:16:22.350023  2966 net.cpp:226] ReLU1 needs backward computation.
I1127 14:16:22.350025  2966 net.cpp:226] Convolution1 needs backward computation.
I1127 14:16:22.350029  2966 net.cpp:228] Data1 does not need backward computation.
I1127 14:16:22.350031  2966 net.cpp:270] This network produces output SoftmaxWithLoss1
I1127 14:16:22.350065  2966 net.cpp:283] Network initialization done.
I1127 14:16:22.351076  2966 solver.cpp:181] Creating test net (#0) specified by test_net file: /home/caffe/caffe/examples/myfile/myfile_nin/val.prototxt
I1127 14:16:22.351372  2966 net.cpp:58] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    mirror: false
    crop_size: 114
    mean_file: "/home/caffe/caffe/examples/myfile/myfile_nin/mean.binaryproto"
  }
  data_param {
    source: "/home/caffe/caffe/examples/myfile/myfile_nin/img_test_lmdb"
    batch_size: 4
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution3"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.09
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Convolution3"
  top: "Convolution3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Convolution3"
  top: "Convolution4"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution5"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.2
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Convolution5"
  top: "Convolution5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Convolution5"
  top: "Convolution6"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Pooling2"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Pooling3"
  top: "Convolution7"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Convolution7"
  top: "Convolution7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Pooling2"
  top: "Convolution8"
  param {
    lr_mult: 0.1
    decay_mult: 1
  }
  param {
    lr_mult: 0.2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Convolution4"
  bottom: "Convolution6"
  bottom: "Convolution7"
  bottom: "Convolution8"
  top: "Concat1"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.09
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.2
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Pooling4"
  type: "Pooling"
  bottom: "Concat1"
  top: "Pooling4"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Pooling4"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Convolution9"
  bottom: "Convolution11"
  bottom: "Convolution13"
  bottom: "Convolution14"
  top: "Concat2"
}
layer {
  name: "Pooling5"
  type: "Pooling"
  bottom: "Concat2"
  top: "Pooling5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Pooling5"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 192
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Pooling5"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.09
    }
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 208
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Pooling5"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.2
    }
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    pad: 2
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Pooling6"
  type: "Pooling"
  bottom: "Pooling5"
  top: "Pooling6"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 1
    pad: 1
  }
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Pooling6"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Convolution15"
  bottom: "Convolution17"
  bottom: "Convolution19"
  bottom: "Convolution20"
  top: "Concat3"
}
layer {
  name: "Pooling7"
  type: "Pooling"
  bottom: "Concat3"
  top: "Pooling7"
  pooling_param {
    pool: AVE
    kernel_size: 7
    stride: 1
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling7"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
  dropout_param {
    dropout_ratio: 0.4
  }
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  param {
    lr_mult: 10
    decay_mult: 1
  }
  param {
    lr_mult: 20
    decay_mult: 0
  }
  inner_product_param {
    num_output: 1024
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct2"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct2"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I1127 14:16:22.352341  2966 layer_factory.hpp:77] Creating layer Data1
I1127 14:16:22.352392  2966 net.cpp:100] Creating Layer Data1
I1127 14:16:22.352409  2966 net.cpp:408] Data1 -> Data1
I1127 14:16:22.352453  2966 net.cpp:408] Data1 -> Data2
I1127 14:16:22.352468  2966 data_transformer.cpp:25] Loading mean file from: /home/caffe/caffe/examples/myfile/myfile_nin/mean.binaryproto
I1127 14:16:22.357266  2970 db_lmdb.cpp:35] Opened lmdb /home/caffe/caffe/examples/myfile/myfile_nin/img_test_lmdb
I1127 14:16:22.358213  2966 data_layer.cpp:41] output data size: 4,1,114,114
I1127 14:16:22.358496  2966 net.cpp:150] Setting up Data1
I1127 14:16:22.358551  2966 net.cpp:157] Top shape: 4 1 114 114 (51984)
I1127 14:16:22.358593  2966 net.cpp:157] Top shape: 4 (4)
I1127 14:16:22.358633  2966 net.cpp:165] Memory required for data: 207952
I1127 14:16:22.358677  2966 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I1127 14:16:22.358814  2966 net.cpp:100] Creating Layer Data2_Data1_1_split
I1127 14:16:22.358857  2966 net.cpp:434] Data2_Data1_1_split <- Data2
I1127 14:16:22.358901  2966 net.cpp:408] Data2_Data1_1_split -> Data2_Data1_1_split_0
I1127 14:16:22.358958  2966 net.cpp:408] Data2_Data1_1_split -> Data2_Data1_1_split_1
I1127 14:16:22.359006  2966 net.cpp:150] Setting up Data2_Data1_1_split
I1127 14:16:22.359050  2966 net.cpp:157] Top shape: 4 (4)
I1127 14:16:22.359089  2966 net.cpp:157] Top shape: 4 (4)
I1127 14:16:22.359127  2966 net.cpp:165] Memory required for data: 207984
I1127 14:16:22.359166  2966 layer_factory.hpp:77] Creating layer Convolution1
I1127 14:16:22.359212  2966 net.cpp:100] Creating Layer Convolution1
I1127 14:16:22.359252  2966 net.cpp:434] Convolution1 <- Data1
I1127 14:16:22.359283  2966 net.cpp:408] Convolution1 -> Convolution1
I1127 14:16:22.359329  2966 net.cpp:150] Setting up Convolution1
I1127 14:16:22.359338  2966 net.cpp:157] Top shape: 4 64 57 57 (831744)
I1127 14:16:22.359344  2966 net.cpp:165] Memory required for data: 3534960
I1127 14:16:22.359354  2966 layer_factory.hpp:77] Creating layer ReLU1
I1127 14:16:22.359361  2966 net.cpp:100] Creating Layer ReLU1
I1127 14:16:22.359367  2966 net.cpp:434] ReLU1 <- Convolution1
I1127 14:16:22.359374  2966 net.cpp:395] ReLU1 -> Convolution1 (in-place)
I1127 14:16:22.359381  2966 net.cpp:150] Setting up ReLU1
I1127 14:16:22.359387  2966 net.cpp:157] Top shape: 4 64 57 57 (831744)
I1127 14:16:22.359393  2966 net.cpp:165] Memory required for data: 6861936
I1127 14:16:22.359398  2966 layer_factory.hpp:77] Creating layer Pooling1
I1127 14:16:22.359406  2966 net.cpp:100] Creating Layer Pooling1
I1127 14:16:22.359412  2966 net.cpp:434] Pooling1 <- Convolution1
I1127 14:16:22.359418  2966 net.cpp:408] Pooling1 -> Pooling1
I1127 14:16:22.359428  2966 net.cpp:150] Setting up Pooling1
I1127 14:16:22.359434  2966 net.cpp:157] Top shape: 4 64 28 28 (200704)
I1127 14:16:22.359453  2966 net.cpp:165] Memory required for data: 7664752
I1127 14:16:22.359460  2966 layer_factory.hpp:77] Creating layer Convolution2
I1127 14:16:22.359470  2966 net.cpp:100] Creating Layer Convolution2
I1127 14:16:22.359475  2966 net.cpp:434] Convolution2 <- Pooling1
I1127 14:16:22.359483  2966 net.cpp:408] Convolution2 -> Convolution2
I1127 14:16:22.360258  2966 net.cpp:150] Setting up Convolution2
I1127 14:16:22.360270  2966 net.cpp:157] Top shape: 4 192 28 28 (602112)
I1127 14:16:22.360275  2966 net.cpp:165] Memory required for data: 10073200
I1127 14:16:22.360285  2966 layer_factory.hpp:77] Creating layer ReLU2
I1127 14:16:22.360291  2966 net.cpp:100] Creating Layer ReLU2
I1127 14:16:22.360297  2966 net.cpp:434] ReLU2 <- Convolution2
I1127 14:16:22.360306  2966 net.cpp:395] ReLU2 -> Convolution2 (in-place)
I1127 14:16:22.360313  2966 net.cpp:150] Setting up ReLU2
I1127 14:16:22.360319  2966 net.cpp:157] Top shape: 4 192 28 28 (602112)
I1127 14:16:22.360324  2966 net.cpp:165] Memory required for data: 12481648
I1127 14:16:22.360330  2966 layer_factory.hpp:77] Creating layer Pooling2
I1127 14:16:22.360337  2966 net.cpp:100] Creating Layer Pooling2
I1127 14:16:22.360342  2966 net.cpp:434] Pooling2 <- Convolution2
I1127 14:16:22.360348  2966 net.cpp:408] Pooling2 -> Pooling2
I1127 14:16:22.360374  2966 net.cpp:150] Setting up Pooling2
I1127 14:16:22.360381  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.360386  2966 net.cpp:165] Memory required for data: 13083760
I1127 14:16:22.360391  2966 layer_factory.hpp:77] Creating layer Pooling2_Pooling2_0_split
I1127 14:16:22.360399  2966 net.cpp:100] Creating Layer Pooling2_Pooling2_0_split
I1127 14:16:22.360404  2966 net.cpp:434] Pooling2_Pooling2_0_split <- Pooling2
I1127 14:16:22.360410  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_0
I1127 14:16:22.360420  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_1
I1127 14:16:22.360427  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_2
I1127 14:16:22.360435  2966 net.cpp:408] Pooling2_Pooling2_0_split -> Pooling2_Pooling2_0_split_3
I1127 14:16:22.360455  2966 net.cpp:150] Setting up Pooling2_Pooling2_0_split
I1127 14:16:22.360465  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.360471  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.360477  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.360482  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.360488  2966 net.cpp:165] Memory required for data: 15492208
I1127 14:16:22.360493  2966 layer_factory.hpp:77] Creating layer Convolution3
I1127 14:16:22.360502  2966 net.cpp:100] Creating Layer Convolution3
I1127 14:16:22.360508  2966 net.cpp:434] Convolution3 <- Pooling2_Pooling2_0_split_0
I1127 14:16:22.360517  2966 net.cpp:408] Convolution3 -> Convolution3
I1127 14:16:22.360657  2966 net.cpp:150] Setting up Convolution3
I1127 14:16:22.360666  2966 net.cpp:157] Top shape: 4 96 14 14 (75264)
I1127 14:16:22.360671  2966 net.cpp:165] Memory required for data: 15793264
I1127 14:16:22.360680  2966 layer_factory.hpp:77] Creating layer ReLU3
I1127 14:16:22.360687  2966 net.cpp:100] Creating Layer ReLU3
I1127 14:16:22.360692  2966 net.cpp:434] ReLU3 <- Convolution3
I1127 14:16:22.360698  2966 net.cpp:395] ReLU3 -> Convolution3 (in-place)
I1127 14:16:22.360705  2966 net.cpp:150] Setting up ReLU3
I1127 14:16:22.360712  2966 net.cpp:157] Top shape: 4 96 14 14 (75264)
I1127 14:16:22.360716  2966 net.cpp:165] Memory required for data: 16094320
I1127 14:16:22.360721  2966 layer_factory.hpp:77] Creating layer Convolution4
I1127 14:16:22.360730  2966 net.cpp:100] Creating Layer Convolution4
I1127 14:16:22.361508  2966 net.cpp:434] Convolution4 <- Convolution3
I1127 14:16:22.361524  2966 net.cpp:408] Convolution4 -> Convolution4
I1127 14:16:22.362303  2966 net.cpp:150] Setting up Convolution4
I1127 14:16:22.365607  2966 net.cpp:157] Top shape: 4 128 14 14 (100352)
I1127 14:16:22.365628  2966 net.cpp:165] Memory required for data: 16495728
I1127 14:16:22.365638  2966 layer_factory.hpp:77] Creating layer ReLU4
I1127 14:16:22.365645  2966 net.cpp:100] Creating Layer ReLU4
I1127 14:16:22.365651  2966 net.cpp:434] ReLU4 <- Convolution4
I1127 14:16:22.365658  2966 net.cpp:395] ReLU4 -> Convolution4 (in-place)
I1127 14:16:22.365666  2966 net.cpp:150] Setting up ReLU4
I1127 14:16:22.365672  2966 net.cpp:157] Top shape: 4 128 14 14 (100352)
I1127 14:16:22.365677  2966 net.cpp:165] Memory required for data: 16897136
I1127 14:16:22.365684  2966 layer_factory.hpp:77] Creating layer Convolution5
I1127 14:16:22.365694  2966 net.cpp:100] Creating Layer Convolution5
I1127 14:16:22.365700  2966 net.cpp:434] Convolution5 <- Pooling2_Pooling2_0_split_1
I1127 14:16:22.365707  2966 net.cpp:408] Convolution5 -> Convolution5
I1127 14:16:22.365746  2966 net.cpp:150] Setting up Convolution5
I1127 14:16:22.365753  2966 net.cpp:157] Top shape: 4 16 14 14 (12544)
I1127 14:16:22.365758  2966 net.cpp:165] Memory required for data: 16947312
I1127 14:16:22.365768  2966 layer_factory.hpp:77] Creating layer ReLU5
I1127 14:16:22.365775  2966 net.cpp:100] Creating Layer ReLU5
I1127 14:16:22.365780  2966 net.cpp:434] ReLU5 <- Convolution5
I1127 14:16:22.365787  2966 net.cpp:395] ReLU5 -> Convolution5 (in-place)
I1127 14:16:22.365794  2966 net.cpp:150] Setting up ReLU5
I1127 14:16:22.366243  2966 net.cpp:157] Top shape: 4 16 14 14 (12544)
I1127 14:16:22.366251  2966 net.cpp:165] Memory required for data: 16997488
I1127 14:16:22.366257  2966 layer_factory.hpp:77] Creating layer Convolution6
I1127 14:16:22.366266  2966 net.cpp:100] Creating Layer Convolution6
I1127 14:16:22.366272  2966 net.cpp:434] Convolution6 <- Convolution5
I1127 14:16:22.366293  2966 net.cpp:408] Convolution6 -> Convolution6
I1127 14:16:22.366396  2966 net.cpp:150] Setting up Convolution6
I1127 14:16:22.366405  2966 net.cpp:157] Top shape: 4 32 14 14 (25088)
I1127 14:16:22.366410  2966 net.cpp:165] Memory required for data: 17097840
I1127 14:16:22.366416  2966 layer_factory.hpp:77] Creating layer ReLU6
I1127 14:16:22.366425  2966 net.cpp:100] Creating Layer ReLU6
I1127 14:16:22.366431  2966 net.cpp:434] ReLU6 <- Convolution6
I1127 14:16:22.366436  2966 net.cpp:395] ReLU6 -> Convolution6 (in-place)
I1127 14:16:22.366443  2966 net.cpp:150] Setting up ReLU6
I1127 14:16:22.366449  2966 net.cpp:157] Top shape: 4 32 14 14 (25088)
I1127 14:16:22.366454  2966 net.cpp:165] Memory required for data: 17198192
I1127 14:16:22.366474  2966 layer_factory.hpp:77] Creating layer Pooling3
I1127 14:16:22.366483  2966 net.cpp:100] Creating Layer Pooling3
I1127 14:16:22.366488  2966 net.cpp:434] Pooling3 <- Pooling2_Pooling2_0_split_2
I1127 14:16:22.366495  2966 net.cpp:408] Pooling3 -> Pooling3
I1127 14:16:22.366506  2966 net.cpp:150] Setting up Pooling3
I1127 14:16:22.366515  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.366520  2966 net.cpp:165] Memory required for data: 17800304
I1127 14:16:22.366525  2966 layer_factory.hpp:77] Creating layer Convolution7
I1127 14:16:22.366533  2966 net.cpp:100] Creating Layer Convolution7
I1127 14:16:22.366539  2966 net.cpp:434] Convolution7 <- Pooling3
I1127 14:16:22.366546  2966 net.cpp:408] Convolution7 -> Convolution7
I1127 14:16:22.366605  2966 net.cpp:150] Setting up Convolution7
I1127 14:16:22.366665  2966 net.cpp:157] Top shape: 4 32 14 14 (25088)
I1127 14:16:22.366672  2966 net.cpp:165] Memory required for data: 17900656
I1127 14:16:22.366679  2966 layer_factory.hpp:77] Creating layer ReLU7
I1127 14:16:22.366686  2966 net.cpp:100] Creating Layer ReLU7
I1127 14:16:22.366691  2966 net.cpp:434] ReLU7 <- Convolution7
I1127 14:16:22.366698  2966 net.cpp:395] ReLU7 -> Convolution7 (in-place)
I1127 14:16:22.366828  2966 net.cpp:150] Setting up ReLU7
I1127 14:16:22.366837  2966 net.cpp:157] Top shape: 4 32 14 14 (25088)
I1127 14:16:22.366842  2966 net.cpp:165] Memory required for data: 18001008
I1127 14:16:22.366847  2966 layer_factory.hpp:77] Creating layer Convolution8
I1127 14:16:22.366857  2966 net.cpp:100] Creating Layer Convolution8
I1127 14:16:22.366861  2966 net.cpp:434] Convolution8 <- Pooling2_Pooling2_0_split_3
I1127 14:16:22.366870  2966 net.cpp:408] Convolution8 -> Convolution8
I1127 14:16:22.366996  2966 net.cpp:150] Setting up Convolution8
I1127 14:16:22.367204  2966 net.cpp:157] Top shape: 4 64 14 14 (50176)
I1127 14:16:22.367210  2966 net.cpp:165] Memory required for data: 18201712
I1127 14:16:22.367218  2966 layer_factory.hpp:77] Creating layer ReLU8
I1127 14:16:22.367226  2966 net.cpp:100] Creating Layer ReLU8
I1127 14:16:22.367296  2966 net.cpp:434] ReLU8 <- Convolution8
I1127 14:16:22.367305  2966 net.cpp:395] ReLU8 -> Convolution8 (in-place)
I1127 14:16:22.367311  2966 net.cpp:150] Setting up ReLU8
I1127 14:16:22.367317  2966 net.cpp:157] Top shape: 4 64 14 14 (50176)
I1127 14:16:22.367322  2966 net.cpp:165] Memory required for data: 18402416
I1127 14:16:22.367327  2966 layer_factory.hpp:77] Creating layer Concat1
I1127 14:16:22.367334  2966 net.cpp:100] Creating Layer Concat1
I1127 14:16:22.367339  2966 net.cpp:434] Concat1 <- Convolution4
I1127 14:16:22.367347  2966 net.cpp:434] Concat1 <- Convolution6
I1127 14:16:22.367352  2966 net.cpp:434] Concat1 <- Convolution7
I1127 14:16:22.367357  2966 net.cpp:434] Concat1 <- Convolution8
I1127 14:16:22.367378  2966 net.cpp:408] Concat1 -> Concat1
I1127 14:16:22.367522  2966 net.cpp:150] Setting up Concat1
I1127 14:16:22.367540  2966 net.cpp:157] Top shape: 4 256 14 14 (200704)
I1127 14:16:22.367547  2966 net.cpp:165] Memory required for data: 19205232
I1127 14:16:22.367552  2966 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I1127 14:16:22.367559  2966 net.cpp:100] Creating Layer Concat1_Concat1_0_split
I1127 14:16:22.367564  2966 net.cpp:434] Concat1_Concat1_0_split <- Concat1
I1127 14:16:22.367573  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I1127 14:16:22.367580  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I1127 14:16:22.367588  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_2
I1127 14:16:22.367594  2966 net.cpp:408] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_3
I1127 14:16:22.367604  2966 net.cpp:150] Setting up Concat1_Concat1_0_split
I1127 14:16:22.367609  2966 net.cpp:157] Top shape: 4 256 14 14 (200704)
I1127 14:16:22.367615  2966 net.cpp:157] Top shape: 4 256 14 14 (200704)
I1127 14:16:22.367621  2966 net.cpp:157] Top shape: 4 256 14 14 (200704)
I1127 14:16:22.367626  2966 net.cpp:157] Top shape: 4 256 14 14 (200704)
I1127 14:16:22.367631  2966 net.cpp:165] Memory required for data: 22416496
I1127 14:16:22.367636  2966 layer_factory.hpp:77] Creating layer Convolution9
I1127 14:16:22.367713  2966 net.cpp:100] Creating Layer Convolution9
I1127 14:16:22.367719  2966 net.cpp:434] Convolution9 <- Concat1_Concat1_0_split_0
I1127 14:16:22.367727  2966 net.cpp:408] Convolution9 -> Convolution9
I1127 14:16:22.367982  2966 net.cpp:150] Setting up Convolution9
I1127 14:16:22.368094  2966 net.cpp:157] Top shape: 4 128 14 14 (100352)
I1127 14:16:22.368100  2966 net.cpp:165] Memory required for data: 22817904
I1127 14:16:22.368113  2966 layer_factory.hpp:77] Creating layer ReLU9
I1127 14:16:22.368120  2966 net.cpp:100] Creating Layer ReLU9
I1127 14:16:22.368125  2966 net.cpp:434] ReLU9 <- Convolution9
I1127 14:16:22.368132  2966 net.cpp:395] ReLU9 -> Convolution9 (in-place)
I1127 14:16:22.368140  2966 net.cpp:150] Setting up ReLU9
I1127 14:16:22.368146  2966 net.cpp:157] Top shape: 4 128 14 14 (100352)
I1127 14:16:22.368151  2966 net.cpp:165] Memory required for data: 23219312
I1127 14:16:22.368156  2966 layer_factory.hpp:77] Creating layer Convolution10
I1127 14:16:22.368165  2966 net.cpp:100] Creating Layer Convolution10
I1127 14:16:22.368171  2966 net.cpp:434] Convolution10 <- Concat1_Concat1_0_split_1
I1127 14:16:22.368180  2966 net.cpp:408] Convolution10 -> Convolution10
I1127 14:16:22.368502  2966 net.cpp:150] Setting up Convolution10
I1127 14:16:22.368515  2966 net.cpp:157] Top shape: 4 128 14 14 (100352)
I1127 14:16:22.368520  2966 net.cpp:165] Memory required for data: 23620720
I1127 14:16:22.368526  2966 layer_factory.hpp:77] Creating layer ReLU10
I1127 14:16:22.368533  2966 net.cpp:100] Creating Layer ReLU10
I1127 14:16:22.368538  2966 net.cpp:434] ReLU10 <- Convolution10
I1127 14:16:22.368546  2966 net.cpp:395] ReLU10 -> Convolution10 (in-place)
I1127 14:16:22.368553  2966 net.cpp:150] Setting up ReLU10
I1127 14:16:22.368559  2966 net.cpp:157] Top shape: 4 128 14 14 (100352)
I1127 14:16:22.373857  2966 net.cpp:165] Memory required for data: 24022128
I1127 14:16:22.373875  2966 layer_factory.hpp:77] Creating layer Convolution11
I1127 14:16:22.373894  2966 net.cpp:100] Creating Layer Convolution11
I1127 14:16:22.373903  2966 net.cpp:434] Convolution11 <- Convolution10
I1127 14:16:22.373917  2966 net.cpp:408] Convolution11 -> Convolution11
I1127 14:16:22.375530  2966 net.cpp:150] Setting up Convolution11
I1127 14:16:22.377524  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.377532  2966 net.cpp:165] Memory required for data: 24624240
I1127 14:16:22.377542  2966 layer_factory.hpp:77] Creating layer ReLU11
I1127 14:16:22.377549  2966 net.cpp:100] Creating Layer ReLU11
I1127 14:16:22.377557  2966 net.cpp:434] ReLU11 <- Convolution11
I1127 14:16:22.377563  2966 net.cpp:395] ReLU11 -> Convolution11 (in-place)
I1127 14:16:22.377573  2966 net.cpp:150] Setting up ReLU11
I1127 14:16:22.377578  2966 net.cpp:157] Top shape: 4 192 14 14 (150528)
I1127 14:16:22.377601  2966 net.cpp:165] Memory required for data: 25226352
I1127 14:16:22.377607  2966 layer_factory.hpp:77] Creating layer Convolution12
I1127 14:16:22.377619  2966 net.cpp:100] Creating Layer Convolution12
I1127 14:16:22.377624  2966 net.cpp:434] Convolution12 <- Concat1_Concat1_0_split_2
I1127 14:16:22.377632  2966 net.cpp:408] Convolution12 -> Convolution12
I1127 14:16:22.377707  2966 net.cpp:150] Setting up Convolution12
I1127 14:16:22.377715  2966 net.cpp:157] Top shape: 4 32 14 14 (25088)
I1127 14:16:22.377722  2966 net.cpp:165] Memory required for data: 25326704
I1127 14:16:22.377728  2966 layer_factory.hpp:77] Creating layer ReLU12
I1127 14:16:22.377734  2966 net.cpp:100] Creating Layer ReLU12
I1127 14:16:22.377740  2966 net.cpp:434] ReLU12 <- Convolution12
I1127 14:16:22.377746  2966 net.cpp:395] ReLU12 -> Convolution12 (in-place)
I1127 14:16:22.377753  2966 net.cpp:150] Setting up ReLU12
I1127 14:16:22.377760  2966 net.cpp:157] Top shape: 4 32 14 14 (25088)
I1127 14:16:22.377765  2966 net.cpp:165] Memory required for data: 25427056
I1127 14:16:22.377770  2966 layer_factory.hpp:77] Creating layer Convolution13
I1127 14:16:22.377785  2966 net.cpp:100] Creating Layer Convolution13
I1127 14:16:22.377790  2966 net.cpp:434] Convolution13 <- Convolution12
I1127 14:16:22.377799  2966 net.cpp:408] Convolution13 -> Convolution13
I1127 14:16:22.378504  2966 net.cpp:150] Setting up Convolution13
I1127 14:16:22.378543  2966 net.cpp:157] Top shape: 4 96 14 14 (75264)
I1127 14:16:22.378552  2966 net.cpp:165] Memory required for data: 25728112
I1127 14:16:22.378564  2966 layer_factory.hpp:77] Creating layer ReLU13
I1127 14:16:22.378572  2966 net.cpp:100] Creating Layer ReLU13
I1127 14:16:22.378579  2966 net.cpp:434] ReLU13 <- Convolution13
I1127 14:16:22.378587  2966 net.cpp:395] ReLU13 -> Convolution13 (in-place)
I1127 14:16:22.378597  2966 net.cpp:150] Setting up ReLU13
I1127 14:16:22.378603  2966 net.cpp:157] Top shape: 4 96 14 14 (75264)
I1127 14:16:22.378608  2966 net.cpp:165] Memory required for data: 26029168
I1127 14:16:22.378614  2966 layer_factory.hpp:77] Creating layer Pooling4
I1127 14:16:22.378621  2966 net.cpp:100] Creating Layer Pooling4
I1127 14:16:22.378628  2966 net.cpp:434] Pooling4 <- Concat1_Concat1_0_split_3
I1127 14:16:22.378635  2966 net.cpp:408] Pooling4 -> Pooling4
I1127 14:16:22.378645  2966 net.cpp:150] Setting up Pooling4
I1127 14:16:22.378653  2966 net.cpp:157] Top shape: 4 256 14 14 (200704)
I1127 14:16:22.378659  2966 net.cpp:165] Memory required for data: 26831984
I1127 14:16:22.378665  2966 layer_factory.hpp:77] Creating layer Convolution14
I1127 14:16:22.378676  2966 net.cpp:100] Creating Layer Convolution14
I1127 14:16:22.378681  2966 net.cpp:434] Convolution14 <- Pooling4
I1127 14:16:22.378691  2966 net.cpp:408] Convolution14 -> Convolution14
I1127 14:16:22.378816  2966 net.cpp:150] Setting up Convolution14
I1127 14:16:22.380033  2966 net.cpp:157] Top shape: 4 64 14 14 (50176)
I1127 14:16:22.380039  2966 net.cpp:165] Memory required for data: 27032688
I1127 14:16:22.380048  2966 layer_factory.hpp:77] Creating layer ReLU14
I1127 14:16:22.380054  2966 net.cpp:100] Creating Layer ReLU14
I1127 14:16:22.380060  2966 net.cpp:434] ReLU14 <- Convolution14
I1127 14:16:22.380066  2966 net.cpp:395] ReLU14 -> Convolution14 (in-place)
I1127 14:16:22.380074  2966 net.cpp:150] Setting up ReLU14
I1127 14:16:22.380080  2966 net.cpp:157] Top shape: 4 64 14 14 (50176)
I1127 14:16:22.380086  2966 net.cpp:165] Memory required for data: 27233392
I1127 14:16:22.380091  2966 layer_factory.hpp:77] Creating layer Concat2
I1127 14:16:22.380097  2966 net.cpp:100] Creating Layer Concat2
I1127 14:16:22.380103  2966 net.cpp:434] Concat2 <- Convolution9
I1127 14:16:22.380112  2966 net.cpp:434] Concat2 <- Convolution11
I1127 14:16:22.380118  2966 net.cpp:434] Concat2 <- Convolution13
I1127 14:16:22.380125  2966 net.cpp:434] Concat2 <- Convolution14
I1127 14:16:22.380131  2966 net.cpp:408] Concat2 -> Concat2
I1127 14:16:22.380139  2966 net.cpp:150] Setting up Concat2
I1127 14:16:22.380146  2966 net.cpp:157] Top shape: 4 480 14 14 (376320)
I1127 14:16:22.380164  2966 net.cpp:165] Memory required for data: 28738672
I1127 14:16:22.380169  2966 layer_factory.hpp:77] Creating layer Pooling5
I1127 14:16:22.380177  2966 net.cpp:100] Creating Layer Pooling5
I1127 14:16:22.380183  2966 net.cpp:434] Pooling5 <- Concat2
I1127 14:16:22.380189  2966 net.cpp:408] Pooling5 -> Pooling5
I1127 14:16:22.380201  2966 net.cpp:150] Setting up Pooling5
I1127 14:16:22.380208  2966 net.cpp:157] Top shape: 4 480 7 7 (94080)
I1127 14:16:22.380214  2966 net.cpp:165] Memory required for data: 29114992
I1127 14:16:22.380219  2966 layer_factory.hpp:77] Creating layer Pooling5_Pooling5_0_split
I1127 14:16:22.380228  2966 net.cpp:100] Creating Layer Pooling5_Pooling5_0_split
I1127 14:16:22.380233  2966 net.cpp:434] Pooling5_Pooling5_0_split <- Pooling5
I1127 14:16:22.380239  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_0
I1127 14:16:22.380247  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_1
I1127 14:16:22.380254  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_2
I1127 14:16:22.380261  2966 net.cpp:408] Pooling5_Pooling5_0_split -> Pooling5_Pooling5_0_split_3
I1127 14:16:22.380270  2966 net.cpp:150] Setting up Pooling5_Pooling5_0_split
I1127 14:16:22.380275  2966 net.cpp:157] Top shape: 4 480 7 7 (94080)
I1127 14:16:22.380281  2966 net.cpp:157] Top shape: 4 480 7 7 (94080)
I1127 14:16:22.380286  2966 net.cpp:157] Top shape: 4 480 7 7 (94080)
I1127 14:16:22.380292  2966 net.cpp:157] Top shape: 4 480 7 7 (94080)
I1127 14:16:22.380297  2966 net.cpp:165] Memory required for data: 30620272
I1127 14:16:22.380302  2966 layer_factory.hpp:77] Creating layer Convolution15
I1127 14:16:22.380311  2966 net.cpp:100] Creating Layer Convolution15
I1127 14:16:22.380317  2966 net.cpp:434] Convolution15 <- Pooling5_Pooling5_0_split_0
I1127 14:16:22.380327  2966 net.cpp:408] Convolution15 -> Convolution15
I1127 14:16:22.380985  2966 net.cpp:150] Setting up Convolution15
I1127 14:16:22.381006  2966 net.cpp:157] Top shape: 4 192 7 7 (37632)
I1127 14:16:22.381013  2966 net.cpp:165] Memory required for data: 30770800
I1127 14:16:22.381021  2966 layer_factory.hpp:77] Creating layer ReLU15
I1127 14:16:22.381028  2966 net.cpp:100] Creating Layer ReLU15
I1127 14:16:22.381033  2966 net.cpp:434] ReLU15 <- Convolution15
I1127 14:16:22.381043  2966 net.cpp:395] ReLU15 -> Convolution15 (in-place)
I1127 14:16:22.381055  2966 net.cpp:150] Setting up ReLU15
I1127 14:16:22.381062  2966 net.cpp:157] Top shape: 4 192 7 7 (37632)
I1127 14:16:22.381067  2966 net.cpp:165] Memory required for data: 30921328
I1127 14:16:22.381073  2966 layer_factory.hpp:77] Creating layer Convolution16
I1127 14:16:22.381083  2966 net.cpp:100] Creating Layer Convolution16
I1127 14:16:22.381088  2966 net.cpp:434] Convolution16 <- Pooling5_Pooling5_0_split_1
I1127 14:16:22.381095  2966 net.cpp:408] Convolution16 -> Convolution16
I1127 14:16:22.381427  2966 net.cpp:150] Setting up Convolution16
I1127 14:16:22.382779  2966 net.cpp:157] Top shape: 4 96 7 7 (18816)
I1127 14:16:22.382787  2966 net.cpp:165] Memory required for data: 30996592
I1127 14:16:22.382797  2966 layer_factory.hpp:77] Creating layer ReLU16
I1127 14:16:22.382803  2966 net.cpp:100] Creating Layer ReLU16
I1127 14:16:22.382809  2966 net.cpp:434] ReLU16 <- Convolution16
I1127 14:16:22.382817  2966 net.cpp:395] ReLU16 -> Convolution16 (in-place)
I1127 14:16:22.382825  2966 net.cpp:150] Setting up ReLU16
I1127 14:16:22.382833  2966 net.cpp:157] Top shape: 4 96 7 7 (18816)
I1127 14:16:22.382838  2966 net.cpp:165] Memory required for data: 31071856
I1127 14:16:22.382843  2966 layer_factory.hpp:77] Creating layer Convolution17
I1127 14:16:22.382851  2966 net.cpp:100] Creating Layer Convolution17
I1127 14:16:22.382856  2966 net.cpp:434] Convolution17 <- Convolution16
I1127 14:16:22.382863  2966 net.cpp:408] Convolution17 -> Convolution17
I1127 14:16:22.384130  2966 net.cpp:150] Setting up Convolution17
I1127 14:16:22.385340  2966 net.cpp:157] Top shape: 4 208 7 7 (40768)
I1127 14:16:22.385488  2966 net.cpp:165] Memory required for data: 31234928
I1127 14:16:22.385599  2966 layer_factory.hpp:77] Creating layer ReLU17
I1127 14:16:22.385720  2966 net.cpp:100] Creating Layer ReLU17
I1127 14:16:22.385767  2966 net.cpp:434] ReLU17 <- Convolution17
I1127 14:16:22.385814  2966 net.cpp:395] ReLU17 -> Convolution17 (in-place)
I1127 14:16:22.385864  2966 net.cpp:150] Setting up ReLU17
I1127 14:16:22.385967  2966 net.cpp:157] Top shape: 4 208 7 7 (40768)
I1127 14:16:22.386023  2966 net.cpp:165] Memory required for data: 31398000
I1127 14:16:22.386073  2966 layer_factory.hpp:77] Creating layer Convolution18
I1127 14:16:22.386159  2966 net.cpp:100] Creating Layer Convolution18
I1127 14:16:22.386227  2966 net.cpp:434] Convolution18 <- Pooling5_Pooling5_0_split_2
I1127 14:16:22.386276  2966 net.cpp:408] Convolution18 -> Convolution18
I1127 14:16:22.386386  2966 net.cpp:150] Setting up Convolution18
I1127 14:16:22.386744  2966 net.cpp:157] Top shape: 4 16 7 7 (3136)
I1127 14:16:22.386842  2966 net.cpp:165] Memory required for data: 31410544
I1127 14:16:22.386873  2966 layer_factory.hpp:77] Creating layer ReLU18
I1127 14:16:22.386936  2966 net.cpp:100] Creating Layer ReLU18
I1127 14:16:22.386981  2966 net.cpp:434] ReLU18 <- Convolution18
I1127 14:16:22.387109  2966 net.cpp:395] ReLU18 -> Convolution18 (in-place)
I1127 14:16:22.387178  2966 net.cpp:150] Setting up ReLU18
I1127 14:16:22.387226  2966 net.cpp:157] Top shape: 4 16 7 7 (3136)
I1127 14:16:22.387233  2966 net.cpp:165] Memory required for data: 31423088
I1127 14:16:22.387238  2966 layer_factory.hpp:77] Creating layer Convolution19
I1127 14:16:22.387270  2966 net.cpp:100] Creating Layer Convolution19
I1127 14:16:22.387370  2966 net.cpp:434] Convolution19 <- Convolution18
I1127 14:16:22.387418  2966 net.cpp:408] Convolution19 -> Convolution19
I1127 14:16:22.387622  2966 net.cpp:150] Setting up Convolution19
I1127 14:16:22.388401  2966 net.cpp:157] Top shape: 4 48 7 7 (9408)
I1127 14:16:22.388608  2966 net.cpp:165] Memory required for data: 31460720
I1127 14:16:22.388620  2966 layer_factory.hpp:77] Creating layer ReLU19
I1127 14:16:22.388631  2966 net.cpp:100] Creating Layer ReLU19
I1127 14:16:22.388700  2966 net.cpp:434] ReLU19 <- Convolution19
I1127 14:16:22.388707  2966 net.cpp:395] ReLU19 -> Convolution19 (in-place)
I1127 14:16:22.388716  2966 net.cpp:150] Setting up ReLU19
I1127 14:16:22.388816  2966 net.cpp:157] Top shape: 4 48 7 7 (9408)
I1127 14:16:22.388823  2966 net.cpp:165] Memory required for data: 31498352
I1127 14:16:22.388828  2966 layer_factory.hpp:77] Creating layer Pooling6
I1127 14:16:22.388836  2966 net.cpp:100] Creating Layer Pooling6
I1127 14:16:22.388932  2966 net.cpp:434] Pooling6 <- Pooling5_Pooling5_0_split_3
I1127 14:16:22.389032  2966 net.cpp:408] Pooling6 -> Pooling6
I1127 14:16:22.389145  2966 net.cpp:150] Setting up Pooling6
I1127 14:16:22.389154  2966 net.cpp:157] Top shape: 4 480 7 7 (94080)
I1127 14:16:22.389230  2966 net.cpp:165] Memory required for data: 31874672
I1127 14:16:22.389237  2966 layer_factory.hpp:77] Creating layer Convolution20
I1127 14:16:22.389308  2966 net.cpp:100] Creating Layer Convolution20
I1127 14:16:22.389315  2966 net.cpp:434] Convolution20 <- Pooling6
I1127 14:16:22.389324  2966 net.cpp:408] Convolution20 -> Convolution20
I1127 14:16:22.389685  2966 net.cpp:150] Setting up Convolution20
I1127 14:16:22.390151  2966 net.cpp:157] Top shape: 4 64 7 7 (12544)
I1127 14:16:22.390159  2966 net.cpp:165] Memory required for data: 31924848
I1127 14:16:22.390167  2966 layer_factory.hpp:77] Creating layer ReLU20
I1127 14:16:22.390177  2966 net.cpp:100] Creating Layer ReLU20
I1127 14:16:22.390183  2966 net.cpp:434] ReLU20 <- Convolution20
I1127 14:16:22.390189  2966 net.cpp:395] ReLU20 -> Convolution20 (in-place)
I1127 14:16:22.390198  2966 net.cpp:150] Setting up ReLU20
I1127 14:16:22.390204  2966 net.cpp:157] Top shape: 4 64 7 7 (12544)
I1127 14:16:22.390209  2966 net.cpp:165] Memory required for data: 31975024
I1127 14:16:22.390214  2966 layer_factory.hpp:77] Creating layer Concat3
I1127 14:16:22.390236  2966 net.cpp:100] Creating Layer Concat3
I1127 14:16:22.390242  2966 net.cpp:434] Concat3 <- Convolution15
I1127 14:16:22.390249  2966 net.cpp:434] Concat3 <- Convolution17
I1127 14:16:22.390255  2966 net.cpp:434] Concat3 <- Convolution19
I1127 14:16:22.390261  2966 net.cpp:434] Concat3 <- Convolution20
I1127 14:16:22.390269  2966 net.cpp:408] Concat3 -> Concat3
I1127 14:16:22.390277  2966 net.cpp:150] Setting up Concat3
I1127 14:16:22.390283  2966 net.cpp:157] Top shape: 4 512 7 7 (100352)
I1127 14:16:22.390288  2966 net.cpp:165] Memory required for data: 32376432
I1127 14:16:22.390293  2966 layer_factory.hpp:77] Creating layer Pooling7
I1127 14:16:22.390300  2966 net.cpp:100] Creating Layer Pooling7
I1127 14:16:22.390305  2966 net.cpp:434] Pooling7 <- Concat3
I1127 14:16:22.390313  2966 net.cpp:408] Pooling7 -> Pooling7
I1127 14:16:22.390322  2966 net.cpp:150] Setting up Pooling7
I1127 14:16:22.390328  2966 net.cpp:157] Top shape: 4 512 1 1 (2048)
I1127 14:16:22.390333  2966 net.cpp:165] Memory required for data: 32384624
I1127 14:16:22.390339  2966 layer_factory.hpp:77] Creating layer InnerProduct1
I1127 14:16:22.390347  2966 net.cpp:100] Creating Layer InnerProduct1
I1127 14:16:22.390352  2966 net.cpp:434] InnerProduct1 <- Pooling7
I1127 14:16:22.390359  2966 net.cpp:408] InnerProduct1 -> InnerProduct1
I1127 14:16:22.442556  2966 net.cpp:150] Setting up InnerProduct1
I1127 14:16:22.444195  2966 net.cpp:157] Top shape: 4 4096 (16384)
I1127 14:16:22.444394  2966 net.cpp:165] Memory required for data: 32450160
I1127 14:16:22.444495  2966 layer_factory.hpp:77] Creating layer ReLU21
I1127 14:16:22.444631  2966 net.cpp:100] Creating Layer ReLU21
I1127 14:16:22.444746  2966 net.cpp:434] ReLU21 <- InnerProduct1
I1127 14:16:22.444869  2966 net.cpp:395] ReLU21 -> InnerProduct1 (in-place)
I1127 14:16:22.444955  2966 net.cpp:150] Setting up ReLU21
I1127 14:16:22.445101  2966 net.cpp:157] Top shape: 4 4096 (16384)
I1127 14:16:22.445178  2966 net.cpp:165] Memory required for data: 32515696
I1127 14:16:22.445253  2966 layer_factory.hpp:77] Creating layer Dropout1
I1127 14:16:22.445402  2966 net.cpp:100] Creating Layer Dropout1
I1127 14:16:22.445497  2966 net.cpp:434] Dropout1 <- InnerProduct1
I1127 14:16:22.445580  2966 net.cpp:395] Dropout1 -> InnerProduct1 (in-place)
I1127 14:16:22.445624  2966 net.cpp:150] Setting up Dropout1
I1127 14:16:22.445703  2966 net.cpp:157] Top shape: 4 4096 (16384)
I1127 14:16:22.445888  2966 net.cpp:165] Memory required for data: 32581232
I1127 14:16:22.445967  2966 layer_factory.hpp:77] Creating layer InnerProduct2
I1127 14:16:22.446064  2966 net.cpp:100] Creating Layer InnerProduct2
I1127 14:16:22.446192  2966 net.cpp:434] InnerProduct2 <- InnerProduct1
I1127 14:16:22.446259  2966 net.cpp:408] InnerProduct2 -> InnerProduct2
I1127 14:16:22.516191  2966 net.cpp:150] Setting up InnerProduct2
I1127 14:16:22.521857  2966 net.cpp:157] Top shape: 4 1024 (4096)
I1127 14:16:22.521939  2966 net.cpp:165] Memory required for data: 32597616
I1127 14:16:22.521999  2966 layer_factory.hpp:77] Creating layer InnerProduct2_InnerProduct2_0_split
I1127 14:16:22.522056  2966 net.cpp:100] Creating Layer InnerProduct2_InnerProduct2_0_split
I1127 14:16:22.522138  2966 net.cpp:434] InnerProduct2_InnerProduct2_0_split <- InnerProduct2
I1127 14:16:22.522217  2966 net.cpp:408] InnerProduct2_InnerProduct2_0_split -> InnerProduct2_InnerProduct2_0_split_0
I1127 14:16:22.522306  2966 net.cpp:408] InnerProduct2_InnerProduct2_0_split -> InnerProduct2_InnerProduct2_0_split_1
I1127 14:16:22.522385  2966 net.cpp:150] Setting up InnerProduct2_InnerProduct2_0_split
I1127 14:16:22.522450  2966 net.cpp:157] Top shape: 4 1024 (4096)
I1127 14:16:22.522496  2966 net.cpp:157] Top shape: 4 1024 (4096)
I1127 14:16:22.522543  2966 net.cpp:165] Memory required for data: 32630384
I1127 14:16:22.522584  2966 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1127 14:16:22.522653  2966 net.cpp:100] Creating Layer SoftmaxWithLoss1
I1127 14:16:22.522696  2966 net.cpp:434] SoftmaxWithLoss1 <- InnerProduct2_InnerProduct2_0_split_0
I1127 14:16:22.522789  2966 net.cpp:434] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I1127 14:16:22.522856  2966 net.cpp:408] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1127 14:16:22.522938  2966 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1127 14:16:22.523062  2966 net.cpp:150] Setting up SoftmaxWithLoss1
I1127 14:16:22.523218  2966 net.cpp:157] Top shape: (1)
I1127 14:16:22.523288  2966 net.cpp:160]     with loss weight 1
I1127 14:16:22.523391  2966 net.cpp:165] Memory required for data: 32630388
I1127 14:16:22.523478  2966 layer_factory.hpp:77] Creating layer Accuracy1
I1127 14:16:22.527827  2966 net.cpp:100] Creating Layer Accuracy1
I1127 14:16:22.527848  2966 net.cpp:434] Accuracy1 <- InnerProduct2_InnerProduct2_0_split_1
I1127 14:16:22.527855  2966 net.cpp:434] Accuracy1 <- Data2_Data1_1_split_1
I1127 14:16:22.527860  2966 net.cpp:408] Accuracy1 -> Accuracy1
I1127 14:16:22.527875  2966 net.cpp:150] Setting up Accuracy1
I1127 14:16:22.527880  2966 net.cpp:157] Top shape: (1)
I1127 14:16:22.527884  2966 net.cpp:165] Memory required for data: 32630392
I1127 14:16:22.527886  2966 net.cpp:228] Accuracy1 does not need backward computation.
I1127 14:16:22.527889  2966 net.cpp:226] SoftmaxWithLoss1 needs backward computation.
I1127 14:16:22.527894  2966 net.cpp:226] InnerProduct2_InnerProduct2_0_split needs backward computation.
I1127 14:16:22.527896  2966 net.cpp:226] InnerProduct2 needs backward computation.
I1127 14:16:22.527900  2966 net.cpp:226] Dropout1 needs backward computation.
I1127 14:16:22.527902  2966 net.cpp:226] ReLU21 needs backward computation.
I1127 14:16:22.527905  2966 net.cpp:226] InnerProduct1 needs backward computation.
I1127 14:16:22.527909  2966 net.cpp:226] Pooling7 needs backward computation.
I1127 14:16:22.527911  2966 net.cpp:226] Concat3 needs backward computation.
I1127 14:16:22.527915  2966 net.cpp:226] ReLU20 needs backward computation.
I1127 14:16:22.527918  2966 net.cpp:226] Convolution20 needs backward computation.
I1127 14:16:22.527921  2966 net.cpp:226] Pooling6 needs backward computation.
I1127 14:16:22.527925  2966 net.cpp:226] ReLU19 needs backward computation.
I1127 14:16:22.527927  2966 net.cpp:226] Convolution19 needs backward computation.
I1127 14:16:22.527930  2966 net.cpp:226] ReLU18 needs backward computation.
I1127 14:16:22.527933  2966 net.cpp:226] Convolution18 needs backward computation.
I1127 14:16:22.527936  2966 net.cpp:226] ReLU17 needs backward computation.
I1127 14:16:22.527940  2966 net.cpp:226] Convolution17 needs backward computation.
I1127 14:16:22.527941  2966 net.cpp:226] ReLU16 needs backward computation.
I1127 14:16:22.527945  2966 net.cpp:226] Convolution16 needs backward computation.
I1127 14:16:22.527947  2966 net.cpp:226] ReLU15 needs backward computation.
I1127 14:16:22.527951  2966 net.cpp:226] Convolution15 needs backward computation.
I1127 14:16:22.527953  2966 net.cpp:226] Pooling5_Pooling5_0_split needs backward computation.
I1127 14:16:22.527956  2966 net.cpp:226] Pooling5 needs backward computation.
I1127 14:16:22.527959  2966 net.cpp:226] Concat2 needs backward computation.
I1127 14:16:22.527962  2966 net.cpp:226] ReLU14 needs backward computation.
I1127 14:16:22.527966  2966 net.cpp:226] Convolution14 needs backward computation.
I1127 14:16:22.527968  2966 net.cpp:226] Pooling4 needs backward computation.
I1127 14:16:22.527971  2966 net.cpp:226] ReLU13 needs backward computation.
I1127 14:16:22.527974  2966 net.cpp:226] Convolution13 needs backward computation.
I1127 14:16:22.527976  2966 net.cpp:226] ReLU12 needs backward computation.
I1127 14:16:22.527979  2966 net.cpp:226] Convolution12 needs backward computation.
I1127 14:16:22.527982  2966 net.cpp:226] ReLU11 needs backward computation.
I1127 14:16:22.527984  2966 net.cpp:226] Convolution11 needs backward computation.
I1127 14:16:22.527987  2966 net.cpp:226] ReLU10 needs backward computation.
I1127 14:16:22.527990  2966 net.cpp:226] Convolution10 needs backward computation.
I1127 14:16:22.527993  2966 net.cpp:226] ReLU9 needs backward computation.
I1127 14:16:22.527997  2966 net.cpp:226] Convolution9 needs backward computation.
I1127 14:16:22.528017  2966 net.cpp:226] Concat1_Concat1_0_split needs backward computation.
I1127 14:16:22.528020  2966 net.cpp:226] Concat1 needs backward computation.
I1127 14:16:22.528023  2966 net.cpp:226] ReLU8 needs backward computation.
I1127 14:16:22.528026  2966 net.cpp:226] Convolution8 needs backward computation.
I1127 14:16:22.528031  2966 net.cpp:226] ReLU7 needs backward computation.
I1127 14:16:22.528034  2966 net.cpp:226] Convolution7 needs backward computation.
I1127 14:16:22.528038  2966 net.cpp:226] Pooling3 needs backward computation.
I1127 14:16:22.528041  2966 net.cpp:226] ReLU6 needs backward computation.
I1127 14:16:22.528043  2966 net.cpp:226] Convolution6 needs backward computation.
I1127 14:16:22.528046  2966 net.cpp:226] ReLU5 needs backward computation.
I1127 14:16:22.528049  2966 net.cpp:226] Convolution5 needs backward computation.
I1127 14:16:22.528053  2966 net.cpp:226] ReLU4 needs backward computation.
I1127 14:16:22.528054  2966 net.cpp:226] Convolution4 needs backward computation.
I1127 14:16:22.528058  2966 net.cpp:226] ReLU3 needs backward computation.
I1127 14:16:22.528060  2966 net.cpp:226] Convolution3 needs backward computation.
I1127 14:16:22.528064  2966 net.cpp:226] Pooling2_Pooling2_0_split needs backward computation.
I1127 14:16:22.528066  2966 net.cpp:226] Pooling2 needs backward computation.
I1127 14:16:22.528069  2966 net.cpp:226] ReLU2 needs backward computation.
I1127 14:16:22.528072  2966 net.cpp:226] Convolution2 needs backward computation.
I1127 14:16:22.528075  2966 net.cpp:226] Pooling1 needs backward computation.
I1127 14:16:22.528079  2966 net.cpp:226] ReLU1 needs backward computation.
I1127 14:16:22.528080  2966 net.cpp:226] Convolution1 needs backward computation.
I1127 14:16:22.528084  2966 net.cpp:228] Data2_Data1_1_split does not need backward computation.
I1127 14:16:22.528087  2966 net.cpp:228] Data1 does not need backward computation.
I1127 14:16:22.528090  2966 net.cpp:270] This network produces output Accuracy1
I1127 14:16:22.528093  2966 net.cpp:270] This network produces output SoftmaxWithLoss1
I1127 14:16:22.528169  2966 net.cpp:283] Network initialization done.
I1127 14:16:22.528312  2966 solver.cpp:60] Solver scaffolding done.
I1127 14:16:22.528383  2966 caffe.cpp:155] Finetuning from examples/myfile/myfile_nin/cloudernet_train_snapshot/highest_caffemodel/cloudernet_train_snapshot_iter_50000.caffemodel
I1127 14:16:22.761404  2966 caffe.cpp:251] Starting Optimization
I1127 14:16:22.765591  2966 solver.cpp:279] Solving 
I1127 14:16:22.765635  2966 solver.cpp:280] Learning Rate Policy: poly
I1127 14:16:22.781503  2966 solver.cpp:337] Iteration 0, Testing net (#0)
I1127 14:16:37.133003  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.723837
I1127 14:16:37.133165  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.821595 (* 1 = 0.821595 loss)
I1127 14:16:37.804625  2966 solver.cpp:228] Iteration 0, loss = 0.139987
I1127 14:16:37.804745  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.139987 (* 1 = 0.139987 loss)
I1127 14:16:37.804811  2966 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I1127 14:17:43.289865  2966 solver.cpp:228] Iteration 100, loss = 1.43411
I1127 14:17:43.290012  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.43411 (* 1 = 1.43411 loss)
I1127 14:17:43.290076  2966 sgd_solver.cpp:106] Iteration 100, lr = 9.975e-06
I1127 14:18:48.075007  2966 solver.cpp:228] Iteration 200, loss = 0.407247
I1127 14:18:48.075182  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.407247 (* 1 = 0.407247 loss)
I1127 14:18:48.075256  2966 sgd_solver.cpp:106] Iteration 200, lr = 9.95e-06
I1127 14:19:14.992925  2966 solver.cpp:337] Iteration 242, Testing net (#0)
I1127 14:19:29.026475  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.648256
I1127 14:19:29.026793  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.99709 (* 1 = 0.99709 loss)
I1127 14:20:05.798951  2966 solver.cpp:228] Iteration 300, loss = 0.415287
I1127 14:20:05.799084  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.415287 (* 1 = 0.415287 loss)
I1127 14:20:05.799129  2966 sgd_solver.cpp:106] Iteration 300, lr = 9.925e-06
I1127 14:21:08.235144  2966 solver.cpp:228] Iteration 400, loss = 0.365059
I1127 14:21:08.235313  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.365059 (* 1 = 0.365059 loss)
I1127 14:21:08.235363  2966 sgd_solver.cpp:106] Iteration 400, lr = 9.9e-06
I1127 14:22:34.259037  2966 solver.cpp:337] Iteration 484, Testing net (#0)
I1127 14:22:49.095355  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.668605
I1127 14:22:49.095471  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.919166 (* 1 = 0.919166 loss)
I1127 14:23:01.071707  2966 solver.cpp:228] Iteration 500, loss = 0.477025
I1127 14:23:01.071822  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.477025 (* 1 = 0.477025 loss)
I1127 14:23:01.071867  2966 sgd_solver.cpp:106] Iteration 500, lr = 9.875e-06
I1127 14:24:10.001287  2966 solver.cpp:228] Iteration 600, loss = 1.09843
I1127 14:24:10.001467  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.09843 (* 1 = 1.09843 loss)
I1127 14:24:10.001513  2966 sgd_solver.cpp:106] Iteration 600, lr = 9.85e-06
I1127 14:25:19.324481  2966 solver.cpp:228] Iteration 700, loss = 0.273158
I1127 14:25:19.324622  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.273158 (* 1 = 0.273158 loss)
I1127 14:25:19.324667  2966 sgd_solver.cpp:106] Iteration 700, lr = 9.825e-06
I1127 14:25:34.589967  2966 solver.cpp:337] Iteration 726, Testing net (#0)
I1127 14:25:49.083616  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.65407
I1127 14:25:49.083781  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.899303 (* 1 = 0.899303 loss)
I1127 14:26:35.040521  2966 solver.cpp:228] Iteration 800, loss = 0.314163
I1127 14:26:35.040655  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.314163 (* 1 = 0.314163 loss)
I1127 14:26:35.040700  2966 sgd_solver.cpp:106] Iteration 800, lr = 9.8e-06
I1127 14:27:38.559468  2966 solver.cpp:228] Iteration 900, loss = 0.58145
I1127 14:27:38.559630  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.58145 (* 1 = 0.58145 loss)
I1127 14:27:38.559676  2966 sgd_solver.cpp:106] Iteration 900, lr = 9.775e-06
I1127 14:28:24.999325  2966 solver.cpp:337] Iteration 968, Testing net (#0)
I1127 14:28:42.124619  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.677326
I1127 14:28:42.124742  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.954566 (* 1 = 0.954566 loss)
I1127 14:29:07.232529  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_1000.caffemodel
I1127 14:29:07.363188  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_1000.solverstate
I1127 14:29:08.014417  2966 solver.cpp:228] Iteration 1000, loss = 0.878947
I1127 14:29:08.014524  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.878947 (* 1 = 0.878947 loss)
I1127 14:29:08.014570  2966 sgd_solver.cpp:106] Iteration 1000, lr = 9.75e-06
I1127 14:30:27.694615  2966 solver.cpp:228] Iteration 1100, loss = 0.611118
I1127 14:30:27.694744  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.611118 (* 1 = 0.611118 loss)
I1127 14:30:27.694790  2966 sgd_solver.cpp:106] Iteration 1100, lr = 9.725e-06
I1127 14:31:48.389828  2966 solver.cpp:228] Iteration 1200, loss = 0.584395
I1127 14:31:48.389961  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.584395 (* 1 = 0.584395 loss)
I1127 14:31:48.390007  2966 sgd_solver.cpp:106] Iteration 1200, lr = 9.7e-06
I1127 14:31:54.558565  2966 solver.cpp:337] Iteration 1210, Testing net (#0)
I1127 14:32:10.028532  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.627907
I1127 14:32:10.028729  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 1.00609 (* 1 = 1.00609 loss)
I1127 14:33:06.993425  2966 solver.cpp:228] Iteration 1300, loss = 0.157468
I1127 14:33:06.993731  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.157468 (* 1 = 0.157468 loss)
I1127 14:33:06.993777  2966 sgd_solver.cpp:106] Iteration 1300, lr = 9.675e-06
I1127 14:34:09.183333  2966 solver.cpp:228] Iteration 1400, loss = 0.172204
I1127 14:34:09.183472  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.172203 (* 1 = 0.172203 loss)
I1127 14:34:09.183516  2966 sgd_solver.cpp:106] Iteration 1400, lr = 9.65e-06
I1127 14:34:42.152979  2966 solver.cpp:337] Iteration 1452, Testing net (#0)
I1127 14:34:55.261761  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.668605
I1127 14:34:55.261870  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.957748 (* 1 = 0.957748 loss)
I1127 14:35:26.536206  2966 solver.cpp:228] Iteration 1500, loss = 0.263407
I1127 14:35:26.536345  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.263406 (* 1 = 0.263406 loss)
I1127 14:35:26.536387  2966 sgd_solver.cpp:106] Iteration 1500, lr = 9.625e-06
I1127 14:36:31.991425  2966 solver.cpp:228] Iteration 1600, loss = 0.108032
I1127 14:36:31.991556  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.108032 (* 1 = 0.108032 loss)
I1127 14:36:31.991600  2966 sgd_solver.cpp:106] Iteration 1600, lr = 9.6e-06
I1127 14:37:30.122598  2966 solver.cpp:337] Iteration 1694, Testing net (#0)
I1127 14:37:42.858656  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.648256
I1127 14:37:42.858762  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.930626 (* 1 = 0.930626 loss)
I1127 14:37:47.124035  2966 solver.cpp:228] Iteration 1700, loss = 0.768688
I1127 14:37:47.124148  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.768688 (* 1 = 0.768688 loss)
I1127 14:37:47.124214  2966 sgd_solver.cpp:106] Iteration 1700, lr = 9.575e-06
I1127 14:38:47.431668  2966 solver.cpp:228] Iteration 1800, loss = 0.409027
I1127 14:38:47.431803  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.409027 (* 1 = 0.409027 loss)
I1127 14:38:47.431848  2966 sgd_solver.cpp:106] Iteration 1800, lr = 9.55e-06
I1127 14:39:47.846055  2966 solver.cpp:228] Iteration 1900, loss = 0.378651
I1127 14:39:47.846179  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.378651 (* 1 = 0.378651 loss)
I1127 14:39:47.846221  2966 sgd_solver.cpp:106] Iteration 1900, lr = 9.525e-06
I1127 14:40:09.132889  2966 solver.cpp:337] Iteration 1936, Testing net (#0)
I1127 14:40:22.602190  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.662791
I1127 14:40:22.602318  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.98801 (* 1 = 0.98801 loss)
I1127 14:41:05.444665  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_2000.caffemodel
I1127 14:41:05.570593  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_2000.solverstate
I1127 14:41:06.212450  2966 solver.cpp:228] Iteration 2000, loss = 0.160252
I1127 14:41:06.212556  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.160252 (* 1 = 0.160252 loss)
I1127 14:41:06.212615  2966 sgd_solver.cpp:106] Iteration 2000, lr = 9.5e-06
I1127 14:42:14.838465  2966 solver.cpp:228] Iteration 2100, loss = 0.245426
I1127 14:42:14.838667  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.245426 (* 1 = 0.245426 loss)
I1127 14:42:14.838711  2966 sgd_solver.cpp:106] Iteration 2100, lr = 9.475e-06
I1127 14:43:01.509632  2966 solver.cpp:337] Iteration 2178, Testing net (#0)
I1127 14:43:14.752348  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.69186
I1127 14:43:14.752462  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.894628 (* 1 = 0.894628 loss)
I1127 14:43:29.374797  2966 solver.cpp:228] Iteration 2200, loss = 1.08618
I1127 14:43:29.374907  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.08618 (* 1 = 1.08618 loss)
I1127 14:43:29.374948  2966 sgd_solver.cpp:106] Iteration 2200, lr = 9.45e-06
I1127 14:44:35.682986  2966 solver.cpp:228] Iteration 2300, loss = 1.3402
I1127 14:44:35.683295  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.3402 (* 1 = 1.3402 loss)
I1127 14:44:35.683339  2966 sgd_solver.cpp:106] Iteration 2300, lr = 9.425e-06
I1127 14:45:36.344779  2966 solver.cpp:228] Iteration 2400, loss = 0.199312
I1127 14:45:36.344938  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.199312 (* 1 = 0.199312 loss)
I1127 14:45:36.344981  2966 sgd_solver.cpp:106] Iteration 2400, lr = 9.4e-06
I1127 14:45:48.153290  2966 solver.cpp:337] Iteration 2420, Testing net (#0)
I1127 14:46:01.275404  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.656977
I1127 14:46:01.275517  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.93607 (* 1 = 0.93607 loss)
I1127 14:46:51.851227  2966 solver.cpp:228] Iteration 2500, loss = 0.433401
I1127 14:46:51.851362  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.433401 (* 1 = 0.433401 loss)
I1127 14:46:51.851407  2966 sgd_solver.cpp:106] Iteration 2500, lr = 9.375e-06
I1127 14:47:59.215026  2966 solver.cpp:228] Iteration 2600, loss = 0.358302
I1127 14:47:59.215133  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.358302 (* 1 = 0.358302 loss)
I1127 14:47:59.215175  2966 sgd_solver.cpp:106] Iteration 2600, lr = 9.35e-06
I1127 14:48:37.118527  2966 solver.cpp:337] Iteration 2662, Testing net (#0)
I1127 14:48:50.181527  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.723837
I1127 14:48:50.181635  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.841806 (* 1 = 0.841806 loss)
I1127 14:49:13.597769  2966 solver.cpp:228] Iteration 2700, loss = 0.0960444
I1127 14:49:13.597898  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0960446 (* 1 = 0.0960446 loss)
I1127 14:49:13.597940  2966 sgd_solver.cpp:106] Iteration 2700, lr = 9.325e-06
I1127 14:50:13.492569  2966 solver.cpp:228] Iteration 2800, loss = 0.334629
I1127 14:50:13.492713  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.334629 (* 1 = 0.334629 loss)
I1127 14:50:13.492756  2966 sgd_solver.cpp:106] Iteration 2800, lr = 9.3e-06
I1127 14:51:13.645016  2966 solver.cpp:228] Iteration 2900, loss = 0.545169
I1127 14:51:13.645246  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.545169 (* 1 = 0.545169 loss)
I1127 14:51:13.645323  2966 sgd_solver.cpp:106] Iteration 2900, lr = 9.275e-06
I1127 14:51:15.568897  2966 solver.cpp:337] Iteration 2904, Testing net (#0)
I1127 14:51:28.201043  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.723837
I1127 14:51:28.201158  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.815233 (* 1 = 0.815233 loss)
I1127 14:52:28.817931  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_3000.caffemodel
I1127 14:52:28.927672  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_3000.solverstate
I1127 14:52:29.566371  2966 solver.cpp:228] Iteration 3000, loss = 0.479865
I1127 14:52:29.566481  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.479865 (* 1 = 0.479865 loss)
I1127 14:52:29.566522  2966 sgd_solver.cpp:106] Iteration 3000, lr = 9.25e-06
I1127 14:53:30.791332  2966 solver.cpp:228] Iteration 3100, loss = 0.636687
I1127 14:53:30.791460  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.636687 (* 1 = 0.636687 loss)
I1127 14:53:30.791502  2966 sgd_solver.cpp:106] Iteration 3100, lr = 9.225e-06
I1127 14:53:58.620427  2966 solver.cpp:337] Iteration 3146, Testing net (#0)
I1127 14:54:11.137754  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.668605
I1127 14:54:11.138043  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.881909 (* 1 = 0.881909 loss)
I1127 14:54:44.488510  2966 solver.cpp:228] Iteration 3200, loss = 0.490881
I1127 14:54:44.488641  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.490881 (* 1 = 0.490881 loss)
I1127 14:54:44.488683  2966 sgd_solver.cpp:106] Iteration 3200, lr = 9.2e-06
I1127 14:55:45.580557  2966 solver.cpp:228] Iteration 3300, loss = 0.256384
I1127 14:55:45.580689  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.256383 (* 1 = 0.256383 loss)
I1127 14:55:45.580732  2966 sgd_solver.cpp:106] Iteration 3300, lr = 9.175e-06
I1127 14:56:38.179039  2966 solver.cpp:337] Iteration 3388, Testing net (#0)
I1127 14:56:50.653916  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.680233
I1127 14:56:50.654047  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.953619 (* 1 = 0.953619 loss)
I1127 14:56:58.435993  2966 solver.cpp:228] Iteration 3400, loss = 0.310853
I1127 14:56:58.436206  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.310853 (* 1 = 0.310853 loss)
I1127 14:56:58.436265  2966 sgd_solver.cpp:106] Iteration 3400, lr = 9.15e-06
I1127 14:57:58.152432  2966 solver.cpp:228] Iteration 3500, loss = 0.435794
I1127 14:57:58.152609  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.435794 (* 1 = 0.435794 loss)
I1127 14:57:58.152652  2966 sgd_solver.cpp:106] Iteration 3500, lr = 9.125e-06
I1127 14:59:04.215425  2966 solver.cpp:228] Iteration 3600, loss = 0.46152
I1127 14:59:04.215665  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.46152 (* 1 = 0.46152 loss)
I1127 14:59:04.215740  2966 sgd_solver.cpp:106] Iteration 3600, lr = 9.1e-06
I1127 14:59:23.100368  2966 solver.cpp:337] Iteration 3630, Testing net (#0)
I1127 14:59:36.211098  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.72093
I1127 14:59:36.211236  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.805144 (* 1 = 0.805144 loss)
I1127 15:00:21.286885  2966 solver.cpp:228] Iteration 3700, loss = 0.35534
I1127 15:00:21.287037  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.35534 (* 1 = 0.35534 loss)
I1127 15:00:21.287098  2966 sgd_solver.cpp:106] Iteration 3700, lr = 9.075e-06
I1127 15:01:21.197304  2966 solver.cpp:228] Iteration 3800, loss = 0.762135
I1127 15:01:21.197438  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.762135 (* 1 = 0.762135 loss)
I1127 15:01:21.197480  2966 sgd_solver.cpp:106] Iteration 3800, lr = 9.05e-06
I1127 15:02:03.797852  2966 solver.cpp:337] Iteration 3872, Testing net (#0)
I1127 15:02:16.350098  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.671512
I1127 15:02:16.350203  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.985921 (* 1 = 0.985921 loss)
I1127 15:02:33.639940  2966 solver.cpp:228] Iteration 3900, loss = 0.174784
I1127 15:02:33.640053  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.174783 (* 1 = 0.174783 loss)
I1127 15:02:33.640096  2966 sgd_solver.cpp:106] Iteration 3900, lr = 9.025e-06
I1127 15:03:33.506731  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_4000.caffemodel
I1127 15:03:33.615547  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_4000.solverstate
I1127 15:03:34.251761  2966 solver.cpp:228] Iteration 4000, loss = 0.677179
I1127 15:03:34.251871  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.677179 (* 1 = 0.677179 loss)
I1127 15:03:34.251914  2966 sgd_solver.cpp:106] Iteration 4000, lr = 9e-06
I1127 15:04:34.577672  2966 solver.cpp:228] Iteration 4100, loss = 0.38657
I1127 15:04:34.577970  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.38657 (* 1 = 0.38657 loss)
I1127 15:04:34.578047  2966 sgd_solver.cpp:106] Iteration 4100, lr = 8.975e-06
I1127 15:04:42.433820  2966 solver.cpp:337] Iteration 4114, Testing net (#0)
I1127 15:04:54.956483  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.659884
I1127 15:04:54.956674  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.909712 (* 1 = 0.909712 loss)
I1127 15:05:48.675019  2966 solver.cpp:228] Iteration 4200, loss = 0.351459
I1127 15:05:48.675209  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.35146 (* 1 = 0.35146 loss)
I1127 15:05:48.675251  2966 sgd_solver.cpp:106] Iteration 4200, lr = 8.95e-06
I1127 15:06:49.737179  2966 solver.cpp:228] Iteration 4300, loss = 1.01729
I1127 15:06:49.737308  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.01729 (* 1 = 1.01729 loss)
I1127 15:06:49.737349  2966 sgd_solver.cpp:106] Iteration 4300, lr = 8.925e-06
I1127 15:07:23.252699  2966 solver.cpp:337] Iteration 4356, Testing net (#0)
I1127 15:07:36.974702  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.69186
I1127 15:07:36.974814  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.930544 (* 1 = 0.930544 loss)
I1127 15:08:05.108038  2966 solver.cpp:228] Iteration 4400, loss = 0.315013
I1127 15:08:05.108171  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.315013 (* 1 = 0.315013 loss)
I1127 15:08:05.108214  2966 sgd_solver.cpp:106] Iteration 4400, lr = 8.9e-06
I1127 15:09:06.192201  2966 solver.cpp:228] Iteration 4500, loss = 0.29016
I1127 15:09:06.192402  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.290161 (* 1 = 0.290161 loss)
I1127 15:09:06.192517  2966 sgd_solver.cpp:106] Iteration 4500, lr = 8.875e-06
I1127 15:10:06.972827  2966 solver.cpp:337] Iteration 4598, Testing net (#0)
I1127 15:10:19.557909  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.694767
I1127 15:10:19.558019  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.964791 (* 1 = 0.964791 loss)
I1127 15:10:21.370743  2966 solver.cpp:228] Iteration 4600, loss = 0.529685
I1127 15:10:21.370849  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.529686 (* 1 = 0.529686 loss)
I1127 15:10:21.370893  2966 sgd_solver.cpp:106] Iteration 4600, lr = 8.85e-06
I1127 15:11:22.468210  2966 solver.cpp:228] Iteration 4700, loss = 0.42504
I1127 15:11:22.468345  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.425041 (* 1 = 0.425041 loss)
I1127 15:11:22.468389  2966 sgd_solver.cpp:106] Iteration 4700, lr = 8.825e-06
I1127 15:12:24.066979  2966 solver.cpp:228] Iteration 4800, loss = 0.0782792
I1127 15:12:24.067111  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0782799 (* 1 = 0.0782799 loss)
I1127 15:12:24.067154  2966 sgd_solver.cpp:106] Iteration 4800, lr = 8.8e-06
I1127 15:12:47.778478  2966 solver.cpp:337] Iteration 4840, Testing net (#0)
I1127 15:13:00.405853  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.706395
I1127 15:13:00.405989  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.825248 (* 1 = 0.825248 loss)
I1127 15:13:39.318388  2966 solver.cpp:228] Iteration 4900, loss = 0.315463
I1127 15:13:39.318879  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.315464 (* 1 = 0.315464 loss)
I1127 15:13:39.318927  2966 sgd_solver.cpp:106] Iteration 4900, lr = 8.775e-06
I1127 15:14:42.313925  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_5000.caffemodel
I1127 15:14:42.421573  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_5000.solverstate
I1127 15:14:43.071465  2966 solver.cpp:228] Iteration 5000, loss = 0.712548
I1127 15:14:43.071600  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.712549 (* 1 = 0.712549 loss)
I1127 15:14:43.071643  2966 sgd_solver.cpp:106] Iteration 5000, lr = 8.75e-06
I1127 15:15:33.311035  2966 solver.cpp:337] Iteration 5082, Testing net (#0)
I1127 15:15:46.064261  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.709302
I1127 15:15:46.064375  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.863615 (* 1 = 0.863615 loss)
I1127 15:15:57.613813  2966 solver.cpp:228] Iteration 5100, loss = 0.147889
I1127 15:15:57.613927  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.14789 (* 1 = 0.14789 loss)
I1127 15:15:57.613970  2966 sgd_solver.cpp:106] Iteration 5100, lr = 8.725e-06
I1127 15:16:59.802171  2966 solver.cpp:228] Iteration 5200, loss = 0.864836
I1127 15:16:59.802307  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.864837 (* 1 = 0.864837 loss)
I1127 15:16:59.802350  2966 sgd_solver.cpp:106] Iteration 5200, lr = 8.7e-06
I1127 15:18:01.229714  2966 solver.cpp:228] Iteration 5300, loss = 0.337939
I1127 15:18:01.229897  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.33794 (* 1 = 0.33794 loss)
I1127 15:18:01.229939  2966 sgd_solver.cpp:106] Iteration 5300, lr = 8.675e-06
I1127 15:18:15.305222  2966 solver.cpp:337] Iteration 5324, Testing net (#0)
I1127 15:18:27.993276  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.688953
I1127 15:18:27.993391  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.942739 (* 1 = 0.942739 loss)
I1127 15:19:16.034961  2966 solver.cpp:228] Iteration 5400, loss = 0.254586
I1127 15:19:16.035099  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.254588 (* 1 = 0.254588 loss)
I1127 15:19:16.035142  2966 sgd_solver.cpp:106] Iteration 5400, lr = 8.65e-06
I1127 15:20:17.282809  2966 solver.cpp:228] Iteration 5500, loss = 0.141925
I1127 15:20:17.282944  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.141926 (* 1 = 0.141926 loss)
I1127 15:20:17.282989  2966 sgd_solver.cpp:106] Iteration 5500, lr = 8.625e-06
I1127 15:20:57.749287  2966 solver.cpp:337] Iteration 5566, Testing net (#0)
I1127 15:21:10.744575  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.68314
I1127 15:21:10.744690  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.945146 (* 1 = 0.945146 loss)
I1127 15:21:32.154032  2966 solver.cpp:228] Iteration 5600, loss = 1.07856
I1127 15:21:32.154199  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.07856 (* 1 = 1.07856 loss)
I1127 15:21:32.154247  2966 sgd_solver.cpp:106] Iteration 5600, lr = 8.6e-06
I1127 15:22:32.554769  2966 solver.cpp:228] Iteration 5700, loss = 0.0787824
I1127 15:22:32.554905  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0787836 (* 1 = 0.0787836 loss)
I1127 15:22:32.554949  2966 sgd_solver.cpp:106] Iteration 5700, lr = 8.575e-06
I1127 15:23:34.254143  2966 solver.cpp:228] Iteration 5800, loss = 0.683497
I1127 15:23:34.254295  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.683498 (* 1 = 0.683498 loss)
I1127 15:23:34.254364  2966 sgd_solver.cpp:106] Iteration 5800, lr = 8.55e-06
I1127 15:23:39.171465  2966 solver.cpp:337] Iteration 5808, Testing net (#0)
I1127 15:23:51.899513  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.718023
I1127 15:23:51.899641  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.889308 (* 1 = 0.889308 loss)
I1127 15:24:49.420855  2966 solver.cpp:228] Iteration 5900, loss = 0.557875
I1127 15:24:49.421041  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.557876 (* 1 = 0.557876 loss)
I1127 15:24:49.421084  2966 sgd_solver.cpp:106] Iteration 5900, lr = 8.525e-06
I1127 15:25:50.367358  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_6000.caffemodel
I1127 15:25:50.486392  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_6000.solverstate
I1127 15:25:51.180079  2966 solver.cpp:228] Iteration 6000, loss = 0.57874
I1127 15:25:51.180263  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.578742 (* 1 = 0.578742 loss)
I1127 15:25:51.180308  2966 sgd_solver.cpp:106] Iteration 6000, lr = 8.5e-06
I1127 15:26:22.899530  2966 solver.cpp:337] Iteration 6050, Testing net (#0)
I1127 15:26:36.043071  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.668605
I1127 15:26:36.043182  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.934639 (* 1 = 0.934639 loss)
I1127 15:27:08.321789  2966 solver.cpp:228] Iteration 6100, loss = 0.350046
I1127 15:27:08.321954  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.350047 (* 1 = 0.350047 loss)
I1127 15:27:08.322015  2966 sgd_solver.cpp:106] Iteration 6100, lr = 8.475e-06
I1127 15:28:10.035939  2966 solver.cpp:228] Iteration 6200, loss = 0.441455
I1127 15:28:10.036077  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.441456 (* 1 = 0.441456 loss)
I1127 15:28:10.036118  2966 sgd_solver.cpp:106] Iteration 6200, lr = 8.45e-06
I1127 15:29:05.794302  2966 solver.cpp:337] Iteration 6292, Testing net (#0)
I1127 15:29:18.572465  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.709302
I1127 15:29:18.572580  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.884359 (* 1 = 0.884359 loss)
I1127 15:29:24.139494  2966 solver.cpp:228] Iteration 6300, loss = 0.133617
I1127 15:29:24.139698  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.133618 (* 1 = 0.133618 loss)
I1127 15:29:24.139742  2966 sgd_solver.cpp:106] Iteration 6300, lr = 8.425e-06
I1127 15:30:43.079944  2966 solver.cpp:228] Iteration 6400, loss = 0.249308
I1127 15:30:43.080075  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.249309 (* 1 = 0.249309 loss)
I1127 15:30:43.080118  2966 sgd_solver.cpp:106] Iteration 6400, lr = 8.4e-06
I1127 15:31:51.928215  2966 solver.cpp:228] Iteration 6500, loss = 0.414937
I1127 15:31:51.928351  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.414938 (* 1 = 0.414938 loss)
I1127 15:31:51.928393  2966 sgd_solver.cpp:106] Iteration 6500, lr = 8.375e-06
I1127 15:32:12.813802  2966 solver.cpp:337] Iteration 6534, Testing net (#0)
I1127 15:32:31.163739  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.668605
I1127 15:32:31.163888  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.945406 (* 1 = 0.945406 loss)
I1127 15:33:14.993284  2966 solver.cpp:228] Iteration 6600, loss = 0.687469
I1127 15:33:14.993446  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.687471 (* 1 = 0.687471 loss)
I1127 15:33:14.993490  2966 sgd_solver.cpp:106] Iteration 6600, lr = 8.35e-06
I1127 15:34:23.742357  2966 solver.cpp:228] Iteration 6700, loss = 0.630717
I1127 15:34:23.742489  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.630719 (* 1 = 0.630719 loss)
I1127 15:34:23.742532  2966 sgd_solver.cpp:106] Iteration 6700, lr = 8.325e-06
I1127 15:35:18.280043  2966 solver.cpp:337] Iteration 6776, Testing net (#0)
I1127 15:35:34.621744  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.706395
I1127 15:35:34.621857  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.903255 (* 1 = 0.903255 loss)
I1127 15:35:51.065510  2966 solver.cpp:228] Iteration 6800, loss = 0.14506
I1127 15:35:51.065641  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.145062 (* 1 = 0.145062 loss)
I1127 15:35:51.065685  2966 sgd_solver.cpp:106] Iteration 6800, lr = 8.3e-06
I1127 15:36:54.972908  2966 solver.cpp:228] Iteration 6900, loss = 0.455159
I1127 15:36:54.973047  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.45516 (* 1 = 0.45516 loss)
I1127 15:36:54.973090  2966 sgd_solver.cpp:106] Iteration 6900, lr = 8.275e-06
I1127 15:38:02.266410  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_7000.caffemodel
I1127 15:38:02.404364  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_7000.solverstate
I1127 15:38:03.045547  2966 solver.cpp:228] Iteration 7000, loss = 1.39323
I1127 15:38:03.045655  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.39323 (* 1 = 1.39323 loss)
I1127 15:38:03.045698  2966 sgd_solver.cpp:106] Iteration 7000, lr = 8.25e-06
I1127 15:38:13.746282  2966 solver.cpp:337] Iteration 7018, Testing net (#0)
I1127 15:38:26.378690  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.72093
I1127 15:38:26.378801  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.789304 (* 1 = 0.789304 loss)
I1127 15:39:18.071184  2966 solver.cpp:228] Iteration 7100, loss = 0.119693
I1127 15:39:18.071378  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.119694 (* 1 = 0.119694 loss)
I1127 15:39:18.071422  2966 sgd_solver.cpp:106] Iteration 7100, lr = 8.225e-06
I1127 15:40:20.455413  2966 solver.cpp:228] Iteration 7200, loss = 0.490765
I1127 15:40:20.455554  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.490766 (* 1 = 0.490766 loss)
I1127 15:40:20.455597  2966 sgd_solver.cpp:106] Iteration 7200, lr = 8.2e-06
I1127 15:40:57.056529  2966 solver.cpp:337] Iteration 7260, Testing net (#0)
I1127 15:41:10.015933  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.700581
I1127 15:41:10.016048  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.9463 (* 1 = 0.9463 loss)
I1127 15:41:34.709219  2966 solver.cpp:228] Iteration 7300, loss = 0.42871
I1127 15:41:34.709367  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.428711 (* 1 = 0.428711 loss)
I1127 15:41:34.709409  2966 sgd_solver.cpp:106] Iteration 7300, lr = 8.175e-06
I1127 15:42:36.002753  2966 solver.cpp:228] Iteration 7400, loss = 0.121683
I1127 15:42:36.002898  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.121685 (* 1 = 0.121685 loss)
I1127 15:42:36.002941  2966 sgd_solver.cpp:106] Iteration 7400, lr = 8.15e-06
I1127 15:43:37.235406  2966 solver.cpp:228] Iteration 7500, loss = 0.42569
I1127 15:43:37.235565  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.425691 (* 1 = 0.425691 loss)
I1127 15:43:37.235611  2966 sgd_solver.cpp:106] Iteration 7500, lr = 8.125e-06
I1127 15:43:37.967008  2966 solver.cpp:337] Iteration 7502, Testing net (#0)
I1127 15:43:50.871611  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.732558
I1127 15:43:50.871726  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.821217 (* 1 = 0.821217 loss)
I1127 15:44:50.881856  2966 solver.cpp:228] Iteration 7600, loss = 0.498726
I1127 15:44:50.881989  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.498727 (* 1 = 0.498727 loss)
I1127 15:44:50.882032  2966 sgd_solver.cpp:106] Iteration 7600, lr = 8.1e-06
I1127 15:45:52.051779  2966 solver.cpp:228] Iteration 7700, loss = 0.236925
I1127 15:45:52.051913  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.236927 (* 1 = 0.236927 loss)
I1127 15:45:52.051957  2966 sgd_solver.cpp:106] Iteration 7700, lr = 8.075e-06
I1127 15:46:18.443907  2966 solver.cpp:337] Iteration 7744, Testing net (#0)
I1127 15:46:31.050716  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 15:46:31.050850  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.846328 (* 1 = 0.846328 loss)
I1127 15:47:06.069540  2966 solver.cpp:228] Iteration 7800, loss = 0.66844
I1127 15:47:06.069733  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.668441 (* 1 = 0.668441 loss)
I1127 15:47:06.069782  2966 sgd_solver.cpp:106] Iteration 7800, lr = 8.05e-06
I1127 15:48:07.114994  2966 solver.cpp:228] Iteration 7900, loss = 0.864811
I1127 15:48:07.115129  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.864812 (* 1 = 0.864812 loss)
I1127 15:48:07.115172  2966 sgd_solver.cpp:106] Iteration 7900, lr = 8.025e-06
I1127 15:48:59.273151  2966 solver.cpp:337] Iteration 7986, Testing net (#0)
I1127 15:49:12.039793  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.715116
I1127 15:49:12.039906  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.811602 (* 1 = 0.811602 loss)
I1127 15:49:20.668622  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_8000.caffemodel
I1127 15:49:20.790246  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_8000.solverstate
I1127 15:49:21.429724  2966 solver.cpp:228] Iteration 8000, loss = 0.557289
I1127 15:49:21.429839  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.55729 (* 1 = 0.55729 loss)
I1127 15:49:21.429885  2966 sgd_solver.cpp:106] Iteration 8000, lr = 8e-06
I1127 15:50:22.517977  2966 solver.cpp:228] Iteration 8100, loss = 0.505026
I1127 15:50:22.518115  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.505028 (* 1 = 0.505028 loss)
I1127 15:50:22.518204  2966 sgd_solver.cpp:106] Iteration 8100, lr = 7.975e-06
I1127 15:51:23.288471  2966 solver.cpp:228] Iteration 8200, loss = 0.469819
I1127 15:51:23.288607  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.46982 (* 1 = 0.46982 loss)
I1127 15:51:23.288650  2966 sgd_solver.cpp:106] Iteration 8200, lr = 7.95e-06
I1127 15:51:39.727581  2966 solver.cpp:337] Iteration 8228, Testing net (#0)
I1127 15:51:52.701344  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.712209
I1127 15:51:52.701455  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.875677 (* 1 = 0.875677 loss)
I1127 15:52:37.035917  2966 solver.cpp:228] Iteration 8300, loss = 0.661579
I1127 15:52:37.036052  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.66158 (* 1 = 0.66158 loss)
I1127 15:52:37.036095  2966 sgd_solver.cpp:106] Iteration 8300, lr = 7.925e-06
I1127 15:53:36.733217  2966 solver.cpp:228] Iteration 8400, loss = 0.688248
I1127 15:53:36.733353  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.688249 (* 1 = 0.688249 loss)
I1127 15:53:36.733397  2966 sgd_solver.cpp:106] Iteration 8400, lr = 7.9e-06
I1127 15:54:18.979668  2966 solver.cpp:337] Iteration 8470, Testing net (#0)
I1127 15:54:32.277756  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.718023
I1127 15:54:32.277858  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.878132 (* 1 = 0.878132 loss)
I1127 15:54:51.398180  2966 solver.cpp:228] Iteration 8500, loss = 0.0893242
I1127 15:54:51.398316  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0893252 (* 1 = 0.0893252 loss)
I1127 15:54:51.398360  2966 sgd_solver.cpp:106] Iteration 8500, lr = 7.875e-06
I1127 15:55:52.975796  2966 solver.cpp:228] Iteration 8600, loss = 0.30792
I1127 15:55:52.975934  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.307921 (* 1 = 0.307921 loss)
I1127 15:55:52.976059  2966 sgd_solver.cpp:106] Iteration 8600, lr = 7.85e-06
I1127 15:56:54.326853  2966 solver.cpp:228] Iteration 8700, loss = 0.787917
I1127 15:56:54.326987  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.787918 (* 1 = 0.787918 loss)
I1127 15:56:54.327046  2966 sgd_solver.cpp:106] Iteration 8700, lr = 7.825e-06
I1127 15:57:01.250716  2966 solver.cpp:337] Iteration 8712, Testing net (#0)
I1127 15:57:13.862260  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 15:57:13.862373  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.759999 (* 1 = 0.759999 loss)
I1127 15:58:07.980439  2966 solver.cpp:228] Iteration 8800, loss = 0.421056
I1127 15:58:07.980576  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.421057 (* 1 = 0.421057 loss)
I1127 15:58:07.980619  2966 sgd_solver.cpp:106] Iteration 8800, lr = 7.8e-06
I1127 15:59:09.384097  2966 solver.cpp:228] Iteration 8900, loss = 0.277409
I1127 15:59:09.384233  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.27741 (* 1 = 0.27741 loss)
I1127 15:59:09.384276  2966 sgd_solver.cpp:106] Iteration 8900, lr = 7.775e-06
I1127 15:59:41.935340  2966 solver.cpp:337] Iteration 8954, Testing net (#0)
I1127 15:59:54.733331  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.729651
I1127 15:59:54.733476  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.808736 (* 1 = 0.808736 loss)
I1127 16:00:23.598371  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_9000.caffemodel
I1127 16:00:23.729821  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_9000.solverstate
I1127 16:00:24.513800  2966 solver.cpp:228] Iteration 9000, loss = 0.303995
I1127 16:00:24.513972  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.303996 (* 1 = 0.303996 loss)
I1127 16:00:24.514020  2966 sgd_solver.cpp:106] Iteration 9000, lr = 7.75e-06
I1127 16:01:26.318137  2966 solver.cpp:228] Iteration 9100, loss = 0.694382
I1127 16:01:26.318296  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.694384 (* 1 = 0.694384 loss)
I1127 16:01:26.318341  2966 sgd_solver.cpp:106] Iteration 9100, lr = 7.725e-06
I1127 16:02:24.902237  2966 solver.cpp:337] Iteration 9196, Testing net (#0)
I1127 16:02:37.561419  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.700581
I1127 16:02:37.561599  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.876528 (* 1 = 0.876528 loss)
I1127 16:02:40.583992  2966 solver.cpp:228] Iteration 9200, loss = 0.509934
I1127 16:02:40.584154  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.509935 (* 1 = 0.509935 loss)
I1127 16:02:40.584205  2966 sgd_solver.cpp:106] Iteration 9200, lr = 7.7e-06
I1127 16:03:43.551568  2966 solver.cpp:228] Iteration 9300, loss = 0.367066
I1127 16:03:43.551766  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.367067 (* 1 = 0.367067 loss)
I1127 16:03:43.551816  2966 sgd_solver.cpp:106] Iteration 9300, lr = 7.675e-06
I1127 16:04:43.880372  2966 solver.cpp:228] Iteration 9400, loss = 0.88854
I1127 16:04:43.880512  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.888541 (* 1 = 0.888541 loss)
I1127 16:04:43.880555  2966 sgd_solver.cpp:106] Iteration 9400, lr = 7.65e-06
I1127 16:05:05.926230  2966 solver.cpp:337] Iteration 9438, Testing net (#0)
I1127 16:05:18.340986  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.703488
I1127 16:05:18.341122  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.889153 (* 1 = 0.889153 loss)
I1127 16:05:55.804486  2966 solver.cpp:228] Iteration 9500, loss = 0.567796
I1127 16:05:55.804625  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.567797 (* 1 = 0.567797 loss)
I1127 16:05:55.804667  2966 sgd_solver.cpp:106] Iteration 9500, lr = 7.625e-06
I1127 16:06:55.934073  2966 solver.cpp:228] Iteration 9600, loss = 0.512494
I1127 16:06:55.934206  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.512495 (* 1 = 0.512495 loss)
I1127 16:06:55.934247  2966 sgd_solver.cpp:106] Iteration 9600, lr = 7.6e-06
I1127 16:07:45.269657  2966 solver.cpp:337] Iteration 9680, Testing net (#0)
I1127 16:07:59.634995  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.738372
I1127 16:07:59.635167  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.801474 (* 1 = 0.801474 loss)
I1127 16:08:12.500859  2966 solver.cpp:228] Iteration 9700, loss = 0.381025
I1127 16:08:12.501037  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.381026 (* 1 = 0.381026 loss)
I1127 16:08:12.501096  2966 sgd_solver.cpp:106] Iteration 9700, lr = 7.575e-06
I1127 16:09:14.722472  2966 solver.cpp:228] Iteration 9800, loss = 0.36395
I1127 16:09:14.722609  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.363951 (* 1 = 0.363951 loss)
I1127 16:09:14.722652  2966 sgd_solver.cpp:106] Iteration 9800, lr = 7.55e-06
I1127 16:10:16.381937  2966 solver.cpp:228] Iteration 9900, loss = 0.414577
I1127 16:10:16.382235  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.414578 (* 1 = 0.414578 loss)
I1127 16:10:16.382277  2966 sgd_solver.cpp:106] Iteration 9900, lr = 7.525e-06
I1127 16:10:29.431313  2966 solver.cpp:337] Iteration 9922, Testing net (#0)
I1127 16:10:42.745483  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 16:10:42.745584  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.892196 (* 1 = 0.892196 loss)
I1127 16:11:32.868371  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_10000.caffemodel
I1127 16:11:32.994073  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_10000.solverstate
I1127 16:11:33.808147  2966 solver.cpp:228] Iteration 10000, loss = 0.124402
I1127 16:11:33.808310  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.124403 (* 1 = 0.124403 loss)
I1127 16:11:33.808354  2966 sgd_solver.cpp:106] Iteration 10000, lr = 7.5e-06
I1127 16:12:52.177012  2966 solver.cpp:228] Iteration 10100, loss = 0.305375
I1127 16:12:52.177211  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.305376 (* 1 = 0.305376 loss)
I1127 16:12:52.177255  2966 sgd_solver.cpp:106] Iteration 10100, lr = 7.475e-06
I1127 16:13:32.435768  2966 solver.cpp:337] Iteration 10164, Testing net (#0)
I1127 16:13:45.126515  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.723837
I1127 16:13:45.126627  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.762563 (* 1 = 0.762563 loss)
I1127 16:14:07.681221  2966 solver.cpp:228] Iteration 10200, loss = 0.265058
I1127 16:14:07.681759  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.265059 (* 1 = 0.265059 loss)
I1127 16:14:07.681871  2966 sgd_solver.cpp:106] Iteration 10200, lr = 7.45e-06
I1127 16:15:08.025787  2966 solver.cpp:228] Iteration 10300, loss = 0.472306
I1127 16:15:08.025912  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.472307 (* 1 = 0.472307 loss)
I1127 16:15:08.025954  2966 sgd_solver.cpp:106] Iteration 10300, lr = 7.425e-06
I1127 16:16:10.520784  2966 solver.cpp:228] Iteration 10400, loss = 1.75144
I1127 16:16:10.520948  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.75144 (* 1 = 1.75144 loss)
I1127 16:16:10.520992  2966 sgd_solver.cpp:106] Iteration 10400, lr = 7.4e-06
I1127 16:16:13.572031  2966 solver.cpp:337] Iteration 10406, Testing net (#0)
I1127 16:16:26.111197  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.712209
I1127 16:16:26.111332  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.855638 (* 1 = 0.855638 loss)
I1127 16:17:23.023394  2966 solver.cpp:228] Iteration 10500, loss = 0.713262
I1127 16:17:23.023533  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.713264 (* 1 = 0.713264 loss)
I1127 16:17:23.023577  2966 sgd_solver.cpp:106] Iteration 10500, lr = 7.375e-06
I1127 16:18:23.504240  2966 solver.cpp:228] Iteration 10600, loss = 0.359113
I1127 16:18:23.504415  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.359114 (* 1 = 0.359114 loss)
I1127 16:18:23.504462  2966 sgd_solver.cpp:106] Iteration 10600, lr = 7.35e-06
I1127 16:18:52.062371  2966 solver.cpp:337] Iteration 10648, Testing net (#0)
I1127 16:19:04.960841  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 16:19:04.960978  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.75432 (* 1 = 0.75432 loss)
I1127 16:19:38.252123  2966 solver.cpp:228] Iteration 10700, loss = 0.130254
I1127 16:19:38.252271  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.130256 (* 1 = 0.130256 loss)
I1127 16:19:38.252323  2966 sgd_solver.cpp:106] Iteration 10700, lr = 7.325e-06
I1127 16:20:41.301393  2966 solver.cpp:228] Iteration 10800, loss = 0.633184
I1127 16:20:41.301726  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.633185 (* 1 = 0.633185 loss)
I1127 16:20:41.301775  2966 sgd_solver.cpp:106] Iteration 10800, lr = 7.3e-06
I1127 16:21:35.808701  2966 solver.cpp:337] Iteration 10890, Testing net (#0)
I1127 16:21:49.139431  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 16:21:49.139536  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.791288 (* 1 = 0.791288 loss)
I1127 16:21:56.009205  2966 solver.cpp:228] Iteration 10900, loss = 0.238896
I1127 16:21:56.009318  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.238897 (* 1 = 0.238897 loss)
I1127 16:21:56.009361  2966 sgd_solver.cpp:106] Iteration 10900, lr = 7.275e-06
I1127 16:22:57.209039  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_11000.caffemodel
I1127 16:22:57.336133  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_11000.solverstate
I1127 16:22:58.012501  2966 solver.cpp:228] Iteration 11000, loss = 0.233449
I1127 16:22:58.012615  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.233451 (* 1 = 0.233451 loss)
I1127 16:22:58.012658  2966 sgd_solver.cpp:106] Iteration 11000, lr = 7.25e-06
I1127 16:24:00.484668  2966 solver.cpp:228] Iteration 11100, loss = 0.379001
I1127 16:24:00.484846  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.379002 (* 1 = 0.379002 loss)
I1127 16:24:00.484910  2966 sgd_solver.cpp:106] Iteration 11100, lr = 7.225e-06
I1127 16:24:19.693152  2966 solver.cpp:337] Iteration 11132, Testing net (#0)
I1127 16:24:32.955510  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 16:24:32.955647  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.816959 (* 1 = 0.816959 loss)
I1127 16:25:15.006978  2966 solver.cpp:228] Iteration 11200, loss = 0.355329
I1127 16:25:15.007105  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.355331 (* 1 = 0.355331 loss)
I1127 16:25:15.007148  2966 sgd_solver.cpp:106] Iteration 11200, lr = 7.2e-06
I1127 16:26:14.844425  2966 solver.cpp:228] Iteration 11300, loss = 0.662256
I1127 16:26:14.844558  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.662257 (* 1 = 0.662257 loss)
I1127 16:26:14.844601  2966 sgd_solver.cpp:106] Iteration 11300, lr = 7.175e-06
I1127 16:26:58.509186  2966 solver.cpp:337] Iteration 11374, Testing net (#0)
I1127 16:27:11.379693  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.723837
I1127 16:27:11.379804  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.803769 (* 1 = 0.803769 loss)
I1127 16:27:27.679564  2966 solver.cpp:228] Iteration 11400, loss = 0.845709
I1127 16:27:27.679728  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.84571 (* 1 = 0.84571 loss)
I1127 16:27:27.679772  2966 sgd_solver.cpp:106] Iteration 11400, lr = 7.15e-06
I1127 16:28:28.701844  2966 solver.cpp:228] Iteration 11500, loss = 0.684401
I1127 16:28:28.701977  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.684402 (* 1 = 0.684402 loss)
I1127 16:28:28.702019  2966 sgd_solver.cpp:106] Iteration 11500, lr = 7.125e-06
I1127 16:29:29.751451  2966 solver.cpp:228] Iteration 11600, loss = 0.116866
I1127 16:29:29.751605  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.116867 (* 1 = 0.116867 loss)
I1127 16:29:29.751649  2966 sgd_solver.cpp:106] Iteration 11600, lr = 7.1e-06
I1127 16:29:39.155706  2966 solver.cpp:337] Iteration 11616, Testing net (#0)
I1127 16:29:51.867921  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 16:29:51.868034  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.813744 (* 1 = 0.813744 loss)
I1127 16:30:42.993907  2966 solver.cpp:228] Iteration 11700, loss = 0.064507
I1127 16:30:42.994233  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0645078 (* 1 = 0.0645078 loss)
I1127 16:30:42.994277  2966 sgd_solver.cpp:106] Iteration 11700, lr = 7.075e-06
I1127 16:31:43.741184  2966 solver.cpp:228] Iteration 11800, loss = 0.134374
I1127 16:31:43.741322  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.134375 (* 1 = 0.134375 loss)
I1127 16:31:43.741364  2966 sgd_solver.cpp:106] Iteration 11800, lr = 7.05e-06
I1127 16:32:18.626207  2966 solver.cpp:337] Iteration 11858, Testing net (#0)
I1127 16:32:31.824517  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.712209
I1127 16:32:31.824631  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.838651 (* 1 = 0.838651 loss)
I1127 16:32:58.414369  2966 solver.cpp:228] Iteration 11900, loss = 0.0573964
I1127 16:32:58.414508  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0573977 (* 1 = 0.0573977 loss)
I1127 16:32:58.414552  2966 sgd_solver.cpp:106] Iteration 11900, lr = 7.025e-06
I1127 16:33:59.243372  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_12000.caffemodel
I1127 16:33:59.374784  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_12000.solverstate
I1127 16:34:00.116569  2966 solver.cpp:228] Iteration 12000, loss = 1.24928
I1127 16:34:00.116683  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.24928 (* 1 = 1.24928 loss)
I1127 16:34:00.116726  2966 sgd_solver.cpp:106] Iteration 12000, lr = 7e-06
I1127 16:35:00.751765  2966 solver.cpp:337] Iteration 12100, Testing net (#0)
I1127 16:35:13.888396  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 16:35:13.888602  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.789393 (* 1 = 0.789393 loss)
I1127 16:35:14.513574  2966 solver.cpp:228] Iteration 12100, loss = 0.158971
I1127 16:35:14.513727  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.158972 (* 1 = 0.158972 loss)
I1127 16:35:14.513787  2966 sgd_solver.cpp:106] Iteration 12100, lr = 6.975e-06
I1127 16:36:15.438096  2966 solver.cpp:228] Iteration 12200, loss = 0.121098
I1127 16:36:15.438232  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.121099 (* 1 = 0.121099 loss)
I1127 16:36:15.438275  2966 sgd_solver.cpp:106] Iteration 12200, lr = 6.95e-06
I1127 16:37:14.956223  2966 solver.cpp:228] Iteration 12300, loss = 0.45693
I1127 16:37:14.956372  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.456931 (* 1 = 0.456931 loss)
I1127 16:37:14.956415  2966 sgd_solver.cpp:106] Iteration 12300, lr = 6.925e-06
I1127 16:37:39.529567  2966 solver.cpp:337] Iteration 12342, Testing net (#0)
I1127 16:37:52.347004  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.738372
I1127 16:37:52.347137  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.833631 (* 1 = 0.833631 loss)
I1127 16:38:28.283349  2966 solver.cpp:228] Iteration 12400, loss = 0.209161
I1127 16:38:28.283483  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.209162 (* 1 = 0.209162 loss)
I1127 16:38:28.283525  2966 sgd_solver.cpp:106] Iteration 12400, lr = 6.9e-06
I1127 16:39:29.494761  2966 solver.cpp:228] Iteration 12500, loss = 0.0965704
I1127 16:39:29.494940  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0965712 (* 1 = 0.0965712 loss)
I1127 16:39:29.494982  2966 sgd_solver.cpp:106] Iteration 12500, lr = 6.875e-06
I1127 16:40:20.559222  2966 solver.cpp:337] Iteration 12584, Testing net (#0)
I1127 16:40:33.548867  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.712209
I1127 16:40:33.548984  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.870683 (* 1 = 0.870683 loss)
I1127 16:40:44.114487  2966 solver.cpp:228] Iteration 12600, loss = 0.358621
I1127 16:40:44.114619  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.358622 (* 1 = 0.358622 loss)
I1127 16:40:44.114666  2966 sgd_solver.cpp:106] Iteration 12600, lr = 6.85e-06
I1127 16:41:45.326635  2966 solver.cpp:228] Iteration 12700, loss = 0.52719
I1127 16:41:45.326930  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.527191 (* 1 = 0.527191 loss)
I1127 16:41:45.326973  2966 sgd_solver.cpp:106] Iteration 12700, lr = 6.825e-06
I1127 16:42:48.613797  2966 solver.cpp:228] Iteration 12800, loss = 0.26825
I1127 16:42:48.614009  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.268251 (* 1 = 0.268251 loss)
I1127 16:42:48.614053  2966 sgd_solver.cpp:106] Iteration 12800, lr = 6.8e-06
I1127 16:43:04.595885  2966 solver.cpp:337] Iteration 12826, Testing net (#0)
I1127 16:43:18.472839  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 16:43:18.472955  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.761536 (* 1 = 0.761536 loss)
I1127 16:44:07.448653  2966 solver.cpp:228] Iteration 12900, loss = 0.194539
I1127 16:44:07.448849  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.19454 (* 1 = 0.19454 loss)
I1127 16:44:07.448899  2966 sgd_solver.cpp:106] Iteration 12900, lr = 6.775e-06
I1127 16:45:13.035398  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_13000.caffemodel
I1127 16:45:13.171250  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_13000.solverstate
I1127 16:45:13.884212  2966 solver.cpp:228] Iteration 13000, loss = 0.3522
I1127 16:45:13.884326  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.3522 (* 1 = 0.3522 loss)
I1127 16:45:13.884369  2966 sgd_solver.cpp:106] Iteration 13000, lr = 6.75e-06
I1127 16:45:57.585995  2966 solver.cpp:337] Iteration 13068, Testing net (#0)
I1127 16:46:11.081823  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.729651
I1127 16:46:11.082825  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.854962 (* 1 = 0.854962 loss)
I1127 16:46:32.943629  2966 solver.cpp:228] Iteration 13100, loss = 0.207902
I1127 16:46:32.943827  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.207903 (* 1 = 0.207903 loss)
I1127 16:46:32.943889  2966 sgd_solver.cpp:106] Iteration 13100, lr = 6.725e-06
I1127 16:47:38.954581  2966 solver.cpp:228] Iteration 13200, loss = 0.655048
I1127 16:47:38.954834  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.655049 (* 1 = 0.655049 loss)
I1127 16:47:38.954880  2966 sgd_solver.cpp:106] Iteration 13200, lr = 6.7e-06
I1127 16:48:39.417003  2966 solver.cpp:228] Iteration 13300, loss = 0.170696
I1127 16:48:39.417135  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.170697 (* 1 = 0.170697 loss)
I1127 16:48:39.417176  2966 sgd_solver.cpp:106] Iteration 13300, lr = 6.675e-06
I1127 16:48:44.876981  2966 solver.cpp:337] Iteration 13310, Testing net (#0)
I1127 16:48:57.756176  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.69186
I1127 16:48:57.756320  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.88456 (* 1 = 0.88456 loss)
I1127 16:49:53.638656  2966 solver.cpp:228] Iteration 13400, loss = 0.658914
I1127 16:49:53.638794  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.658914 (* 1 = 0.658914 loss)
I1127 16:49:53.638839  2966 sgd_solver.cpp:106] Iteration 13400, lr = 6.65e-06
I1127 16:50:54.394067  2966 solver.cpp:228] Iteration 13500, loss = 0.116852
I1127 16:50:54.394214  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.116853 (* 1 = 0.116853 loss)
I1127 16:50:54.394258  2966 sgd_solver.cpp:106] Iteration 13500, lr = 6.625e-06
I1127 16:51:25.516659  2966 solver.cpp:337] Iteration 13552, Testing net (#0)
I1127 16:51:38.277329  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.738372
I1127 16:51:38.277446  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.730878 (* 1 = 0.730878 loss)
I1127 16:52:08.092131  2966 solver.cpp:228] Iteration 13600, loss = 0.330809
I1127 16:52:08.092432  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.33081 (* 1 = 0.33081 loss)
I1127 16:52:08.092476  2966 sgd_solver.cpp:106] Iteration 13600, lr = 6.6e-06
I1127 16:53:10.599536  2966 solver.cpp:228] Iteration 13700, loss = 0.326659
I1127 16:53:10.599673  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.32666 (* 1 = 0.32666 loss)
I1127 16:53:10.599794  2966 sgd_solver.cpp:106] Iteration 13700, lr = 6.575e-06
I1127 16:54:07.638321  2966 solver.cpp:337] Iteration 13794, Testing net (#0)
I1127 16:54:20.464848  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.718023
I1127 16:54:20.464962  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.907068 (* 1 = 0.907068 loss)
I1127 16:54:24.717888  2966 solver.cpp:228] Iteration 13800, loss = 0.113686
I1127 16:54:24.717995  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.113687 (* 1 = 0.113687 loss)
I1127 16:54:24.718039  2966 sgd_solver.cpp:106] Iteration 13800, lr = 6.55e-06
I1127 16:55:25.785941  2966 solver.cpp:228] Iteration 13900, loss = 0.0845202
I1127 16:55:25.786089  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0845211 (* 1 = 0.0845211 loss)
I1127 16:55:25.786133  2966 sgd_solver.cpp:106] Iteration 13900, lr = 6.525e-06
I1127 16:56:26.157054  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_14000.caffemodel
I1127 16:56:26.278537  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_14000.solverstate
I1127 16:56:26.942272  2966 solver.cpp:228] Iteration 14000, loss = 0.100655
I1127 16:56:26.942385  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.100656 (* 1 = 0.100656 loss)
I1127 16:56:26.942428  2966 sgd_solver.cpp:106] Iteration 14000, lr = 6.5e-06
I1127 16:56:49.154645  2966 solver.cpp:337] Iteration 14036, Testing net (#0)
I1127 16:57:02.480242  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 16:57:02.480443  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.787137 (* 1 = 0.787137 loss)
I1127 16:57:43.038359  2966 solver.cpp:228] Iteration 14100, loss = 0.0761399
I1127 16:57:43.038494  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.076141 (* 1 = 0.076141 loss)
I1127 16:57:43.038537  2966 sgd_solver.cpp:106] Iteration 14100, lr = 6.475e-06
I1127 16:58:44.840050  2966 solver.cpp:228] Iteration 14200, loss = 0.118412
I1127 16:58:44.840191  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.118413 (* 1 = 0.118413 loss)
I1127 16:58:44.840235  2966 sgd_solver.cpp:106] Iteration 14200, lr = 6.45e-06
I1127 16:59:35.831830  2966 solver.cpp:337] Iteration 14278, Testing net (#0)
I1127 16:59:48.979360  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.738372
I1127 16:59:48.979472  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.782516 (* 1 = 0.782516 loss)
I1127 17:00:03.511806  2966 solver.cpp:228] Iteration 14300, loss = 0.518574
I1127 17:00:03.511932  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.518575 (* 1 = 0.518575 loss)
I1127 17:00:03.511976  2966 sgd_solver.cpp:106] Iteration 14300, lr = 6.425e-06
I1127 17:01:06.184788  2966 solver.cpp:228] Iteration 14400, loss = 0.823799
I1127 17:01:06.184926  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.823801 (* 1 = 0.823801 loss)
I1127 17:01:06.184968  2966 sgd_solver.cpp:106] Iteration 14400, lr = 6.4e-06
I1127 17:02:08.913832  2966 solver.cpp:228] Iteration 14500, loss = 0.0437355
I1127 17:02:08.913967  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0437368 (* 1 = 0.0437368 loss)
I1127 17:02:08.914011  2966 sgd_solver.cpp:106] Iteration 14500, lr = 6.375e-06
I1127 17:02:20.608937  2966 solver.cpp:337] Iteration 14520, Testing net (#0)
I1127 17:02:33.231003  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.694767
I1127 17:02:33.231117  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.888786 (* 1 = 0.888786 loss)
I1127 17:03:22.644706  2966 solver.cpp:228] Iteration 14600, loss = 0.19224
I1127 17:03:22.645012  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.192241 (* 1 = 0.192241 loss)
I1127 17:03:22.645057  2966 sgd_solver.cpp:106] Iteration 14600, lr = 6.35e-06
I1127 17:04:25.213024  2966 solver.cpp:228] Iteration 14700, loss = 0.131393
I1127 17:04:25.213230  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.131394 (* 1 = 0.131394 loss)
I1127 17:04:25.213279  2966 sgd_solver.cpp:106] Iteration 14700, lr = 6.325e-06
I1127 17:05:03.542666  2966 solver.cpp:337] Iteration 14762, Testing net (#0)
I1127 17:05:16.775243  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 17:05:16.775352  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.762335 (* 1 = 0.762335 loss)
I1127 17:05:42.172405  2966 solver.cpp:228] Iteration 14800, loss = 0.43209
I1127 17:05:42.172590  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.432091 (* 1 = 0.432091 loss)
I1127 17:05:42.172636  2966 sgd_solver.cpp:106] Iteration 14800, lr = 6.3e-06
I1127 17:06:43.512471  2966 solver.cpp:228] Iteration 14900, loss = 0.148119
I1127 17:06:43.512624  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.14812 (* 1 = 0.14812 loss)
I1127 17:06:43.512670  2966 sgd_solver.cpp:106] Iteration 14900, lr = 6.275e-06
I1127 17:07:45.281057  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_15000.caffemodel
I1127 17:07:45.401113  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_15000.solverstate
I1127 17:07:46.118896  2966 solver.cpp:228] Iteration 15000, loss = 0.044677
I1127 17:07:46.118998  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.044678 (* 1 = 0.044678 loss)
I1127 17:07:46.119040  2966 sgd_solver.cpp:106] Iteration 15000, lr = 6.25e-06
I1127 17:07:48.096678  2966 solver.cpp:337] Iteration 15004, Testing net (#0)
I1127 17:08:01.119423  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 17:08:01.119537  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.775511 (* 1 = 0.775511 loss)
I1127 17:09:01.343461  2966 solver.cpp:228] Iteration 15100, loss = 0.142745
I1127 17:09:01.343600  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.142746 (* 1 = 0.142746 loss)
I1127 17:09:01.343641  2966 sgd_solver.cpp:106] Iteration 15100, lr = 6.225e-06
I1127 17:10:02.844910  2966 solver.cpp:228] Iteration 15200, loss = 0.309467
I1127 17:10:02.845057  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.309468 (* 1 = 0.309468 loss)
I1127 17:10:02.845115  2966 sgd_solver.cpp:106] Iteration 15200, lr = 6.2e-06
I1127 17:10:31.496953  2966 solver.cpp:337] Iteration 15246, Testing net (#0)
I1127 17:10:44.825170  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.741279
I1127 17:10:44.825312  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.823158 (* 1 = 0.823158 loss)
I1127 17:11:18.986632  2966 solver.cpp:228] Iteration 15300, loss = 0.311882
I1127 17:11:18.986763  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.311883 (* 1 = 0.311883 loss)
I1127 17:11:18.986806  2966 sgd_solver.cpp:106] Iteration 15300, lr = 6.175e-06
I1127 17:12:20.503595  2966 solver.cpp:228] Iteration 15400, loss = 0.150321
I1127 17:12:20.503743  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.150321 (* 1 = 0.150321 loss)
I1127 17:12:20.503787  2966 sgd_solver.cpp:106] Iteration 15400, lr = 6.15e-06
I1127 17:13:13.678491  2966 solver.cpp:337] Iteration 15488, Testing net (#0)
I1127 17:13:26.594362  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.726744
I1127 17:13:26.594466  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.834208 (* 1 = 0.834208 loss)
I1127 17:13:34.917678  2966 solver.cpp:228] Iteration 15500, loss = 0.0413099
I1127 17:13:34.917790  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0413105 (* 1 = 0.0413105 loss)
I1127 17:13:34.917832  2966 sgd_solver.cpp:106] Iteration 15500, lr = 6.125e-06
I1127 17:14:36.321154  2966 solver.cpp:228] Iteration 15600, loss = 0.208849
I1127 17:14:36.321297  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.20885 (* 1 = 0.20885 loss)
I1127 17:14:36.321341  2966 sgd_solver.cpp:106] Iteration 15600, lr = 6.1e-06
I1127 17:15:37.296098  2966 solver.cpp:228] Iteration 15700, loss = 0.36829
I1127 17:15:37.296234  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.36829 (* 1 = 0.36829 loss)
I1127 17:15:37.296277  2966 sgd_solver.cpp:106] Iteration 15700, lr = 6.075e-06
I1127 17:15:54.877584  2966 solver.cpp:337] Iteration 15730, Testing net (#0)
I1127 17:16:07.551506  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.732558
I1127 17:16:07.551640  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.765575 (* 1 = 0.765575 loss)
I1127 17:16:52.508998  2966 solver.cpp:228] Iteration 15800, loss = 0.604089
I1127 17:16:52.509161  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.604089 (* 1 = 0.604089 loss)
I1127 17:16:52.509207  2966 sgd_solver.cpp:106] Iteration 15800, lr = 6.05e-06
I1127 17:18:00.776440  2966 solver.cpp:228] Iteration 15900, loss = 0.324026
I1127 17:18:00.776573  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.324027 (* 1 = 0.324027 loss)
I1127 17:18:00.776615  2966 sgd_solver.cpp:106] Iteration 15900, lr = 6.025e-06
I1127 17:18:43.941519  2966 solver.cpp:337] Iteration 15972, Testing net (#0)
I1127 17:18:57.075541  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 17:18:57.075690  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.808833 (* 1 = 0.808833 loss)
I1127 17:19:14.174738  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_16000.caffemodel
I1127 17:19:14.307425  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_16000.solverstate
I1127 17:19:14.957543  2966 solver.cpp:228] Iteration 16000, loss = 0.229734
I1127 17:19:14.957657  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.229735 (* 1 = 0.229735 loss)
I1127 17:19:14.957700  2966 sgd_solver.cpp:106] Iteration 16000, lr = 6e-06
I1127 17:20:14.469691  2966 solver.cpp:228] Iteration 16100, loss = 0.205922
I1127 17:20:14.469827  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.205922 (* 1 = 0.205922 loss)
I1127 17:20:14.469903  2966 sgd_solver.cpp:106] Iteration 16100, lr = 5.975e-06
I1127 17:21:14.690721  2966 solver.cpp:228] Iteration 16200, loss = 0.275563
I1127 17:21:14.690884  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.275564 (* 1 = 0.275564 loss)
I1127 17:21:14.690929  2966 sgd_solver.cpp:106] Iteration 16200, lr = 5.95e-06
I1127 17:21:23.060115  2966 solver.cpp:337] Iteration 16214, Testing net (#0)
I1127 17:21:36.277979  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 17:21:36.278090  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.793085 (* 1 = 0.793085 loss)
I1127 17:22:30.211395  2966 solver.cpp:228] Iteration 16300, loss = 0.275796
I1127 17:22:30.211529  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.275797 (* 1 = 0.275797 loss)
I1127 17:22:30.211572  2966 sgd_solver.cpp:106] Iteration 16300, lr = 5.925e-06
I1127 17:23:31.558899  2966 solver.cpp:228] Iteration 16400, loss = 0.691291
I1127 17:23:31.559033  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.691291 (* 1 = 0.691291 loss)
I1127 17:23:31.559077  2966 sgd_solver.cpp:106] Iteration 16400, lr = 5.9e-06
I1127 17:24:07.302173  2966 solver.cpp:337] Iteration 16456, Testing net (#0)
I1127 17:24:20.443519  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 17:24:20.443637  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.820585 (* 1 = 0.820585 loss)
I1127 17:24:48.517843  2966 solver.cpp:228] Iteration 16500, loss = 0.118828
I1127 17:24:48.517997  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.118828 (* 1 = 0.118828 loss)
I1127 17:24:48.518043  2966 sgd_solver.cpp:106] Iteration 16500, lr = 5.875e-06
I1127 17:25:50.763365  2966 solver.cpp:228] Iteration 16600, loss = 0.442645
I1127 17:25:50.763501  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.442645 (* 1 = 0.442645 loss)
I1127 17:25:50.763545  2966 sgd_solver.cpp:106] Iteration 16600, lr = 5.85e-06
I1127 17:26:50.961504  2966 solver.cpp:337] Iteration 16698, Testing net (#0)
I1127 17:27:04.442416  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 17:27:04.443313  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.758356 (* 1 = 0.758356 loss)
I1127 17:27:06.361557  2966 solver.cpp:228] Iteration 16700, loss = 0.538863
I1127 17:27:06.361682  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.538863 (* 1 = 0.538863 loss)
I1127 17:27:06.361735  2966 sgd_solver.cpp:106] Iteration 16700, lr = 5.825e-06
I1127 17:28:08.503023  2966 solver.cpp:228] Iteration 16800, loss = 0.148545
I1127 17:28:08.503171  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.148545 (* 1 = 0.148545 loss)
I1127 17:28:08.503214  2966 sgd_solver.cpp:106] Iteration 16800, lr = 5.8e-06
I1127 17:29:10.069610  2966 solver.cpp:228] Iteration 16900, loss = 0.157557
I1127 17:29:10.069747  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.157558 (* 1 = 0.157558 loss)
I1127 17:29:10.069790  2966 sgd_solver.cpp:106] Iteration 16900, lr = 5.775e-06
I1127 17:29:35.568395  2966 solver.cpp:337] Iteration 16940, Testing net (#0)
I1127 17:29:49.353921  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.741279
I1127 17:29:49.354053  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.778146 (* 1 = 0.778146 loss)
I1127 17:30:27.100340  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_17000.caffemodel
I1127 17:30:27.219969  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_17000.solverstate
I1127 17:30:27.888563  2966 solver.cpp:228] Iteration 17000, loss = 0.490025
I1127 17:30:27.888671  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.490027 (* 1 = 0.490027 loss)
I1127 17:30:27.888715  2966 sgd_solver.cpp:106] Iteration 17000, lr = 5.75e-06
I1127 17:31:29.653790  2966 solver.cpp:228] Iteration 17100, loss = 0.688314
I1127 17:31:29.653921  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.688316 (* 1 = 0.688316 loss)
I1127 17:31:29.653964  2966 sgd_solver.cpp:106] Iteration 17100, lr = 5.725e-06
I1127 17:32:20.203200  2966 solver.cpp:337] Iteration 17182, Testing net (#0)
I1127 17:32:33.574101  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.732558
I1127 17:32:33.574215  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.760757 (* 1 = 0.760757 loss)
I1127 17:32:45.910776  2966 solver.cpp:228] Iteration 17200, loss = 0.50408
I1127 17:32:45.910881  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.504081 (* 1 = 0.504081 loss)
I1127 17:32:45.910923  2966 sgd_solver.cpp:106] Iteration 17200, lr = 5.7e-06
I1127 17:33:48.123083  2966 solver.cpp:228] Iteration 17300, loss = 0.616079
I1127 17:33:48.123334  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.616081 (* 1 = 0.616081 loss)
I1127 17:33:48.123378  2966 sgd_solver.cpp:106] Iteration 17300, lr = 5.675e-06
I1127 17:34:49.950112  2966 solver.cpp:228] Iteration 17400, loss = 0.695625
I1127 17:34:49.950438  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.695627 (* 1 = 0.695627 loss)
I1127 17:34:49.950482  2966 sgd_solver.cpp:106] Iteration 17400, lr = 5.65e-06
I1127 17:35:04.504461  2966 solver.cpp:337] Iteration 17424, Testing net (#0)
I1127 17:35:17.737195  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.729651
I1127 17:35:17.737323  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.859137 (* 1 = 0.859137 loss)
I1127 17:36:05.535143  2966 solver.cpp:228] Iteration 17500, loss = 0.19369
I1127 17:36:05.535284  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.193692 (* 1 = 0.193692 loss)
I1127 17:36:05.535327  2966 sgd_solver.cpp:106] Iteration 17500, lr = 5.625e-06
I1127 17:37:06.765267  2966 solver.cpp:228] Iteration 17600, loss = 0.278215
I1127 17:37:06.765404  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.278216 (* 1 = 0.278216 loss)
I1127 17:37:06.765480  2966 sgd_solver.cpp:106] Iteration 17600, lr = 5.6e-06
I1127 17:37:45.468950  2966 solver.cpp:337] Iteration 17666, Testing net (#0)
I1127 17:37:58.022251  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.732558
I1127 17:37:58.022362  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.80021 (* 1 = 0.80021 loss)
I1127 17:38:19.513334  2966 solver.cpp:228] Iteration 17700, loss = 0.860269
I1127 17:38:19.513473  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.86027 (* 1 = 0.86027 loss)
I1127 17:38:19.513517  2966 sgd_solver.cpp:106] Iteration 17700, lr = 5.575e-06
I1127 17:39:24.690955  2966 solver.cpp:228] Iteration 17800, loss = 0.478808
I1127 17:39:24.691082  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.47881 (* 1 = 0.47881 loss)
I1127 17:39:24.691124  2966 sgd_solver.cpp:106] Iteration 17800, lr = 5.55e-06
I1127 17:40:27.771147  2966 solver.cpp:228] Iteration 17900, loss = 0.0923679
I1127 17:40:27.771282  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0923691 (* 1 = 0.0923691 loss)
I1127 17:40:27.771325  2966 sgd_solver.cpp:106] Iteration 17900, lr = 5.525e-06
I1127 17:40:32.203320  2966 solver.cpp:337] Iteration 17908, Testing net (#0)
I1127 17:40:44.788705  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.747093
I1127 17:40:44.788828  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.787267 (* 1 = 0.787267 loss)
I1127 17:41:46.404511  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_18000.caffemodel
I1127 17:41:46.514879  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_18000.solverstate
I1127 17:41:47.175362  2966 solver.cpp:228] Iteration 18000, loss = 0.45899
I1127 17:41:47.175478  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.458991 (* 1 = 0.458991 loss)
I1127 17:41:47.175520  2966 sgd_solver.cpp:106] Iteration 18000, lr = 5.5e-06
I1127 17:42:50.287529  2966 solver.cpp:228] Iteration 18100, loss = 0.365876
I1127 17:42:50.287657  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.365877 (* 1 = 0.365877 loss)
I1127 17:42:50.287698  2966 sgd_solver.cpp:106] Iteration 18100, lr = 5.475e-06
I1127 17:43:22.603710  2966 solver.cpp:337] Iteration 18150, Testing net (#0)
I1127 17:43:35.938763  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.767442
I1127 17:43:35.938876  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.748955 (* 1 = 0.748955 loss)
I1127 17:44:07.436965  2966 solver.cpp:228] Iteration 18200, loss = 0.60866
I1127 17:44:07.437170  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.608661 (* 1 = 0.608661 loss)
I1127 17:44:07.437233  2966 sgd_solver.cpp:106] Iteration 18200, lr = 5.45e-06
I1127 17:45:08.343438  2966 solver.cpp:228] Iteration 18300, loss = 0.410585
I1127 17:45:08.343732  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.410586 (* 1 = 0.410586 loss)
I1127 17:45:08.343777  2966 sgd_solver.cpp:106] Iteration 18300, lr = 5.425e-06
I1127 17:46:04.210690  2966 solver.cpp:337] Iteration 18392, Testing net (#0)
I1127 17:46:16.943871  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.741279
I1127 17:46:16.944021  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.812444 (* 1 = 0.812444 loss)
I1127 17:46:22.472018  2966 solver.cpp:228] Iteration 18400, loss = 0.0565696
I1127 17:46:22.472131  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0565705 (* 1 = 0.0565705 loss)
I1127 17:46:22.472173  2966 sgd_solver.cpp:106] Iteration 18400, lr = 5.4e-06
I1127 17:47:23.990502  2966 solver.cpp:228] Iteration 18500, loss = 0.202285
I1127 17:47:23.990674  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.202286 (* 1 = 0.202286 loss)
I1127 17:47:23.990716  2966 sgd_solver.cpp:106] Iteration 18500, lr = 5.375e-06
I1127 17:48:25.278236  2966 solver.cpp:228] Iteration 18600, loss = 0.436275
I1127 17:48:25.278399  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.436276 (* 1 = 0.436276 loss)
I1127 17:48:25.278461  2966 sgd_solver.cpp:106] Iteration 18600, lr = 5.35e-06
I1127 17:48:45.282115  2966 solver.cpp:337] Iteration 18634, Testing net (#0)
I1127 17:48:58.685278  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 17:48:58.685411  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.799766 (* 1 = 0.799766 loss)
I1127 17:49:40.527375  2966 solver.cpp:228] Iteration 18700, loss = 0.359192
I1127 17:49:40.527611  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.359193 (* 1 = 0.359193 loss)
I1127 17:49:40.527683  2966 sgd_solver.cpp:106] Iteration 18700, lr = 5.325e-06
I1127 17:50:53.706832  2966 solver.cpp:228] Iteration 18800, loss = 0.266926
I1127 17:50:53.706972  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.266927 (* 1 = 0.266927 loss)
I1127 17:50:53.707033  2966 sgd_solver.cpp:106] Iteration 18800, lr = 5.3e-06
I1127 17:52:24.982861  2966 solver.cpp:337] Iteration 18876, Testing net (#0)
I1127 17:52:37.638648  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.700581
I1127 17:52:37.638764  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.835383 (* 1 = 0.835383 loss)
I1127 17:52:57.891441  2966 solver.cpp:228] Iteration 18900, loss = 0.187115
I1127 17:52:57.891573  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.187116 (* 1 = 0.187116 loss)
I1127 17:52:57.891616  2966 sgd_solver.cpp:106] Iteration 18900, lr = 5.275e-06
I1127 17:53:58.460804  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_19000.caffemodel
I1127 17:53:58.579481  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_19000.solverstate
I1127 17:53:59.207620  2966 solver.cpp:228] Iteration 19000, loss = 0.0756076
I1127 17:53:59.207733  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0756084 (* 1 = 0.0756084 loss)
I1127 17:53:59.207777  2966 sgd_solver.cpp:106] Iteration 19000, lr = 5.25e-06
I1127 17:55:04.540594  2966 solver.cpp:228] Iteration 19100, loss = 0.543035
I1127 17:55:04.540761  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.543036 (* 1 = 0.543036 loss)
I1127 17:55:04.540881  2966 sgd_solver.cpp:106] Iteration 19100, lr = 5.225e-06
I1127 17:55:15.724472  2966 solver.cpp:337] Iteration 19118, Testing net (#0)
I1127 17:55:29.529947  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.752907
I1127 17:55:29.530056  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.776688 (* 1 = 0.776688 loss)
I1127 17:56:23.699908  2966 solver.cpp:228] Iteration 19200, loss = 0.294749
I1127 17:56:23.700043  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.29475 (* 1 = 0.29475 loss)
I1127 17:56:23.700086  2966 sgd_solver.cpp:106] Iteration 19200, lr = 5.2e-06
I1127 17:57:30.009227  2966 solver.cpp:228] Iteration 19300, loss = 0.0640553
I1127 17:57:30.009615  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0640558 (* 1 = 0.0640558 loss)
I1127 17:57:30.009665  2966 sgd_solver.cpp:106] Iteration 19300, lr = 5.175e-06
I1127 17:58:07.901291  2966 solver.cpp:337] Iteration 19360, Testing net (#0)
I1127 17:58:21.889503  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.738372
I1127 17:58:21.889636  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.787012 (* 1 = 0.787012 loss)
I1127 17:58:48.361420  2966 solver.cpp:228] Iteration 19400, loss = 0.050258
I1127 17:58:48.361559  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0502586 (* 1 = 0.0502586 loss)
I1127 17:58:48.361603  2966 sgd_solver.cpp:106] Iteration 19400, lr = 5.15e-06
I1127 17:59:53.387011  2966 solver.cpp:228] Iteration 19500, loss = 0.0509065
I1127 17:59:53.387142  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0509072 (* 1 = 0.0509072 loss)
I1127 17:59:53.387186  2966 sgd_solver.cpp:106] Iteration 19500, lr = 5.125e-06
I1127 18:00:58.271260  2966 solver.cpp:228] Iteration 19600, loss = 0.0754888
I1127 18:00:58.271394  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0754892 (* 1 = 0.0754892 loss)
I1127 18:00:58.271436  2966 sgd_solver.cpp:106] Iteration 19600, lr = 5.1e-06
I1127 18:00:59.037580  2966 solver.cpp:337] Iteration 19602, Testing net (#0)
I1127 18:01:12.871232  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.758721
I1127 18:01:12.871337  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.794696 (* 1 = 0.794696 loss)
I1127 18:02:17.897925  2966 solver.cpp:228] Iteration 19700, loss = 0.671894
I1127 18:02:17.898063  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.671895 (* 1 = 0.671895 loss)
I1127 18:02:17.898108  2966 sgd_solver.cpp:106] Iteration 19700, lr = 5.075e-06
I1127 18:03:21.631235  2966 solver.cpp:228] Iteration 19800, loss = 0.176305
I1127 18:03:21.631376  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.176305 (* 1 = 0.176305 loss)
I1127 18:03:21.631419  2966 sgd_solver.cpp:106] Iteration 19800, lr = 5.05e-06
I1127 18:03:49.290719  2966 solver.cpp:337] Iteration 19844, Testing net (#0)
I1127 18:04:03.142048  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.718023
I1127 18:04:03.142184  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.786026 (* 1 = 0.786026 loss)
I1127 18:04:40.098014  2966 solver.cpp:228] Iteration 19900, loss = 0.702881
I1127 18:04:40.098141  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.702881 (* 1 = 0.702881 loss)
I1127 18:04:40.098184  2966 sgd_solver.cpp:106] Iteration 19900, lr = 5.025e-06
I1127 18:05:44.190384  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_20000.caffemodel
I1127 18:05:44.331693  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_20000.solverstate
I1127 18:05:45.048221  2966 solver.cpp:228] Iteration 20000, loss = 0.581715
I1127 18:05:45.048341  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.581716 (* 1 = 0.581716 loss)
I1127 18:05:45.048385  2966 sgd_solver.cpp:106] Iteration 20000, lr = 5e-06
I1127 18:06:40.457921  2966 solver.cpp:337] Iteration 20086, Testing net (#0)
I1127 18:06:54.333986  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.703488
I1127 18:06:54.334118  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.817643 (* 1 = 0.817643 loss)
I1127 18:07:04.361682  2966 solver.cpp:228] Iteration 20100, loss = 0.154937
I1127 18:07:04.361821  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.154938 (* 1 = 0.154938 loss)
I1127 18:07:04.361865  2966 sgd_solver.cpp:106] Iteration 20100, lr = 4.975e-06
I1127 18:08:08.925128  2966 solver.cpp:228] Iteration 20200, loss = 0.394684
I1127 18:08:08.925480  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.394684 (* 1 = 0.394684 loss)
I1127 18:08:08.925540  2966 sgd_solver.cpp:106] Iteration 20200, lr = 4.95e-06
I1127 18:09:15.769681  2966 solver.cpp:228] Iteration 20300, loss = 0.738805
I1127 18:09:15.769811  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.738805 (* 1 = 0.738805 loss)
I1127 18:09:15.769853  2966 sgd_solver.cpp:106] Iteration 20300, lr = 4.925e-06
I1127 18:09:33.820366  2966 solver.cpp:337] Iteration 20328, Testing net (#0)
I1127 18:09:47.434135  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.755814
I1127 18:09:47.434300  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.716612 (* 1 = 0.716612 loss)
I1127 18:10:33.208658  2966 solver.cpp:228] Iteration 20400, loss = 0.107553
I1127 18:10:33.208794  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.107554 (* 1 = 0.107554 loss)
I1127 18:10:33.208837  2966 sgd_solver.cpp:106] Iteration 20400, lr = 4.9e-06
I1127 18:11:35.248639  2966 solver.cpp:228] Iteration 20500, loss = 0.0788133
I1127 18:11:35.248792  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0788138 (* 1 = 0.0788138 loss)
I1127 18:11:35.248836  2966 sgd_solver.cpp:106] Iteration 20500, lr = 4.875e-06
I1127 18:12:18.260170  2966 solver.cpp:337] Iteration 20570, Testing net (#0)
I1127 18:12:31.720281  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 18:12:31.720396  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.744319 (* 1 = 0.744319 loss)
I1127 18:12:51.569150  2966 solver.cpp:228] Iteration 20600, loss = 0.024246
I1127 18:12:51.569293  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0242463 (* 1 = 0.0242463 loss)
I1127 18:12:51.569337  2966 sgd_solver.cpp:106] Iteration 20600, lr = 4.85e-06
I1127 18:13:54.595122  2966 solver.cpp:228] Iteration 20700, loss = 0.12832
I1127 18:13:54.595280  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.128321 (* 1 = 0.128321 loss)
I1127 18:13:54.595324  2966 sgd_solver.cpp:106] Iteration 20700, lr = 4.825e-06
I1127 18:14:57.768898  2966 solver.cpp:228] Iteration 20800, loss = 0.971325
I1127 18:14:57.769054  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.971325 (* 1 = 0.971325 loss)
I1127 18:14:57.769098  2966 sgd_solver.cpp:106] Iteration 20800, lr = 4.8e-06
I1127 18:15:04.956116  2966 solver.cpp:337] Iteration 20812, Testing net (#0)
I1127 18:15:18.199169  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.732558
I1127 18:15:18.199350  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.778331 (* 1 = 0.778331 loss)
I1127 18:16:16.236953  2966 solver.cpp:228] Iteration 20900, loss = 0.163659
I1127 18:16:16.237113  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.16366 (* 1 = 0.16366 loss)
I1127 18:16:16.237160  2966 sgd_solver.cpp:106] Iteration 20900, lr = 4.775e-06
I1127 18:17:18.257488  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_21000.caffemodel
I1127 18:17:18.395448  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_21000.solverstate
I1127 18:17:19.090695  2966 solver.cpp:228] Iteration 21000, loss = 0.257312
I1127 18:17:19.091037  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.257312 (* 1 = 0.257312 loss)
I1127 18:17:19.091131  2966 sgd_solver.cpp:106] Iteration 21000, lr = 4.75e-06
I1127 18:17:54.208334  2966 solver.cpp:337] Iteration 21054, Testing net (#0)
I1127 18:18:08.669013  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.752907
I1127 18:18:08.669189  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.76474 (* 1 = 0.76474 loss)
I1127 18:18:38.912875  2966 solver.cpp:228] Iteration 21100, loss = 0.231828
I1127 18:18:38.913167  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.231828 (* 1 = 0.231828 loss)
I1127 18:18:38.913213  2966 sgd_solver.cpp:106] Iteration 21100, lr = 4.725e-06
I1127 18:19:42.673280  2966 solver.cpp:228] Iteration 21200, loss = 0.255255
I1127 18:19:42.673444  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.255255 (* 1 = 0.255255 loss)
I1127 18:19:42.673589  2966 sgd_solver.cpp:106] Iteration 21200, lr = 4.7e-06
I1127 18:20:45.444471  2966 solver.cpp:337] Iteration 21296, Testing net (#0)
I1127 18:20:59.017225  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 18:20:59.017346  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.770275 (* 1 = 0.770275 loss)
I1127 18:21:02.246443  2966 solver.cpp:228] Iteration 21300, loss = 0.295915
I1127 18:21:02.246548  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.295915 (* 1 = 0.295915 loss)
I1127 18:21:02.246641  2966 sgd_solver.cpp:106] Iteration 21300, lr = 4.675e-06
I1127 18:22:06.808323  2966 solver.cpp:228] Iteration 21400, loss = 0.112475
I1127 18:22:06.808456  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.112475 (* 1 = 0.112475 loss)
I1127 18:22:06.808500  2966 sgd_solver.cpp:106] Iteration 21400, lr = 4.65e-06
I1127 18:23:06.475168  2966 solver.cpp:228] Iteration 21500, loss = 0.035407
I1127 18:23:06.475338  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0354075 (* 1 = 0.0354075 loss)
I1127 18:23:06.475400  2966 sgd_solver.cpp:106] Iteration 21500, lr = 4.625e-06
I1127 18:23:28.587193  2966 solver.cpp:337] Iteration 21538, Testing net (#0)
I1127 18:23:41.189806  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 18:23:41.190026  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.811055 (* 1 = 0.811055 loss)
I1127 18:24:22.841522  2966 solver.cpp:228] Iteration 21600, loss = 0.237845
I1127 18:24:22.841656  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.237846 (* 1 = 0.237846 loss)
I1127 18:24:22.841701  2966 sgd_solver.cpp:106] Iteration 21600, lr = 4.6e-06
I1127 18:25:25.749933  2966 solver.cpp:228] Iteration 21700, loss = 0.54068
I1127 18:25:25.750058  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.54068 (* 1 = 0.54068 loss)
I1127 18:25:25.750119  2966 sgd_solver.cpp:106] Iteration 21700, lr = 4.575e-06
I1127 18:26:14.458900  2966 solver.cpp:337] Iteration 21780, Testing net (#0)
I1127 18:26:28.159343  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.741279
I1127 18:26:28.159826  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.735581 (* 1 = 0.735581 loss)
I1127 18:26:41.067795  2966 solver.cpp:228] Iteration 21800, loss = 0.200994
I1127 18:26:41.067906  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.200994 (* 1 = 0.200994 loss)
I1127 18:26:41.067948  2966 sgd_solver.cpp:106] Iteration 21800, lr = 4.55e-06
I1127 18:27:42.978905  2966 solver.cpp:228] Iteration 21900, loss = 0.517673
I1127 18:27:42.979040  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.517674 (* 1 = 0.517674 loss)
I1127 18:27:42.979082  2966 sgd_solver.cpp:106] Iteration 21900, lr = 4.525e-06
I1127 18:28:42.672797  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_22000.caffemodel
I1127 18:28:42.799641  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_22000.solverstate
I1127 18:28:43.453326  2966 solver.cpp:228] Iteration 22000, loss = 0.447824
I1127 18:28:43.453444  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.447825 (* 1 = 0.447825 loss)
I1127 18:28:43.453486  2966 sgd_solver.cpp:106] Iteration 22000, lr = 4.5e-06
I1127 18:28:56.562974  2966 solver.cpp:337] Iteration 22022, Testing net (#0)
I1127 18:29:09.625797  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 18:29:09.625906  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.786829 (* 1 = 0.786829 loss)
I1127 18:30:00.288763  2966 solver.cpp:228] Iteration 22100, loss = 0.00870153
I1127 18:30:00.289063  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.00870219 (* 1 = 0.00870219 loss)
I1127 18:30:00.289108  2966 sgd_solver.cpp:106] Iteration 22100, lr = 4.475e-06
I1127 18:31:02.068864  2966 solver.cpp:228] Iteration 22200, loss = 0.396872
I1127 18:31:02.069003  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.396872 (* 1 = 0.396872 loss)
I1127 18:31:02.069046  2966 sgd_solver.cpp:106] Iteration 22200, lr = 4.45e-06
I1127 18:31:40.706465  2966 solver.cpp:337] Iteration 22264, Testing net (#0)
I1127 18:31:53.889004  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 18:31:53.889118  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.725921 (* 1 = 0.725921 loss)
I1127 18:32:16.891448  2966 solver.cpp:228] Iteration 22300, loss = 0.130968
I1127 18:32:16.891579  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.130969 (* 1 = 0.130969 loss)
I1127 18:32:16.891623  2966 sgd_solver.cpp:106] Iteration 22300, lr = 4.425e-06
I1127 18:33:18.161739  2966 solver.cpp:228] Iteration 22400, loss = 0.0786923
I1127 18:33:18.161896  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0786927 (* 1 = 0.0786927 loss)
I1127 18:33:18.161941  2966 sgd_solver.cpp:106] Iteration 22400, lr = 4.4e-06
I1127 18:34:19.765864  2966 solver.cpp:228] Iteration 22500, loss = 0.373804
I1127 18:34:19.766037  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.373805 (* 1 = 0.373805 loss)
I1127 18:34:19.766082  2966 sgd_solver.cpp:106] Iteration 22500, lr = 4.375e-06
I1127 18:34:23.153815  2966 solver.cpp:337] Iteration 22506, Testing net (#0)
I1127 18:34:36.340081  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.747093
I1127 18:34:36.340195  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.721761 (* 1 = 0.721761 loss)
I1127 18:35:33.577425  2966 solver.cpp:228] Iteration 22600, loss = 0.44788
I1127 18:35:33.577576  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.44788 (* 1 = 0.44788 loss)
I1127 18:35:33.577622  2966 sgd_solver.cpp:106] Iteration 22600, lr = 4.35e-06
I1127 18:36:35.917770  2966 solver.cpp:228] Iteration 22700, loss = 0.448655
I1127 18:36:35.917902  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.448656 (* 1 = 0.448656 loss)
I1127 18:36:35.917946  2966 sgd_solver.cpp:106] Iteration 22700, lr = 4.325e-06
I1127 18:37:04.765610  2966 solver.cpp:337] Iteration 22748, Testing net (#0)
I1127 18:37:17.759840  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 18:37:17.759974  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.80639 (* 1 = 0.80639 loss)
I1127 18:37:49.855955  2966 solver.cpp:228] Iteration 22800, loss = 0.176709
I1127 18:37:49.856086  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.17671 (* 1 = 0.17671 loss)
I1127 18:37:49.856140  2966 sgd_solver.cpp:106] Iteration 22800, lr = 4.3e-06
I1127 18:38:50.982723  2966 solver.cpp:228] Iteration 22900, loss = 0.557004
I1127 18:38:50.982854  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.557004 (* 1 = 0.557004 loss)
I1127 18:38:50.982897  2966 sgd_solver.cpp:106] Iteration 22900, lr = 4.275e-06
I1127 18:39:44.491593  2966 solver.cpp:337] Iteration 22990, Testing net (#0)
I1127 18:39:57.409824  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.752907
I1127 18:39:57.409948  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.754076 (* 1 = 0.754076 loss)
I1127 18:40:03.658902  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_23000.caffemodel
I1127 18:40:03.792115  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_23000.solverstate
I1127 18:40:04.448846  2966 solver.cpp:228] Iteration 23000, loss = 0.362587
I1127 18:40:04.448956  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.362587 (* 1 = 0.362587 loss)
I1127 18:40:04.448999  2966 sgd_solver.cpp:106] Iteration 23000, lr = 4.25e-06
I1127 18:41:04.215276  2966 solver.cpp:228] Iteration 23100, loss = 0.093683
I1127 18:41:04.215574  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0936832 (* 1 = 0.0936832 loss)
I1127 18:41:04.215618  2966 sgd_solver.cpp:106] Iteration 23100, lr = 4.225e-06
I1127 18:42:04.075098  2966 solver.cpp:228] Iteration 23200, loss = 0.45526
I1127 18:42:04.075263  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.45526 (* 1 = 0.45526 loss)
I1127 18:42:04.075305  2966 sgd_solver.cpp:106] Iteration 23200, lr = 4.2e-06
I1127 18:42:23.045243  2966 solver.cpp:337] Iteration 23232, Testing net (#0)
I1127 18:42:36.254103  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.767442
I1127 18:42:36.254232  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.747126 (* 1 = 0.747126 loss)
I1127 18:43:17.882848  2966 solver.cpp:228] Iteration 23300, loss = 0.0244951
I1127 18:43:17.882993  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.024495 (* 1 = 0.024495 loss)
I1127 18:43:17.883036  2966 sgd_solver.cpp:106] Iteration 23300, lr = 4.175e-06
I1127 18:44:18.023979  2966 solver.cpp:228] Iteration 23400, loss = 0.115194
I1127 18:44:18.024111  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.115194 (* 1 = 0.115194 loss)
I1127 18:44:18.024152  2966 sgd_solver.cpp:106] Iteration 23400, lr = 4.15e-06
I1127 18:45:03.664049  2966 solver.cpp:337] Iteration 23474, Testing net (#0)
I1127 18:45:18.488785  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 18:45:18.488898  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.777737 (* 1 = 0.777737 loss)
I1127 18:45:35.174108  2966 solver.cpp:228] Iteration 23500, loss = 0.455893
I1127 18:45:35.174242  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.455893 (* 1 = 0.455893 loss)
I1127 18:45:35.174285  2966 sgd_solver.cpp:106] Iteration 23500, lr = 4.125e-06
I1127 18:46:35.737468  2966 solver.cpp:228] Iteration 23600, loss = 0.458126
I1127 18:46:35.737599  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.458126 (* 1 = 0.458126 loss)
I1127 18:46:35.737642  2966 sgd_solver.cpp:106] Iteration 23600, lr = 4.1e-06
I1127 18:47:38.807648  2966 solver.cpp:228] Iteration 23700, loss = 0.0523074
I1127 18:47:38.807776  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0523075 (* 1 = 0.0523075 loss)
I1127 18:47:38.807818  2966 sgd_solver.cpp:106] Iteration 23700, lr = 4.075e-06
I1127 18:47:49.506343  2966 solver.cpp:337] Iteration 23716, Testing net (#0)
I1127 18:48:02.588788  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 18:48:02.588902  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.776617 (* 1 = 0.776617 loss)
I1127 18:48:54.338863  2966 solver.cpp:228] Iteration 23800, loss = 0.0800768
I1127 18:48:54.338999  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0800769 (* 1 = 0.0800769 loss)
I1127 18:48:54.339041  2966 sgd_solver.cpp:106] Iteration 23800, lr = 4.05e-06
I1127 18:49:56.637830  2966 solver.cpp:228] Iteration 23900, loss = 0.0781743
I1127 18:49:56.637961  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0781743 (* 1 = 0.0781743 loss)
I1127 18:49:56.638003  2966 sgd_solver.cpp:106] Iteration 23900, lr = 4.025e-06
I1127 18:50:32.259799  2966 solver.cpp:337] Iteration 23958, Testing net (#0)
I1127 18:50:45.496517  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 18:50:45.496697  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.716682 (* 1 = 0.716682 loss)
I1127 18:51:11.253587  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_24000.caffemodel
I1127 18:51:11.399091  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_24000.solverstate
I1127 18:51:12.150326  2966 solver.cpp:228] Iteration 24000, loss = 0.0423934
I1127 18:51:12.150436  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0423936 (* 1 = 0.0423936 loss)
I1127 18:51:12.150480  2966 sgd_solver.cpp:106] Iteration 24000, lr = 4e-06
I1127 18:52:14.640336  2966 solver.cpp:228] Iteration 24100, loss = 0.27575
I1127 18:52:14.640475  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.27575 (* 1 = 0.27575 loss)
I1127 18:52:14.640517  2966 sgd_solver.cpp:106] Iteration 24100, lr = 3.975e-06
I1127 18:53:16.269419  2966 solver.cpp:337] Iteration 24200, Testing net (#0)
I1127 18:53:29.353070  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 18:53:29.353184  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.720984 (* 1 = 0.720984 loss)
I1127 18:53:29.960772  2966 solver.cpp:228] Iteration 24200, loss = 0.180144
I1127 18:53:29.960908  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.180144 (* 1 = 0.180144 loss)
I1127 18:53:29.960953  2966 sgd_solver.cpp:106] Iteration 24200, lr = 3.95e-06
I1127 18:54:31.958533  2966 solver.cpp:228] Iteration 24300, loss = 0.158314
I1127 18:54:31.958673  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.158314 (* 1 = 0.158314 loss)
I1127 18:54:31.958716  2966 sgd_solver.cpp:106] Iteration 24300, lr = 3.925e-06
I1127 18:55:39.758574  2966 solver.cpp:228] Iteration 24400, loss = 0.0375169
I1127 18:55:39.758713  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0375166 (* 1 = 0.0375166 loss)
I1127 18:55:39.758756  2966 sgd_solver.cpp:106] Iteration 24400, lr = 3.9e-06
I1127 18:56:11.729622  2966 solver.cpp:337] Iteration 24442, Testing net (#0)
I1127 18:56:27.835121  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.752907
I1127 18:56:27.835368  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.765682 (* 1 = 0.765682 loss)
I1127 18:57:12.970230  2966 solver.cpp:228] Iteration 24500, loss = 0.133552
I1127 18:57:12.970963  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.133551 (* 1 = 0.133551 loss)
I1127 18:57:12.971181  2966 sgd_solver.cpp:106] Iteration 24500, lr = 3.875e-06
I1127 18:58:16.642051  2966 solver.cpp:228] Iteration 24600, loss = 0.187557
I1127 18:58:16.642180  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.187556 (* 1 = 0.187556 loss)
I1127 18:58:16.642222  2966 sgd_solver.cpp:106] Iteration 24600, lr = 3.85e-06
I1127 18:59:09.088006  2966 solver.cpp:337] Iteration 24684, Testing net (#0)
I1127 18:59:21.983146  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 18:59:21.983304  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.725226 (* 1 = 0.725226 loss)
I1127 18:59:32.606750  2966 solver.cpp:228] Iteration 24700, loss = 0.486669
I1127 18:59:32.606915  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.486669 (* 1 = 0.486669 loss)
I1127 18:59:32.606972  2966 sgd_solver.cpp:106] Iteration 24700, lr = 3.825e-06
I1127 19:00:32.439744  2966 solver.cpp:228] Iteration 24800, loss = 0.459267
I1127 19:00:32.439929  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.459267 (* 1 = 0.459267 loss)
I1127 19:00:32.439972  2966 sgd_solver.cpp:106] Iteration 24800, lr = 3.8e-06
I1127 19:01:31.846199  2966 solver.cpp:228] Iteration 24900, loss = 0.117444
I1127 19:01:31.846335  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.117444 (* 1 = 0.117444 loss)
I1127 19:01:31.846377  2966 sgd_solver.cpp:106] Iteration 24900, lr = 3.775e-06
I1127 19:01:47.264868  2966 solver.cpp:337] Iteration 24926, Testing net (#0)
I1127 19:02:00.505054  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.758721
I1127 19:02:00.505200  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.743082 (* 1 = 0.743082 loss)
I1127 19:02:47.175005  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_25000.caffemodel
I1127 19:02:47.309145  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_25000.solverstate
I1127 19:02:47.965775  2966 solver.cpp:228] Iteration 25000, loss = 0.0436699
I1127 19:02:47.965912  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0436698 (* 1 = 0.0436698 loss)
I1127 19:02:47.965956  2966 sgd_solver.cpp:106] Iteration 25000, lr = 3.75e-06
I1127 19:03:48.303112  2966 solver.cpp:228] Iteration 25100, loss = 0.125636
I1127 19:03:48.303243  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.125636 (* 1 = 0.125636 loss)
I1127 19:03:48.303285  2966 sgd_solver.cpp:106] Iteration 25100, lr = 3.725e-06
I1127 19:04:31.820482  2966 solver.cpp:337] Iteration 25168, Testing net (#0)
I1127 19:04:45.573470  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.761628
I1127 19:04:45.573580  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.77589 (* 1 = 0.77589 loss)
I1127 19:05:06.521653  2966 solver.cpp:228] Iteration 25200, loss = 0.207972
I1127 19:05:06.521801  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.207972 (* 1 = 0.207972 loss)
I1127 19:05:06.521917  2966 sgd_solver.cpp:106] Iteration 25200, lr = 3.7e-06
I1127 19:06:07.675215  2966 solver.cpp:228] Iteration 25300, loss = 0.268412
I1127 19:06:07.675371  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.268412 (* 1 = 0.268412 loss)
I1127 19:06:07.675415  2966 sgd_solver.cpp:106] Iteration 25300, lr = 3.675e-06
I1127 19:07:08.448511  2966 solver.cpp:228] Iteration 25400, loss = 0.216106
I1127 19:07:08.448648  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.216106 (* 1 = 0.216106 loss)
I1127 19:07:08.448693  2966 sgd_solver.cpp:106] Iteration 25400, lr = 3.65e-06
I1127 19:07:14.034589  2966 solver.cpp:337] Iteration 25410, Testing net (#0)
I1127 19:07:28.702920  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.761628
I1127 19:07:28.703033  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.756335 (* 1 = 0.756335 loss)
I1127 19:08:27.040302  2966 solver.cpp:228] Iteration 25500, loss = 0.171186
I1127 19:08:27.040438  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.171185 (* 1 = 0.171185 loss)
I1127 19:08:27.040482  2966 sgd_solver.cpp:106] Iteration 25500, lr = 3.625e-06
I1127 19:09:26.705770  2966 solver.cpp:228] Iteration 25600, loss = 0.127646
I1127 19:09:26.705906  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.127646 (* 1 = 0.127646 loss)
I1127 19:09:26.705950  2966 sgd_solver.cpp:106] Iteration 25600, lr = 3.6e-06
I1127 19:09:57.524626  2966 solver.cpp:337] Iteration 25652, Testing net (#0)
I1127 19:10:10.451702  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.761628
I1127 19:10:10.451900  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.721894 (* 1 = 0.721894 loss)
I1127 19:10:41.325063  2966 solver.cpp:228] Iteration 25700, loss = 0.248505
I1127 19:10:41.325196  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.248505 (* 1 = 0.248505 loss)
I1127 19:10:41.325238  2966 sgd_solver.cpp:106] Iteration 25700, lr = 3.575e-06
I1127 19:11:42.336570  2966 solver.cpp:228] Iteration 25800, loss = 0.169948
I1127 19:11:42.336712  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.169947 (* 1 = 0.169947 loss)
I1127 19:11:42.336755  2966 sgd_solver.cpp:106] Iteration 25800, lr = 3.55e-06
I1127 19:12:39.112118  2966 solver.cpp:337] Iteration 25894, Testing net (#0)
I1127 19:12:52.042163  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.770349
I1127 19:12:52.042273  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.7513 (* 1 = 0.7513 loss)
I1127 19:12:56.303349  2966 solver.cpp:228] Iteration 25900, loss = 0.173103
I1127 19:12:56.303459  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.173103 (* 1 = 0.173103 loss)
I1127 19:12:56.303517  2966 sgd_solver.cpp:106] Iteration 25900, lr = 3.525e-06
I1127 19:13:57.100493  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_26000.caffemodel
I1127 19:13:57.228711  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_26000.solverstate
I1127 19:13:57.908004  2966 solver.cpp:228] Iteration 26000, loss = 0.138087
I1127 19:13:57.908118  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.138087 (* 1 = 0.138087 loss)
I1127 19:13:57.908161  2966 sgd_solver.cpp:106] Iteration 26000, lr = 3.5e-06
I1127 19:14:58.399813  2966 solver.cpp:228] Iteration 26100, loss = 0.0609128
I1127 19:14:58.399994  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0609124 (* 1 = 0.0609124 loss)
I1127 19:14:58.400037  2966 sgd_solver.cpp:106] Iteration 26100, lr = 3.475e-06
I1127 19:15:19.685528  2966 solver.cpp:337] Iteration 26136, Testing net (#0)
I1127 19:15:32.526643  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.773256
I1127 19:15:32.526775  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.702817 (* 1 = 0.702817 loss)
I1127 19:16:20.477459  2966 solver.cpp:228] Iteration 26200, loss = 0.195581
I1127 19:16:20.482004  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.195581 (* 1 = 0.195581 loss)
I1127 19:16:20.482049  2966 sgd_solver.cpp:106] Iteration 26200, lr = 3.45e-06
I1127 19:17:53.184238  2966 solver.cpp:228] Iteration 26300, loss = 0.18526
I1127 19:17:53.184375  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.185259 (* 1 = 0.185259 loss)
I1127 19:17:53.184419  2966 sgd_solver.cpp:106] Iteration 26300, lr = 3.425e-06
I1127 19:18:47.084661  2966 solver.cpp:337] Iteration 26378, Testing net (#0)
I1127 19:19:02.488741  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.758721
I1127 19:19:02.488857  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.737057 (* 1 = 0.737057 loss)
I1127 19:19:18.504899  2966 solver.cpp:228] Iteration 26400, loss = 0.133461
I1127 19:19:18.505031  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.13346 (* 1 = 0.13346 loss)
I1127 19:19:18.505074  2966 sgd_solver.cpp:106] Iteration 26400, lr = 3.4e-06
I1127 19:20:28.451238  2966 solver.cpp:228] Iteration 26500, loss = 0.091508
I1127 19:20:28.451376  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0915077 (* 1 = 0.0915077 loss)
I1127 19:20:28.451421  2966 sgd_solver.cpp:106] Iteration 26500, lr = 3.375e-06
I1127 19:21:40.778993  2966 solver.cpp:228] Iteration 26600, loss = 0.252024
I1127 19:21:40.779124  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.252023 (* 1 = 0.252023 loss)
I1127 19:21:40.779166  2966 sgd_solver.cpp:106] Iteration 26600, lr = 3.35e-06
I1127 19:21:54.788610  2966 solver.cpp:337] Iteration 26620, Testing net (#0)
I1127 19:22:10.134093  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 19:22:10.134248  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.755545 (* 1 = 0.755545 loss)
I1127 19:23:09.250530  2966 solver.cpp:228] Iteration 26700, loss = 1.05924
I1127 19:23:09.250658  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.05923 (* 1 = 1.05923 loss)
I1127 19:23:09.250699  2966 sgd_solver.cpp:106] Iteration 26700, lr = 3.325e-06
I1127 19:24:24.859772  2966 solver.cpp:228] Iteration 26800, loss = 0.190465
I1127 19:24:24.859899  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.190464 (* 1 = 0.190464 loss)
I1127 19:24:24.859941  2966 sgd_solver.cpp:106] Iteration 26800, lr = 3.3e-06
I1127 19:25:02.258550  2966 solver.cpp:337] Iteration 26862, Testing net (#0)
I1127 19:25:15.323786  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.767442
I1127 19:25:15.323899  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.719871 (* 1 = 0.719871 loss)
I1127 19:25:40.022156  2966 solver.cpp:228] Iteration 26900, loss = 0.0654798
I1127 19:25:40.022294  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0654796 (* 1 = 0.0654796 loss)
I1127 19:25:40.022338  2966 sgd_solver.cpp:106] Iteration 26900, lr = 3.275e-06
I1127 19:26:39.267735  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_27000.caffemodel
I1127 19:26:39.384673  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_27000.solverstate
I1127 19:26:40.041146  2966 solver.cpp:228] Iteration 27000, loss = 0.168993
I1127 19:26:40.041259  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.168993 (* 1 = 0.168993 loss)
I1127 19:26:40.041301  2966 sgd_solver.cpp:106] Iteration 27000, lr = 3.25e-06
I1127 19:27:40.122927  2966 solver.cpp:228] Iteration 27100, loss = 0.323749
I1127 19:27:40.123059  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.323749 (* 1 = 0.323749 loss)
I1127 19:27:40.123118  2966 sgd_solver.cpp:106] Iteration 27100, lr = 3.225e-06
I1127 19:27:42.076438  2966 solver.cpp:337] Iteration 27104, Testing net (#0)
I1127 19:27:55.422111  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 19:27:55.422219  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.776695 (* 1 = 0.776695 loss)
I1127 19:28:56.091927  2966 solver.cpp:228] Iteration 27200, loss = 0.230842
I1127 19:28:56.092058  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.230842 (* 1 = 0.230842 loss)
I1127 19:28:56.092102  2966 sgd_solver.cpp:106] Iteration 27200, lr = 3.2e-06
I1127 19:29:55.757547  2966 solver.cpp:228] Iteration 27300, loss = 0.806178
I1127 19:29:55.757675  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.806178 (* 1 = 0.806178 loss)
I1127 19:29:55.757719  2966 sgd_solver.cpp:106] Iteration 27300, lr = 3.175e-06
I1127 19:30:22.915789  2966 solver.cpp:337] Iteration 27346, Testing net (#0)
I1127 19:30:35.734555  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 19:30:35.734691  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.758654 (* 1 = 0.758654 loss)
I1127 19:31:08.601707  2966 solver.cpp:228] Iteration 27400, loss = 0.0685328
I1127 19:31:08.601842  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0685327 (* 1 = 0.0685327 loss)
I1127 19:31:08.601884  2966 sgd_solver.cpp:106] Iteration 27400, lr = 3.15e-06
I1127 19:32:09.160579  2966 solver.cpp:228] Iteration 27500, loss = 0.0453595
I1127 19:32:09.160715  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0453594 (* 1 = 0.0453594 loss)
I1127 19:32:09.160758  2966 sgd_solver.cpp:106] Iteration 27500, lr = 3.125e-06
I1127 19:33:02.189924  2966 solver.cpp:337] Iteration 27588, Testing net (#0)
I1127 19:33:15.436142  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 19:33:15.436256  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.774242 (* 1 = 0.774242 loss)
I1127 19:33:23.907101  2966 solver.cpp:228] Iteration 27600, loss = 0.0744265
I1127 19:33:23.907217  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0744264 (* 1 = 0.0744264 loss)
I1127 19:33:23.907260  2966 sgd_solver.cpp:106] Iteration 27600, lr = 3.1e-06
I1127 19:34:25.798491  2966 solver.cpp:228] Iteration 27700, loss = 0.0570802
I1127 19:34:25.798647  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0570802 (* 1 = 0.0570802 loss)
I1127 19:34:25.798705  2966 sgd_solver.cpp:106] Iteration 27700, lr = 3.075e-06
I1127 19:35:27.038233  2966 solver.cpp:228] Iteration 27800, loss = 0.34764
I1127 19:35:27.038523  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.34764 (* 1 = 0.34764 loss)
I1127 19:35:27.038568  2966 sgd_solver.cpp:106] Iteration 27800, lr = 3.05e-06
I1127 19:35:45.529031  2966 solver.cpp:337] Iteration 27830, Testing net (#0)
I1127 19:35:58.956081  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.752907
I1127 19:35:58.956238  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.772793 (* 1 = 0.772793 loss)
I1127 19:36:43.865658  2966 solver.cpp:228] Iteration 27900, loss = 0.135731
I1127 19:36:43.865784  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.135731 (* 1 = 0.135731 loss)
I1127 19:36:43.865828  2966 sgd_solver.cpp:106] Iteration 27900, lr = 3.025e-06
I1127 19:37:45.406302  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_28000.caffemodel
I1127 19:37:45.534811  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_28000.solverstate
I1127 19:37:46.218633  2966 solver.cpp:228] Iteration 28000, loss = 0.17053
I1127 19:37:46.218742  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.17053 (* 1 = 0.17053 loss)
I1127 19:37:46.218787  2966 sgd_solver.cpp:106] Iteration 28000, lr = 3e-06
I1127 19:38:31.276275  2966 solver.cpp:337] Iteration 28072, Testing net (#0)
I1127 19:38:44.470402  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 19:38:44.470515  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.702537 (* 1 = 0.702537 loss)
I1127 19:39:02.606051  2966 solver.cpp:228] Iteration 28100, loss = 0.189834
I1127 19:39:02.606186  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.189834 (* 1 = 0.189834 loss)
I1127 19:39:02.606231  2966 sgd_solver.cpp:106] Iteration 28100, lr = 2.975e-06
I1127 19:40:06.413195  2966 solver.cpp:228] Iteration 28200, loss = 0.416651
I1127 19:40:06.413346  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.416652 (* 1 = 0.416652 loss)
I1127 19:40:06.413390  2966 sgd_solver.cpp:106] Iteration 28200, lr = 2.95e-06
I1127 19:41:09.364593  2966 solver.cpp:228] Iteration 28300, loss = 0.11094
I1127 19:41:09.364728  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.11094 (* 1 = 0.11094 loss)
I1127 19:41:09.364826  2966 sgd_solver.cpp:106] Iteration 28300, lr = 2.925e-06
I1127 19:41:17.496088  2966 solver.cpp:337] Iteration 28314, Testing net (#0)
I1127 19:41:30.183228  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 19:41:30.183341  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.712815 (* 1 = 0.712815 loss)
I1127 19:42:22.481528  2966 solver.cpp:228] Iteration 28400, loss = 0.351185
I1127 19:42:22.481685  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.351186 (* 1 = 0.351186 loss)
I1127 19:42:22.481729  2966 sgd_solver.cpp:106] Iteration 28400, lr = 2.9e-06
I1127 19:43:24.508036  2966 solver.cpp:228] Iteration 28500, loss = 0.652989
I1127 19:43:24.508191  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.65299 (* 1 = 0.65299 loss)
I1127 19:43:24.508236  2966 sgd_solver.cpp:106] Iteration 28500, lr = 2.875e-06
I1127 19:43:59.444403  2966 solver.cpp:337] Iteration 28556, Testing net (#0)
I1127 19:44:12.567811  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.758721
I1127 19:44:12.568011  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.736979 (* 1 = 0.736979 loss)
I1127 19:44:41.355819  2966 solver.cpp:228] Iteration 28600, loss = 0.0575862
I1127 19:44:41.355955  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0575867 (* 1 = 0.0575867 loss)
I1127 19:44:41.355998  2966 sgd_solver.cpp:106] Iteration 28600, lr = 2.85e-06
I1127 19:45:43.498680  2966 solver.cpp:228] Iteration 28700, loss = 0.189312
I1127 19:45:43.498976  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.189313 (* 1 = 0.189313 loss)
I1127 19:45:43.499020  2966 sgd_solver.cpp:106] Iteration 28700, lr = 2.825e-06
I1127 19:46:44.814430  2966 solver.cpp:337] Iteration 28798, Testing net (#0)
I1127 19:46:58.023411  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.758721
I1127 19:46:58.023600  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.696992 (* 1 = 0.696992 loss)
I1127 19:46:59.920145  2966 solver.cpp:228] Iteration 28800, loss = 0.196126
I1127 19:46:59.920258  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.196127 (* 1 = 0.196127 loss)
I1127 19:46:59.920300  2966 sgd_solver.cpp:106] Iteration 28800, lr = 2.8e-06
I1127 19:48:02.223598  2966 solver.cpp:228] Iteration 28900, loss = 0.442332
I1127 19:48:02.223731  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.442333 (* 1 = 0.442333 loss)
I1127 19:48:02.223829  2966 sgd_solver.cpp:106] Iteration 28900, lr = 2.775e-06
I1127 19:49:03.737221  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_29000.caffemodel
I1127 19:49:03.860323  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_29000.solverstate
I1127 19:49:04.520517  2966 solver.cpp:228] Iteration 29000, loss = 0.264421
I1127 19:49:04.520629  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.264422 (* 1 = 0.264422 loss)
I1127 19:49:04.520673  2966 sgd_solver.cpp:106] Iteration 29000, lr = 2.75e-06
I1127 19:49:28.926467  2966 solver.cpp:337] Iteration 29040, Testing net (#0)
I1127 19:49:42.146777  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 19:49:42.146914  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.793406 (* 1 = 0.793406 loss)
I1127 19:50:20.066089  2966 solver.cpp:228] Iteration 29100, loss = 0.31097
I1127 19:50:20.066236  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.310971 (* 1 = 0.310971 loss)
I1127 19:50:20.066280  2966 sgd_solver.cpp:106] Iteration 29100, lr = 2.725e-06
I1127 19:51:22.491403  2966 solver.cpp:228] Iteration 29200, loss = 0.33373
I1127 19:51:22.491539  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.333731 (* 1 = 0.333731 loss)
I1127 19:51:22.491582  2966 sgd_solver.cpp:106] Iteration 29200, lr = 2.7e-06
I1127 19:52:12.634897  2966 solver.cpp:337] Iteration 29282, Testing net (#0)
I1127 19:52:25.902683  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.747093
I1127 19:52:25.902792  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.748646 (* 1 = 0.748646 loss)
I1127 19:52:38.034658  2966 solver.cpp:228] Iteration 29300, loss = 0.361931
I1127 19:52:38.034759  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.361932 (* 1 = 0.361932 loss)
I1127 19:52:38.034801  2966 sgd_solver.cpp:106] Iteration 29300, lr = 2.675e-06
I1127 19:53:41.842571  2966 solver.cpp:228] Iteration 29400, loss = 0.263222
I1127 19:53:41.842795  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.263223 (* 1 = 0.263223 loss)
I1127 19:53:41.842842  2966 sgd_solver.cpp:106] Iteration 29400, lr = 2.65e-06
I1127 19:54:44.787614  2966 solver.cpp:228] Iteration 29500, loss = 0.102167
I1127 19:54:44.787782  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.102168 (* 1 = 0.102168 loss)
I1127 19:54:44.787827  2966 sgd_solver.cpp:106] Iteration 29500, lr = 2.625e-06
I1127 19:54:59.352501  2966 solver.cpp:337] Iteration 29524, Testing net (#0)
I1127 19:55:12.690807  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.773256
I1127 19:55:12.690919  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.699661 (* 1 = 0.699661 loss)
I1127 19:56:00.651500  2966 solver.cpp:228] Iteration 29600, loss = 0.0556001
I1127 19:56:00.651643  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0556012 (* 1 = 0.0556012 loss)
I1127 19:56:00.651687  2966 sgd_solver.cpp:106] Iteration 29600, lr = 2.6e-06
I1127 19:57:05.141580  2966 solver.cpp:228] Iteration 29700, loss = 0.583878
I1127 19:57:05.142109  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.583879 (* 1 = 0.583879 loss)
I1127 19:57:05.142173  2966 sgd_solver.cpp:106] Iteration 29700, lr = 2.575e-06
I1127 19:57:46.478582  2966 solver.cpp:337] Iteration 29766, Testing net (#0)
I1127 19:57:59.896697  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.738372
I1127 19:57:59.896878  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.749507 (* 1 = 0.749507 loss)
I1127 19:58:22.486837  2966 solver.cpp:228] Iteration 29800, loss = 0.91803
I1127 19:58:22.486979  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.918031 (* 1 = 0.918031 loss)
I1127 19:58:22.487026  2966 sgd_solver.cpp:106] Iteration 29800, lr = 2.55e-06
I1127 19:59:26.094672  2966 solver.cpp:228] Iteration 29900, loss = 0.188391
I1127 19:59:26.094810  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.188392 (* 1 = 0.188392 loss)
I1127 19:59:26.094854  2966 sgd_solver.cpp:106] Iteration 29900, lr = 2.525e-06
I1127 20:00:31.341079  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_30000.caffemodel
I1127 20:00:31.465899  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_30000.solverstate
I1127 20:00:32.124313  2966 solver.cpp:228] Iteration 30000, loss = 0.135603
I1127 20:00:32.124425  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.135604 (* 1 = 0.135604 loss)
I1127 20:00:32.124476  2966 sgd_solver.cpp:106] Iteration 30000, lr = 2.5e-06
I1127 20:00:36.539980  2966 solver.cpp:337] Iteration 30008, Testing net (#0)
I1127 20:00:49.413432  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.75
I1127 20:00:49.413544  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.743694 (* 1 = 0.743694 loss)
I1127 20:01:45.639597  2966 solver.cpp:228] Iteration 30100, loss = 0.269735
I1127 20:01:45.639737  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.269736 (* 1 = 0.269736 loss)
I1127 20:01:45.639780  2966 sgd_solver.cpp:106] Iteration 30100, lr = 2.475e-06
I1127 20:02:52.305424  2966 solver.cpp:228] Iteration 30200, loss = 0.45248
I1127 20:02:52.305557  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.452481 (* 1 = 0.452481 loss)
I1127 20:02:52.305599  2966 sgd_solver.cpp:106] Iteration 30200, lr = 2.45e-06
I1127 20:03:26.176826  2966 solver.cpp:337] Iteration 30250, Testing net (#0)
I1127 20:03:45.877197  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 20:03:45.881481  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.752445 (* 1 = 0.752445 loss)
I1127 20:04:55.517169  2966 solver.cpp:228] Iteration 30300, loss = 0.255994
I1127 20:04:55.526264  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.255995 (* 1 = 0.255995 loss)
I1127 20:04:55.526338  2966 sgd_solver.cpp:106] Iteration 30300, lr = 2.425e-06
I1127 20:06:10.405465  2966 solver.cpp:228] Iteration 30400, loss = 0.420708
I1127 20:06:10.405592  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.420709 (* 1 = 0.420709 loss)
I1127 20:06:10.405637  2966 sgd_solver.cpp:106] Iteration 30400, lr = 2.4e-06
I1127 20:07:21.234071  2966 solver.cpp:337] Iteration 30492, Testing net (#0)
I1127 20:07:34.673413  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.735465
I1127 20:07:34.673526  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.77524 (* 1 = 0.77524 loss)
I1127 20:07:40.493621  2966 solver.cpp:228] Iteration 30500, loss = 0.214476
I1127 20:07:40.493732  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.214477 (* 1 = 0.214477 loss)
I1127 20:07:40.493789  2966 sgd_solver.cpp:106] Iteration 30500, lr = 2.375e-06
I1127 20:08:45.866370  2966 solver.cpp:228] Iteration 30600, loss = 0.235584
I1127 20:08:45.866752  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.235585 (* 1 = 0.235585 loss)
I1127 20:08:45.866799  2966 sgd_solver.cpp:106] Iteration 30600, lr = 2.35e-06
I1127 20:09:50.077484  2966 solver.cpp:228] Iteration 30700, loss = 0.726194
I1127 20:09:50.077627  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.726195 (* 1 = 0.726195 loss)
I1127 20:09:50.077672  2966 sgd_solver.cpp:106] Iteration 30700, lr = 2.325e-06
I1127 20:10:11.511031  2966 solver.cpp:337] Iteration 30734, Testing net (#0)
I1127 20:10:25.255225  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.744186
I1127 20:10:25.255722  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.794897 (* 1 = 0.794897 loss)
I1127 20:11:06.591874  2966 solver.cpp:228] Iteration 30800, loss = 0.388427
I1127 20:11:06.592010  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.388428 (* 1 = 0.388428 loss)
I1127 20:11:06.592053  2966 sgd_solver.cpp:106] Iteration 30800, lr = 2.3e-06
I1127 20:12:07.828826  2966 solver.cpp:228] Iteration 30900, loss = 0.233278
I1127 20:12:07.828955  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.233279 (* 1 = 0.233279 loss)
I1127 20:12:07.828999  2966 sgd_solver.cpp:106] Iteration 30900, lr = 2.275e-06
I1127 20:12:53.590001  2966 solver.cpp:337] Iteration 30976, Testing net (#0)
I1127 20:13:06.699820  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.773256
I1127 20:13:06.699959  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.702502 (* 1 = 0.702502 loss)
I1127 20:13:22.289018  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_31000.caffemodel
I1127 20:13:22.411519  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_31000.solverstate
I1127 20:13:23.056771  2966 solver.cpp:228] Iteration 31000, loss = 0.330726
I1127 20:13:23.056892  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.330727 (* 1 = 0.330727 loss)
I1127 20:13:23.056936  2966 sgd_solver.cpp:106] Iteration 31000, lr = 2.25e-06
I1127 20:14:29.297309  2966 solver.cpp:228] Iteration 31100, loss = 0.0573416
I1127 20:14:29.297547  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0573426 (* 1 = 0.0573426 loss)
I1127 20:14:29.297600  2966 sgd_solver.cpp:106] Iteration 31100, lr = 2.225e-06
I1127 20:15:32.230335  2966 solver.cpp:228] Iteration 31200, loss = 0.191543
I1127 20:15:32.230461  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.191544 (* 1 = 0.191544 loss)
I1127 20:15:32.230504  2966 sgd_solver.cpp:106] Iteration 31200, lr = 2.2e-06
I1127 20:15:43.186094  2966 solver.cpp:337] Iteration 31218, Testing net (#0)
I1127 20:15:56.954093  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.767442
I1127 20:15:56.954201  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.722617 (* 1 = 0.722617 loss)
I1127 20:16:50.481526  2966 solver.cpp:228] Iteration 31300, loss = 0.291091
I1127 20:16:50.481683  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.291092 (* 1 = 0.291092 loss)
I1127 20:16:50.481734  2966 sgd_solver.cpp:106] Iteration 31300, lr = 2.175e-06
I1127 20:17:55.158771  2966 solver.cpp:228] Iteration 31400, loss = 0.178962
I1127 20:17:55.158934  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.178963 (* 1 = 0.178963 loss)
I1127 20:17:55.158978  2966 sgd_solver.cpp:106] Iteration 31400, lr = 2.15e-06
I1127 20:18:31.041347  2966 solver.cpp:337] Iteration 31460, Testing net (#0)
I1127 20:18:45.161667  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.761628
I1127 20:18:45.161789  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.725053 (* 1 = 0.725053 loss)
I1127 20:19:11.222482  2966 solver.cpp:228] Iteration 31500, loss = 0.391927
I1127 20:19:11.222868  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.391928 (* 1 = 0.391928 loss)
I1127 20:19:11.222915  2966 sgd_solver.cpp:106] Iteration 31500, lr = 2.125e-06
I1127 20:20:13.702209  2966 solver.cpp:228] Iteration 31600, loss = 0.148192
I1127 20:20:13.702402  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.148193 (* 1 = 0.148193 loss)
I1127 20:20:13.702476  2966 sgd_solver.cpp:106] Iteration 31600, lr = 2.1e-06
I1127 20:21:17.704187  2966 solver.cpp:228] Iteration 31700, loss = 0.0701352
I1127 20:21:17.704370  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0701361 (* 1 = 0.0701361 loss)
I1127 20:21:17.704414  2966 sgd_solver.cpp:106] Iteration 31700, lr = 2.075e-06
I1127 20:21:18.419178  2966 solver.cpp:337] Iteration 31702, Testing net (#0)
I1127 20:21:31.479948  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 20:21:31.480059  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.702569 (* 1 = 0.702569 loss)
I1127 20:22:30.840426  2966 solver.cpp:228] Iteration 31800, loss = 0.405458
I1127 20:22:30.840576  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.405459 (* 1 = 0.405459 loss)
I1127 20:22:30.840620  2966 sgd_solver.cpp:106] Iteration 31800, lr = 2.05e-06
I1127 20:23:31.389699  2966 solver.cpp:228] Iteration 31900, loss = 0.226541
I1127 20:23:31.389879  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.226542 (* 1 = 0.226542 loss)
I1127 20:23:31.389926  2966 sgd_solver.cpp:106] Iteration 31900, lr = 2.025e-06
I1127 20:23:59.636168  2966 solver.cpp:337] Iteration 31944, Testing net (#0)
I1127 20:24:14.804954  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.784884
I1127 20:24:14.805114  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.749693 (* 1 = 0.749693 loss)
I1127 20:24:50.647728  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_32000.caffemodel
I1127 20:24:50.763425  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_32000.solverstate
I1127 20:24:51.428797  2966 solver.cpp:228] Iteration 32000, loss = 0.594835
I1127 20:24:51.428917  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.594836 (* 1 = 0.594836 loss)
I1127 20:24:51.428961  2966 sgd_solver.cpp:106] Iteration 32000, lr = 2e-06
I1127 20:25:57.136384  2966 solver.cpp:228] Iteration 32100, loss = 1.56756
I1127 20:25:57.136514  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.56756 (* 1 = 1.56756 loss)
I1127 20:25:57.136610  2966 sgd_solver.cpp:106] Iteration 32100, lr = 1.975e-06
I1127 20:26:51.929292  2966 solver.cpp:337] Iteration 32186, Testing net (#0)
I1127 20:27:05.190460  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 20:27:05.190574  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.697518 (* 1 = 0.697518 loss)
I1127 20:27:15.293061  2966 solver.cpp:228] Iteration 32200, loss = 0.281122
I1127 20:27:15.293236  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.281123 (* 1 = 0.281123 loss)
I1127 20:27:15.293298  2966 sgd_solver.cpp:106] Iteration 32200, lr = 1.95e-06
I1127 20:28:22.553222  2966 solver.cpp:228] Iteration 32300, loss = 0.32178
I1127 20:28:22.553369  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.321782 (* 1 = 0.321782 loss)
I1127 20:28:22.553413  2966 sgd_solver.cpp:106] Iteration 32300, lr = 1.925e-06
I1127 20:29:26.132200  2966 solver.cpp:228] Iteration 32400, loss = 0.200569
I1127 20:29:26.132336  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.20057 (* 1 = 0.20057 loss)
I1127 20:29:26.132380  2966 sgd_solver.cpp:106] Iteration 32400, lr = 1.9e-06
I1127 20:29:43.441687  2966 solver.cpp:337] Iteration 32428, Testing net (#0)
I1127 20:29:57.280333  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.770349
I1127 20:29:57.280680  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.726577 (* 1 = 0.726577 loss)
I1127 20:30:44.222548  2966 solver.cpp:228] Iteration 32500, loss = 0.36142
I1127 20:30:44.222692  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.361422 (* 1 = 0.361422 loss)
I1127 20:30:44.222736  2966 sgd_solver.cpp:106] Iteration 32500, lr = 1.875e-06
I1127 20:31:49.110314  2966 solver.cpp:228] Iteration 32600, loss = 0.327502
I1127 20:31:49.110453  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.327503 (* 1 = 0.327503 loss)
I1127 20:31:49.110513  2966 sgd_solver.cpp:106] Iteration 32600, lr = 1.85e-06
I1127 20:32:31.225749  2966 solver.cpp:337] Iteration 32670, Testing net (#0)
I1127 20:32:44.490675  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.767442
I1127 20:32:44.490785  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.71961 (* 1 = 0.71961 loss)
I1127 20:33:03.838091  2966 solver.cpp:228] Iteration 32700, loss = 0.00455927
I1127 20:33:03.838225  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.00456064 (* 1 = 0.00456064 loss)
I1127 20:33:03.838286  2966 sgd_solver.cpp:106] Iteration 32700, lr = 1.825e-06
I1127 20:34:07.112884  2966 solver.cpp:228] Iteration 32800, loss = 0.0767047
I1127 20:34:07.113021  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.076706 (* 1 = 0.076706 loss)
I1127 20:34:07.113065  2966 sgd_solver.cpp:106] Iteration 32800, lr = 1.8e-06
I1127 20:35:07.513833  2966 solver.cpp:228] Iteration 32900, loss = 0.32812
I1127 20:35:07.514019  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.328122 (* 1 = 0.328122 loss)
I1127 20:35:07.514062  2966 sgd_solver.cpp:106] Iteration 32900, lr = 1.775e-06
I1127 20:35:14.158395  2966 solver.cpp:337] Iteration 32912, Testing net (#0)
I1127 20:35:27.008594  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.773256
I1127 20:35:27.008708  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.723943 (* 1 = 0.723943 loss)
I1127 20:36:20.606114  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_33000.caffemodel
I1127 20:36:20.714145  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_33000.solverstate
I1127 20:36:21.372946  2966 solver.cpp:228] Iteration 33000, loss = 0.0252104
I1127 20:36:21.373096  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0252117 (* 1 = 0.0252117 loss)
I1127 20:36:21.373147  2966 sgd_solver.cpp:106] Iteration 33000, lr = 1.75e-06
I1127 20:37:22.664378  2966 solver.cpp:228] Iteration 33100, loss = 0.0652878
I1127 20:37:22.664525  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0652891 (* 1 = 0.0652891 loss)
I1127 20:37:22.664572  2966 sgd_solver.cpp:106] Iteration 33100, lr = 1.725e-06
I1127 20:37:57.451431  2966 solver.cpp:337] Iteration 33154, Testing net (#0)
I1127 20:38:11.238644  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.790698
I1127 20:38:11.238863  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.737765 (* 1 = 0.737765 loss)
I1127 20:38:40.833437  2966 solver.cpp:228] Iteration 33200, loss = 0.122647
I1127 20:38:40.833575  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.122648 (* 1 = 0.122648 loss)
I1127 20:38:40.833618  2966 sgd_solver.cpp:106] Iteration 33200, lr = 1.7e-06
I1127 20:39:43.720882  2966 solver.cpp:228] Iteration 33300, loss = 0.146241
I1127 20:39:43.721015  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.146242 (* 1 = 0.146242 loss)
I1127 20:39:43.721057  2966 sgd_solver.cpp:106] Iteration 33300, lr = 1.675e-06
I1127 20:40:41.386945  2966 solver.cpp:337] Iteration 33396, Testing net (#0)
I1127 20:40:54.819972  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.761628
I1127 20:40:54.820089  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.728883 (* 1 = 0.728883 loss)
I1127 20:40:58.173285  2966 solver.cpp:228] Iteration 33400, loss = 0.437212
I1127 20:40:58.173401  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.437213 (* 1 = 0.437213 loss)
I1127 20:40:58.173444  2966 sgd_solver.cpp:106] Iteration 33400, lr = 1.65e-06
I1127 20:42:02.239197  2966 solver.cpp:228] Iteration 33500, loss = 0.0860616
I1127 20:42:02.239511  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0860628 (* 1 = 0.0860628 loss)
I1127 20:42:02.239555  2966 sgd_solver.cpp:106] Iteration 33500, lr = 1.625e-06
I1127 20:43:07.648346  2966 solver.cpp:228] Iteration 33600, loss = 0.102219
I1127 20:43:07.648514  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.10222 (* 1 = 0.10222 loss)
I1127 20:43:07.648560  2966 sgd_solver.cpp:106] Iteration 33600, lr = 1.6e-06
I1127 20:43:32.139205  2966 solver.cpp:337] Iteration 33638, Testing net (#0)
I1127 20:43:45.827205  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 20:43:45.827348  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.720124 (* 1 = 0.720124 loss)
I1127 20:44:27.327231  2966 solver.cpp:228] Iteration 33700, loss = 0.432453
I1127 20:44:27.327368  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.432454 (* 1 = 0.432454 loss)
I1127 20:44:27.327409  2966 sgd_solver.cpp:106] Iteration 33700, lr = 1.575e-06
I1127 20:45:29.508685  2966 solver.cpp:228] Iteration 33800, loss = 0.0808465
I1127 20:45:29.508821  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.080848 (* 1 = 0.080848 loss)
I1127 20:45:29.508864  2966 sgd_solver.cpp:106] Iteration 33800, lr = 1.55e-06
I1127 20:46:20.683542  2966 solver.cpp:337] Iteration 33880, Testing net (#0)
I1127 20:46:34.300019  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.758721
I1127 20:46:34.300132  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.725922 (* 1 = 0.725922 loss)
I1127 20:46:47.960677  2966 solver.cpp:228] Iteration 33900, loss = 0.308533
I1127 20:46:47.960790  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.308535 (* 1 = 0.308535 loss)
I1127 20:46:47.960832  2966 sgd_solver.cpp:106] Iteration 33900, lr = 1.525e-06
I1127 20:47:51.083983  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_34000.caffemodel
I1127 20:47:51.215551  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_34000.solverstate
I1127 20:47:51.874970  2966 solver.cpp:228] Iteration 34000, loss = 0.333297
I1127 20:47:51.875085  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.333299 (* 1 = 0.333299 loss)
I1127 20:47:51.875129  2966 sgd_solver.cpp:106] Iteration 34000, lr = 1.5e-06
I1127 20:48:56.589993  2966 solver.cpp:228] Iteration 34100, loss = 0.42293
I1127 20:48:56.590127  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.422932 (* 1 = 0.422932 loss)
I1127 20:48:56.590170  2966 sgd_solver.cpp:106] Iteration 34100, lr = 1.475e-06
I1127 20:49:10.277547  2966 solver.cpp:337] Iteration 34122, Testing net (#0)
I1127 20:49:23.413298  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 20:49:23.413410  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.680671 (* 1 = 0.680671 loss)
I1127 20:50:11.477766  2966 solver.cpp:228] Iteration 34200, loss = 0.0229683
I1127 20:50:11.477896  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0229698 (* 1 = 0.0229698 loss)
I1127 20:50:11.477939  2966 sgd_solver.cpp:106] Iteration 34200, lr = 1.45e-06
I1127 20:51:11.519796  2966 solver.cpp:228] Iteration 34300, loss = 0.161344
I1127 20:51:11.519929  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.161346 (* 1 = 0.161346 loss)
I1127 20:51:11.519973  2966 sgd_solver.cpp:106] Iteration 34300, lr = 1.425e-06
I1127 20:51:50.278834  2966 solver.cpp:337] Iteration 34364, Testing net (#0)
I1127 20:52:04.455104  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.790698
I1127 20:52:04.455266  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.712492 (* 1 = 0.712492 loss)
I1127 20:52:29.296761  2966 solver.cpp:228] Iteration 34400, loss = 0.352341
I1127 20:52:29.296903  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.352342 (* 1 = 0.352342 loss)
I1127 20:52:29.296947  2966 sgd_solver.cpp:106] Iteration 34400, lr = 1.4e-06
I1127 20:53:34.536403  2966 solver.cpp:228] Iteration 34500, loss = 0.0759252
I1127 20:53:34.536604  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0759267 (* 1 = 0.0759267 loss)
I1127 20:53:34.536650  2966 sgd_solver.cpp:106] Iteration 34500, lr = 1.375e-06
I1127 20:54:39.623690  2966 solver.cpp:228] Iteration 34600, loss = 0.725373
I1127 20:54:39.623824  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.725375 (* 1 = 0.725375 loss)
I1127 20:54:39.623878  2966 sgd_solver.cpp:106] Iteration 34600, lr = 1.35e-06
I1127 20:54:43.002949  2966 solver.cpp:337] Iteration 34606, Testing net (#0)
I1127 20:54:56.372633  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.767442
I1127 20:54:56.372745  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.707822 (* 1 = 0.707822 loss)
I1127 20:56:00.239562  2966 solver.cpp:228] Iteration 34700, loss = 0.100622
I1127 20:56:00.245851  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.100623 (* 1 = 0.100623 loss)
I1127 20:56:00.245903  2966 sgd_solver.cpp:106] Iteration 34700, lr = 1.325e-06
I1127 20:57:05.024996  2966 solver.cpp:228] Iteration 34800, loss = 0.26413
I1127 20:57:05.025135  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.264132 (* 1 = 0.264132 loss)
I1127 20:57:05.025178  2966 sgd_solver.cpp:106] Iteration 34800, lr = 1.3e-06
I1127 20:57:34.657100  2966 solver.cpp:337] Iteration 34848, Testing net (#0)
I1127 20:57:48.953157  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 20:57:48.953292  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.698441 (* 1 = 0.698441 loss)
I1127 20:58:23.380151  2966 solver.cpp:228] Iteration 34900, loss = 0.172968
I1127 20:58:23.380287  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.172969 (* 1 = 0.172969 loss)
I1127 20:58:23.380329  2966 sgd_solver.cpp:106] Iteration 34900, lr = 1.275e-06
I1127 20:59:27.760273  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_35000.caffemodel
I1127 20:59:27.883395  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_35000.solverstate
I1127 20:59:28.561059  2966 solver.cpp:228] Iteration 35000, loss = 0.0211195
I1127 20:59:28.561184  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.021121 (* 1 = 0.021121 loss)
I1127 20:59:28.561228  2966 sgd_solver.cpp:106] Iteration 35000, lr = 1.25e-06
I1127 21:00:23.113133  2966 solver.cpp:337] Iteration 35090, Testing net (#0)
I1127 21:00:36.649549  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:00:36.649660  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.684519 (* 1 = 0.684519 loss)
I1127 21:00:43.863946  2966 solver.cpp:228] Iteration 35100, loss = 0.462524
I1127 21:00:43.864060  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.462525 (* 1 = 0.462525 loss)
I1127 21:00:43.864104  2966 sgd_solver.cpp:106] Iteration 35100, lr = 1.225e-06
I1127 21:01:46.393539  2966 solver.cpp:228] Iteration 35200, loss = 0.185609
I1127 21:01:46.393738  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.185611 (* 1 = 0.185611 loss)
I1127 21:01:46.393815  2966 sgd_solver.cpp:106] Iteration 35200, lr = 1.2e-06
I1127 21:02:49.452440  2966 solver.cpp:228] Iteration 35300, loss = 0.0887635
I1127 21:02:49.452694  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.088765 (* 1 = 0.088765 loss)
I1127 21:02:49.452811  2966 sgd_solver.cpp:106] Iteration 35300, lr = 1.175e-06
I1127 21:03:09.568292  2966 solver.cpp:337] Iteration 35332, Testing net (#0)
I1127 21:03:22.899123  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:03:22.899271  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.712501 (* 1 = 0.712501 loss)
I1127 21:04:06.695488  2966 solver.cpp:228] Iteration 35400, loss = 0.0313408
I1127 21:04:06.695643  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0313422 (* 1 = 0.0313422 loss)
I1127 21:04:06.695704  2966 sgd_solver.cpp:106] Iteration 35400, lr = 1.15e-06
I1127 21:05:14.354266  2966 solver.cpp:228] Iteration 35500, loss = 0.255124
I1127 21:05:14.354408  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.255125 (* 1 = 0.255125 loss)
I1127 21:05:14.354454  2966 sgd_solver.cpp:106] Iteration 35500, lr = 1.125e-06
I1127 21:06:03.138205  2966 solver.cpp:337] Iteration 35574, Testing net (#0)
I1127 21:06:16.878443  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.776163
I1127 21:06:16.878546  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.696078 (* 1 = 0.696078 loss)
I1127 21:06:35.329322  2966 solver.cpp:228] Iteration 35600, loss = 0.0526189
I1127 21:06:35.329512  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0526202 (* 1 = 0.0526202 loss)
I1127 21:06:35.329578  2966 sgd_solver.cpp:106] Iteration 35600, lr = 1.1e-06
I1127 21:07:39.155501  2966 solver.cpp:228] Iteration 35700, loss = 0.282503
I1127 21:07:39.155637  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.282504 (* 1 = 0.282504 loss)
I1127 21:07:39.155680  2966 sgd_solver.cpp:106] Iteration 35700, lr = 1.075e-06
I1127 21:08:43.722440  2966 solver.cpp:228] Iteration 35800, loss = 0.0521044
I1127 21:08:43.722645  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0521057 (* 1 = 0.0521057 loss)
I1127 21:08:43.722692  2966 sgd_solver.cpp:106] Iteration 35800, lr = 1.05e-06
I1127 21:08:53.699429  2966 solver.cpp:337] Iteration 35816, Testing net (#0)
I1127 21:09:07.292071  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.764535
I1127 21:09:07.292182  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.698676 (* 1 = 0.698676 loss)
I1127 21:10:03.963304  2966 solver.cpp:228] Iteration 35900, loss = 0.125976
I1127 21:10:03.963455  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.125978 (* 1 = 0.125978 loss)
I1127 21:10:03.963500  2966 sgd_solver.cpp:106] Iteration 35900, lr = 1.025e-06
I1127 21:11:06.417265  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_36000.caffemodel
I1127 21:11:06.543098  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_36000.solverstate
I1127 21:11:07.233549  2966 solver.cpp:228] Iteration 36000, loss = 0.140141
I1127 21:11:07.233661  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.140142 (* 1 = 0.140142 loss)
I1127 21:11:07.233705  2966 sgd_solver.cpp:106] Iteration 36000, lr = 1e-06
I1127 21:11:44.649986  2966 solver.cpp:337] Iteration 36058, Testing net (#0)
I1127 21:11:58.431892  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.755814
I1127 21:11:58.432008  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.72316 (* 1 = 0.72316 loss)
I1127 21:12:26.924617  2966 solver.cpp:228] Iteration 36100, loss = 0.331818
I1127 21:12:26.924762  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.331819 (* 1 = 0.331819 loss)
I1127 21:12:26.924804  2966 sgd_solver.cpp:106] Iteration 36100, lr = 9.75e-07
I1127 21:13:32.751392  2966 solver.cpp:228] Iteration 36200, loss = 1.43494
I1127 21:13:32.751761  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 1.43494 (* 1 = 1.43494 loss)
I1127 21:13:32.751816  2966 sgd_solver.cpp:106] Iteration 36200, lr = 9.5e-07
I1127 21:14:35.114400  2966 solver.cpp:337] Iteration 36300, Testing net (#0)
I1127 21:14:48.410470  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:14:48.410591  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.680137 (* 1 = 0.680137 loss)
I1127 21:14:49.020949  2966 solver.cpp:228] Iteration 36300, loss = 0.189482
I1127 21:14:49.021070  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.189483 (* 1 = 0.189483 loss)
I1127 21:14:49.021112  2966 sgd_solver.cpp:106] Iteration 36300, lr = 9.25e-07
I1127 21:15:53.283881  2966 solver.cpp:228] Iteration 36400, loss = 0.413932
I1127 21:15:53.284060  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.413933 (* 1 = 0.413933 loss)
I1127 21:15:53.284102  2966 sgd_solver.cpp:106] Iteration 36400, lr = 9e-07
I1127 21:16:58.147174  2966 solver.cpp:228] Iteration 36500, loss = 0.109283
I1127 21:16:58.147389  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.109284 (* 1 = 0.109284 loss)
I1127 21:16:58.147433  2966 sgd_solver.cpp:106] Iteration 36500, lr = 8.75e-07
I1127 21:17:24.587267  2966 solver.cpp:337] Iteration 36542, Testing net (#0)
I1127 21:17:38.126371  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.773256
I1127 21:17:38.126502  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.730564 (* 1 = 0.730564 loss)
I1127 21:18:15.205770  2966 solver.cpp:228] Iteration 36600, loss = 0.262816
I1127 21:18:15.205901  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.262817 (* 1 = 0.262817 loss)
I1127 21:18:15.205945  2966 sgd_solver.cpp:106] Iteration 36600, lr = 8.5e-07
I1127 21:19:20.410738  2966 solver.cpp:228] Iteration 36700, loss = 0.0427373
I1127 21:19:20.410918  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0427385 (* 1 = 0.0427385 loss)
I1127 21:19:20.410961  2966 sgd_solver.cpp:106] Iteration 36700, lr = 8.25e-07
I1127 21:20:13.303774  2966 solver.cpp:337] Iteration 36784, Testing net (#0)
I1127 21:20:26.594200  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.776163
I1127 21:20:26.594313  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.699607 (* 1 = 0.699607 loss)
I1127 21:20:37.959503  2966 solver.cpp:228] Iteration 36800, loss = 0.0541038
I1127 21:20:37.959638  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.054105 (* 1 = 0.054105 loss)
I1127 21:20:37.959681  2966 sgd_solver.cpp:106] Iteration 36800, lr = 8e-07
I1127 21:21:42.930733  2966 solver.cpp:228] Iteration 36900, loss = 0.199546
I1127 21:21:42.930915  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.199548 (* 1 = 0.199548 loss)
I1127 21:21:42.930963  2966 sgd_solver.cpp:106] Iteration 36900, lr = 7.75e-07
I1127 21:22:47.176251  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_37000.caffemodel
I1127 21:22:47.298640  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_37000.solverstate
I1127 21:22:48.173435  2966 solver.cpp:228] Iteration 37000, loss = 0.151396
I1127 21:22:48.173607  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.151397 (* 1 = 0.151397 loss)
I1127 21:22:48.173732  2966 sgd_solver.cpp:106] Iteration 37000, lr = 7.5e-07
I1127 21:23:04.827165  2966 solver.cpp:337] Iteration 37026, Testing net (#0)
I1127 21:23:18.735446  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 21:23:18.735584  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.711693 (* 1 = 0.711693 loss)
I1127 21:24:08.078477  2966 solver.cpp:228] Iteration 37100, loss = 0.364354
I1127 21:24:08.078608  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.364355 (* 1 = 0.364355 loss)
I1127 21:24:08.078651  2966 sgd_solver.cpp:106] Iteration 37100, lr = 7.25e-07
I1127 21:25:13.161859  2966 solver.cpp:228] Iteration 37200, loss = 0.0751051
I1127 21:25:13.162156  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0751064 (* 1 = 0.0751064 loss)
I1127 21:25:13.162200  2966 sgd_solver.cpp:106] Iteration 37200, lr = 7e-07
I1127 21:25:56.897927  2966 solver.cpp:337] Iteration 37268, Testing net (#0)
I1127 21:26:10.776787  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.787791
I1127 21:26:10.776896  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.691821 (* 1 = 0.691821 loss)
I1127 21:26:32.722139  2966 solver.cpp:228] Iteration 37300, loss = 0.276895
I1127 21:26:32.722645  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.276896 (* 1 = 0.276896 loss)
I1127 21:26:32.722692  2966 sgd_solver.cpp:106] Iteration 37300, lr = 6.75e-07
I1127 21:27:38.562836  2966 solver.cpp:228] Iteration 37400, loss = 0.401761
I1127 21:27:38.563009  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.401762 (* 1 = 0.401762 loss)
I1127 21:27:38.563057  2966 sgd_solver.cpp:106] Iteration 37400, lr = 6.5e-07
I1127 21:28:42.709065  2966 solver.cpp:228] Iteration 37500, loss = 0.548383
I1127 21:28:42.709197  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.548384 (* 1 = 0.548384 loss)
I1127 21:28:42.709239  2966 sgd_solver.cpp:106] Iteration 37500, lr = 6.25e-07
I1127 21:28:48.530038  2966 solver.cpp:337] Iteration 37510, Testing net (#0)
I1127 21:29:02.467326  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:29:02.467437  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.699038 (* 1 = 0.699038 loss)
I1127 21:29:59.791601  2966 solver.cpp:228] Iteration 37600, loss = 0.273368
I1127 21:29:59.791730  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.273369 (* 1 = 0.273369 loss)
I1127 21:29:59.791774  2966 sgd_solver.cpp:106] Iteration 37600, lr = 6e-07
I1127 21:31:02.914763  2966 solver.cpp:228] Iteration 37700, loss = 0.0623244
I1127 21:31:02.914898  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0623252 (* 1 = 0.0623252 loss)
I1127 21:31:02.914942  2966 sgd_solver.cpp:106] Iteration 37700, lr = 5.75e-07
I1127 21:31:34.711844  2966 solver.cpp:337] Iteration 37752, Testing net (#0)
I1127 21:31:48.259824  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.784884
I1127 21:31:48.259938  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.718563 (* 1 = 0.718563 loss)
I1127 21:32:20.408674  2966 solver.cpp:228] Iteration 37800, loss = 0.193864
I1127 21:32:20.408852  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.193865 (* 1 = 0.193865 loss)
I1127 21:32:20.408895  2966 sgd_solver.cpp:106] Iteration 37800, lr = 5.5e-07
I1127 21:33:25.486081  2966 solver.cpp:228] Iteration 37900, loss = 0.0225996
I1127 21:33:25.486217  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0226004 (* 1 = 0.0226004 loss)
I1127 21:33:25.486260  2966 sgd_solver.cpp:106] Iteration 37900, lr = 5.25e-07
I1127 21:34:24.360301  2966 solver.cpp:337] Iteration 37994, Testing net (#0)
I1127 21:34:37.818022  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:34:37.818130  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.717309 (* 1 = 0.717309 loss)
I1127 21:34:41.723975  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_38000.caffemodel
I1127 21:34:41.850518  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_38000.solverstate
I1127 21:34:42.554769  2966 solver.cpp:228] Iteration 38000, loss = 0.100483
I1127 21:34:42.554932  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.100484 (* 1 = 0.100484 loss)
I1127 21:34:42.554977  2966 sgd_solver.cpp:106] Iteration 38000, lr = 5e-07
I1127 21:35:46.872476  2966 solver.cpp:228] Iteration 38100, loss = 0.158032
I1127 21:35:46.872788  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.158032 (* 1 = 0.158032 loss)
I1127 21:35:46.872831  2966 sgd_solver.cpp:106] Iteration 38100, lr = 4.75e-07
I1127 21:36:51.934638  2966 solver.cpp:228] Iteration 38200, loss = 0.0803704
I1127 21:36:51.934866  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.080371 (* 1 = 0.080371 loss)
I1127 21:36:51.934911  2966 sgd_solver.cpp:106] Iteration 38200, lr = 4.5e-07
I1127 21:37:15.199780  2966 solver.cpp:337] Iteration 38236, Testing net (#0)
I1127 21:37:28.610946  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.784884
I1127 21:37:28.611080  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.693753 (* 1 = 0.693753 loss)
I1127 21:38:10.251029  2966 solver.cpp:228] Iteration 38300, loss = 0.0898328
I1127 21:38:10.251170  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0898332 (* 1 = 0.0898332 loss)
I1127 21:38:10.251219  2966 sgd_solver.cpp:106] Iteration 38300, lr = 4.25e-07
I1127 21:39:13.000167  2966 solver.cpp:228] Iteration 38400, loss = 0.169825
I1127 21:39:13.000318  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.169825 (* 1 = 0.169825 loss)
I1127 21:39:13.000363  2966 sgd_solver.cpp:106] Iteration 38400, lr = 4e-07
I1127 21:40:00.830476  2966 solver.cpp:337] Iteration 38478, Testing net (#0)
I1127 21:40:14.433480  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.787791
I1127 21:40:14.433595  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.697053 (* 1 = 0.697053 loss)
I1127 21:40:30.623646  2966 solver.cpp:228] Iteration 38500, loss = 0.748581
I1127 21:40:30.623759  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.748582 (* 1 = 0.748582 loss)
I1127 21:40:30.623800  2966 sgd_solver.cpp:106] Iteration 38500, lr = 3.75e-07
I1127 21:41:34.284147  2966 solver.cpp:228] Iteration 38600, loss = 0.123871
I1127 21:41:34.284282  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.123871 (* 1 = 0.123871 loss)
I1127 21:41:34.284325  2966 sgd_solver.cpp:106] Iteration 38600, lr = 3.5e-07
I1127 21:42:40.823194  2966 solver.cpp:228] Iteration 38700, loss = 0.152908
I1127 21:42:40.823331  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.152909 (* 1 = 0.152909 loss)
I1127 21:42:40.823376  2966 sgd_solver.cpp:106] Iteration 38700, lr = 3.25e-07
I1127 21:42:53.518234  2966 solver.cpp:337] Iteration 38720, Testing net (#0)
I1127 21:43:07.008628  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:43:07.008743  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.711126 (* 1 = 0.711126 loss)
I1127 21:43:56.857481  2966 solver.cpp:228] Iteration 38800, loss = 0.287166
I1127 21:43:56.857663  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.287167 (* 1 = 0.287167 loss)
I1127 21:43:56.857712  2966 sgd_solver.cpp:106] Iteration 38800, lr = 3e-07
I1127 21:44:58.683686  2966 solver.cpp:228] Iteration 38900, loss = 0.205488
I1127 21:44:58.683820  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.205489 (* 1 = 0.205489 loss)
I1127 21:44:58.683863  2966 sgd_solver.cpp:106] Iteration 38900, lr = 2.75e-07
I1127 21:45:36.345111  2966 solver.cpp:337] Iteration 38962, Testing net (#0)
I1127 21:45:49.173202  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:45:49.173316  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.701723 (* 1 = 0.701723 loss)
I1127 21:46:12.245488  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_39000.caffemodel
I1127 21:46:12.356287  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_39000.solverstate
I1127 21:46:13.010465  2966 solver.cpp:228] Iteration 39000, loss = 0.0682278
I1127 21:46:13.010648  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0682283 (* 1 = 0.0682283 loss)
I1127 21:46:13.010705  2966 sgd_solver.cpp:106] Iteration 39000, lr = 2.5e-07
I1127 21:47:14.654100  2966 solver.cpp:228] Iteration 39100, loss = 0.0992117
I1127 21:47:14.654505  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0992122 (* 1 = 0.0992122 loss)
I1127 21:47:14.654551  2966 sgd_solver.cpp:106] Iteration 39100, lr = 2.25e-07
I1127 21:48:16.151101  2966 solver.cpp:228] Iteration 39200, loss = 0.218345
I1127 21:48:16.151257  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.218346 (* 1 = 0.218346 loss)
I1127 21:48:16.151304  2966 sgd_solver.cpp:106] Iteration 39200, lr = 2e-07
I1127 21:48:18.025701  2966 solver.cpp:337] Iteration 39204, Testing net (#0)
I1127 21:48:30.996507  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.77907
I1127 21:48:30.996637  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.701137 (* 1 = 0.701137 loss)
I1127 21:49:31.508952  2966 solver.cpp:228] Iteration 39300, loss = 0.094463
I1127 21:49:31.509130  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0944636 (* 1 = 0.0944636 loss)
I1127 21:49:31.509248  2966 sgd_solver.cpp:106] Iteration 39300, lr = 1.75e-07
I1127 21:50:34.354722  2966 solver.cpp:228] Iteration 39400, loss = 0.395738
I1127 21:50:34.354876  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.395739 (* 1 = 0.395739 loss)
I1127 21:50:34.354921  2966 sgd_solver.cpp:106] Iteration 39400, lr = 1.5e-07
I1127 21:51:02.616000  2966 solver.cpp:337] Iteration 39446, Testing net (#0)
I1127 21:51:15.456123  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:51:15.456302  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.707562 (* 1 = 0.707562 loss)
I1127 21:51:49.291187  2966 solver.cpp:228] Iteration 39500, loss = 0.137191
I1127 21:51:49.291329  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.137191 (* 1 = 0.137191 loss)
I1127 21:51:49.291373  2966 sgd_solver.cpp:106] Iteration 39500, lr = 1.25e-07
I1127 21:52:51.712932  2966 solver.cpp:228] Iteration 39600, loss = 0.145313
I1127 21:52:51.713093  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.145314 (* 1 = 0.145314 loss)
I1127 21:52:51.713136  2966 sgd_solver.cpp:106] Iteration 39600, lr = 9.99999e-08
I1127 21:53:46.548692  2966 solver.cpp:337] Iteration 39688, Testing net (#0)
I1127 21:53:59.669028  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.781977
I1127 21:53:59.669140  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.69713 (* 1 = 0.69713 loss)
I1127 21:54:07.886683  2966 solver.cpp:228] Iteration 39700, loss = 0.0260155
I1127 21:54:07.886821  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.0260161 (* 1 = 0.0260161 loss)
I1127 21:54:07.886868  2966 sgd_solver.cpp:106] Iteration 39700, lr = 7.49999e-08
I1127 21:55:09.791316  2966 solver.cpp:228] Iteration 39800, loss = 0.194616
I1127 21:55:09.791448  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.194617 (* 1 = 0.194617 loss)
I1127 21:55:09.791491  2966 sgd_solver.cpp:106] Iteration 39800, lr = 5e-08
I1127 21:56:12.085218  2966 solver.cpp:228] Iteration 39900, loss = 0.500597
I1127 21:56:12.085384  2966 solver.cpp:244]     Train net output #0: SoftmaxWithLoss1 = 0.500597 (* 1 = 0.500597 loss)
I1127 21:56:12.085428  2966 sgd_solver.cpp:106] Iteration 39900, lr = 2.5e-08
I1127 21:56:30.195303  2966 solver.cpp:337] Iteration 39930, Testing net (#0)
I1127 21:56:43.317756  2966 solver.cpp:404]     Test net output #0: Accuracy1 = 0.784884
I1127 21:56:43.317909  2966 solver.cpp:404]     Test net output #1: SoftmaxWithLoss1 = 0.696335 (* 1 = 0.696335 loss)
I1127 21:57:27.580446  2966 solver.cpp:454] Snapshotting to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_40000.caffemodel
I1127 21:57:27.694010  2966 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/caffe/caffe/examples/myfile/myfile_nin/cloudernet_train_snapshot/cloudernet_train_snapshot_iter_40000.solverstate
I1127 21:57:27.984747  2966 solver.cpp:317] Iteration 40000, loss = 0.524002
I1127 21:57:27.984858  2966 solver.cpp:322] Optimization Done.
I1127 21:57:27.984895  2966 caffe.cpp:254] Optimization Done.
